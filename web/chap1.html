<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<link rel="stylesheet" type="text/css" href="css/style.css" />
<title>Intro Python - Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</title>

                <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="viewport-fit=cover, width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            

                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            
<link href="https://fonts.googleapis.com/css2?family=Gentium+Basic&display=swap" rel="stylesheet"> 
<link rel="stylesheet" type="text/css" href="slidey/bootstrap/css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" href="slidey/bootstrap-icons/font/bootstrap-icons.css" />
<link rel="stylesheet" type="text/css" href="slidey/highlight.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.menu.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.step.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.code.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.slide.css" />
<script type="text/javascript" src="slidey/js/jquery.js"></script>
<script type="text/javascript" src="slidey/js/slidey.permalink.js"></script>
<script type="text/javascript" src="slidey/js/slidey.menu.js"></script>
<script type="text/javascript" src="slidey/js/slidey.mobile.js"></script>
<script type="text/javascript" src="slidey/js/slidey.spoilers.js"></script>
<script type="text/javascript" src="slidey/js/slidey.steps.js"></script>
<script type="text/javascript" src="slidey/js/slidey.js"></script>
<script type="text/javascript" src="slidey/js/main.js"></script>
<script type="text/javascript" src="slidey/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="slidey/highlight/highlight.pack.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico" />
</head>
<body>
<!-- Modal log-in window -->
<div class="modal fade" id="loginWindow">
  <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
            <h5 class="modal-title">Log-in</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>    
        <div class="modal-body">
            <form>
                Mot de passe&nbsp;:<br />
                <input class="form-control" type="password" name="password" />
            </form>
        </div>
      </div><!-- /.modal-content -->
    </div><!-- /.modal-dialog -->
</div>

<div class="core">

    <script>hljs.highlightAll();</script>

<!-- Extra-controls for mobile device -->
<div class="mobileControls">
    <div class="btn-group">
        <a href="javascript:void(0)" class="left btn btn-light btn-lg">
            <i class="bi bi-arrow-up"></i>
        </a>
        <a href="javascript:void(0)" class="right btn btn-light btn-lg">
            <i class="bi bi-arrow-down"></i>
        </a>
        <a href="javascript:void(0)" class="up btn btn-light btn-lg">
            <i class="bi bi-arrow-left"></i>
        </a>
        <a href="javascript:void(0)" class="down btn btn-light btn-lg">
            <i class="bi bi-arrow-right"></i>
        </a>
        <a href="javascript:void(0)" class="login btn btn-light btn-lg">
            <i class="bi bi-lock"></i>
        </a>
    </div>
</div>

<!-- Controls -->
<div class="slideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg slideModeSlide"><i class="bi bi-film"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i> </a>
        <a href="index.html" class="btn btn-light btn-lg goHome"><i class="bi bi-house"></i></a>
    </div>
</div>

<div class="exitSlideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg showMobile"><i class="bi bi-phone"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg stopShow">
            <span class="currentSlideNumber">
            </span>
            <i class="bi bi-x-lg"></i>
        </a>
    </div>
</div>

<!-- Browsing menu -->
<div class="menu">
</div>

<div class="contents container">

<div class="slide ">
<a id="title.1"></a><h1>Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</h1>
<a id="title.1.1"></a><h2>üéØ Objectifs du Chapitre</h2>
<div class="alert alert-light p-2 fs-5 text-center"><p>√Ä la fin de ce chapitre, vous saurez : </p>
<ul><li class="dash">Cr√©er et manipuler des tenseurs PyTorch sur CPU et GPU.</li>
<li class="dash">Calculer automatiquement les gradients √† l‚Äôaide de <code>autograd</code>.</li>
<li class="dash">D√©finir une fonction de perte.</li>
<li class="dash">Utiliser un optimiseur pour ajuster les param√®tres d‚Äôun mod√®le.</li>
<li class="dash">Impl√©menter une boucle d&#039;entra√Ænement simple.</li>
</ul>

</div>
</div><div class="slide ">
<a id="title.1.2"></a><h2>üìñ 1. Qu&#039;est-ce que PyTorch ? </h2>
<p>PyTorch est une biblioth√®que Python de machine learning open-source d√©velopp√©e par Facebook (FAIR). Elle est con√ßue pour faciliter la cr√©ation et l&#039;entra√Ænement de mod√®les, en particulier dans le domaine du deep learning. </p>
<p>Elle repose principalement sur deux √©l√©ments :</p>
<p>A) Les <em>tenseurs</em>, des structures de donn√©es similaires aux tableaux NumPy (<code>ndarray</code>), mais avec des fonctionnalit√©s suppl√©mentaires pour :</p>
<ul><li class="dash">le calcul diff√©rentiel automatique,</li>
<li class="dash">l&#039;acc√©l√©ration GPU,</li>
<li class="dash">l‚Äôentra√Ænement de r√©seaux de neurones.</li>
</ul>

<p>B) Le module <code>autograd</code> permet de calculer automatiquement les gradients n√©cessaires √† l&#039;entra√Ænement des mod√®les, en suivant toutes les op√©rations effectu√©es sur les tenseurs.</p>
</div><div class="slide ">
<p>D&#039;autres biblioth√®ques Python similaires existent, comme :</p>
<ul><li class="dash">TensorFlow : d√©velopp√© par Google, tr√®s utilis√© pour des d√©ploiements √† grande √©chelle.</li>
<li class="dash">Keras : interface haut niveau de TensorFlow, plus simple mais moins flexible.</li>
<li class="dash">JAX : plus r√©cent, optimis√© pour la recherche et les calculs scientifiques √† haute performance.</li>
</ul>

</div><div class="slide ">
<p>Dans le cadre de ce cours, nous utiliserons PyTorch car :</p>
<ul><li class="dash">elle est largement adopt√©e par la communaut√© de la recherche en deep learning,</li>
<li class="dash">elle est plus lisible et plus facile √† d√©boguer que TensorFlow et JAX,</li>
<li class="dash">elle offre plus de possibilit√©s que Keras,</li>
<li class="dash">elle est bien document√©e et est l&#039;une des biblioth√®ques les plus utilis√©es en science des donn√©es (Data Science en anglais) et en apprentissage machine (Machine Learning en anglais).</li>
</ul>

</div><div class="slide ">
<a id="title.1.3"></a><h2>üìñ 2. Qu&#039;est-ce qu&#039;un tenseur ?</h2>
<p>Les <strong>tenseurs</strong> sont la structure de base de PyTorch. Ce sont des tableaux multidimensionnels similaires aux <code>ndarray</code> de NumPy, mais avec des fonctionnalit√©s suppl√©mentaires pour le GPU et le calcul automatique des gradients. Un tenseur est une structure de donn√©es qui g√©n√©ralise les matrices √† un nombre quelconque de dimensions:</p>
<ul><li class="dash">Un scalaire est un tenseur 0D.</li>
<li class="dash">Un vecteur est un tenseur 1D.</li>
<li class="dash">Une matrice est un tenseur 2D.</li>
<li class="dash">On peut avoir des tenseurs 3D, 4D, etc.</li>
</ul>

<p>Les tenseurs √† haute dimensions sont tr√®s utilis√©s en deep learning (par exemple pour les images ou les vid√©os). Nous allons voir comment cr√©er et manipuler des tenseurs dans PyTorch. Vous pouvez copier-coller les exemples de code ci-dessous dans un notebook Jupyter pour les tester et voir les affichages. Pour utiliserles fonctions de PyTorch, il faut d&#039;abord l&#039;importer :</p>
<pre><code class="python hljs">import torch</code></pre>
</div><div class="slide ">
<a id="title.1.4"></a><h2>üìñ 3. Cr√©ation de tenseurs</h2>
<p>Il existe plusieurs mani√®res de cr√©er un tenseur en PyTorch.</p>
<a id="title.1.4.1"></a><h3>3.1. √Ä partir de donn√©es Python (listes ou tuples)</h3>
<pre><code class="python hljs"># Depuis une liste
a = torch.tensor([1, 2, 3])
print(a)

# Depuis une liste de listes (matrice)
b = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(b)

# On peut aussi sp√©cifier le type de donn√©es
c = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
print(c, c.dtype)</code></pre>
</div><div class="slide ">
<a id="title.1.4.2"></a><h3>3.2. Avec des fonctions de construction</h3>
<pre><code class="python hljs"># Tenseur rempli de z√©ros
z = torch.zeros(2, 3)
print(z)

# Tenseur rempli de uns
o = torch.ones(2, 3)
print(o)

# Tenseur vide (valeurs non initialis√©es)
e = torch.empty(2, 3)
print(e)

# Identit√© (matrice diagonale)
eye = torch.eye(3)
print(eye)</code></pre>
</div><div class="slide ">
<a id="title.1.4.3"></a><h3>3.3. Avec des suites r√©guli√®res</h3>
<p>PyTorch permet de g√©n√©rer facilement des suites de nombres avec des pas r√©guliers. Deux fonctions sont particuli√®rement utiles :</p>
<ol><li class=""><strong>torch.arange(debut, fin, pas)</strong></li>
<ul><li class="dash">Cr√©e une suite en commen√ßant √† <code>debut</code></li>
<li class="dash">S‚Äôarr√™te <em>avant</em> <code>fin</code> (attention, la borne sup√©rieure est exclue !)</li>
<li class="dash">Utilise le <code>pas</code> indiqu√©</li>
</ul>
</ol>

<pre><code class="python hljs"># De 0 √† 8 inclus, avec un pas de 2
r = torch.arange(0, 10, 2)
print(&quot;torch.arange(0, 10, 2) :&quot;, r)

# De 5 √† 20 exclu, avec un pas de 3
r2 = torch.arange(5, 20, 3)
print(&quot;torch.arange(5, 20, 3) :&quot;, r2)

# ‚ö†Ô∏è Remarque : la borne sup√©rieure (ici 10 ou 20) n&#039;est jamais incluse</code></pre>
</div><div class="slide ">
<ol><li class=""><strong>torch.linspace(debut, fin, steps)</strong></li>
<ul><li class="dash">Cr√©e une suite de <code>steps</code> valeurs r√©guli√®rement espac√©es</li>
<li class="dash">Inclut <strong>√† la fois</strong> <code>debut</code> et <code>fin</code></li>
</ul>
</ol>

<pre><code class="python hljs"># 5 valeurs entre 0 et 1 inclus
l = torch.linspace(0, 1, steps=5)
print(&quot;torch.linspace(0, 1, steps=5) :&quot;, l)

# 4 valeurs entre -1 et 1 inclus
l2 = torch.linspace(-1, 1, steps=4)
print(&quot;torch.linspace(-1, 1, steps=4) :&quot;, l2)</code></pre>
<p><strong>R√©sum√© des diff√©rences</strong></p>
<ul><li class="dash"><code>arange</code> ‚Üí on fixe le <strong>pas</strong> entre les valeurs, la fin est exclue.</li>
<li class="dash"><code>linspace</code> ‚Üí on fixe le <strong>nombre de valeurs</strong>, la fin est incluse.</li>
</ul>

<p>Exemple comparatif :</p>
<pre><code class="python hljs">print(torch.arange(0, 1, 0.25))   # [0.00, 0.25, 0.50, 0.75]
print(torch.linspace(0, 1, 5))    # [0.00, 0.25, 0.50, 0.75, 1.00]</code></pre>
</div><div class="slide ">
<a id="title.1.4.4"></a><h3>3.4. Avec des nombres al√©atoires</h3>
<pre><code class="python hljs"># Attention dans les exemples suivants, les crochets [] veulent dire que la valeur de la borne est incluse, contrairement √† aux parenth√®ses () qui signifient que la borne est exclue.
# Uniforme entre [0, 1)
u = torch.rand(2, 2)
print(&quot;Uniforme [0,1) :\n&quot;, u)

# Distribution normale (moyenne=0, √©cart-type=1)
n = torch.randn(2, 2)
print(&quot;Normale standard (0,1) :\n&quot;, n)

# Distribution normale avec moyenne (mean) et √©cart-type (std) choisis
custom = torch.normal(mean=2.0, std=3.0, size=(2,2))
print(&quot;Normale (moyenne=10, √©cart-type=2) :\n&quot;, custom)

# Fixer la graine pour la reproductibilit√©
torch.manual_seed(42)
print(&quot;Reproductibilit√© :\n&quot;, torch.rand(2, 2))  # toujours le m√™me r√©sultat</code></pre>
</div><div class="slide ">
<a id="title.1.5"></a><h2>üìñ 4. Conna√Ætre la forme d&#039;un tenseur</h2>
<p>Un tenseur peut avoir n‚Äôimporte quelle dimension. La m√©thode <code>.shape</code> permet de conna√Ætre sa taille.</p>
<pre><code class="python hljs"># Scalaire (0D)
s = torch.tensor(5)
print(&quot;Scalaire :&quot;, s, &quot;shape =&quot;, s.shape)

# Vecteur (1D)
v = torch.tensor([1, 2, 3, 4])
print(&quot;Vecteur :&quot;, v, &quot;shape =&quot;, v.shape)

# Matrice (2D)
m = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(&quot;Matrice :\n&quot;, m, &quot;shape =&quot;, m.shape)

# Tenseur 3D (par exemple, 2 matrices de taille 3x3)
t3 = torch.zeros(2, 3, 3)
print(&quot;Tenseur 3D shape =&quot;, t3.shape)

# Tenseur 4D (par exemple, un mini-batch de 10 images RGB de 32x32)
t4 = torch.zeros(10, 3, 32, 32)
print(&quot;Tenseur 4D shape =&quot;, t4.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.6"></a><h2>üìñ 5. Types de tenseurs et conversion</h2>
<ul><li class="dash">Vous pouvez sp√©cifier le type de donn√©es (<code>dtype</code>) lors de la cr√©ation :</li>
</ul>

<pre><code class="python hljs">x = torch.tensor([1.2, 3.4, 5.6])
print(x.dtype)     # float32 par d√©faut

x_int = x.to(torch.int32)
print(x_int, x_int.dtype)

x_float64 = x.double()
print(x_float64, x_float64.dtype)</code></pre>
<ul><li class="dash">Conversion d‚Äôun tenseur existant :</li>
</ul>

<pre><code class="python hljs">x_int = x.to(torch.int32)
print(x_int.dtype)</code></pre>
</div><div class="slide ">
<a id="title.1.7"></a><h2>üìñ 6. Op√©rations de base</h2>
<p>PyTorch supporte de nombreuses op√©rations sur les tenseurs :</p>
<pre><code class="python hljs">a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# Addition
print(a + b)

# Multiplication √©l√©ment par √©l√©ment
print(a * b)

# Produit matriciel
mat1 = torch.rand(2, 3)
mat2 = torch.rand(3, 4)
print(torch.mm(mat1, mat2))</code></pre>
</div><div class="slide ">
<a id="title.1.8"></a><h2>üìñ 7. Tenseurs sur GPU</h2>
<p>Pour profiter de l‚Äôacc√©l√©ration GPU, il suffit de d√©placer un tenseur sur le device CUDA :</p>
<pre><code class="python hljs">if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    x_gpu = x.to(device)
    print(&quot;Tenseur sur GPU :&quot;, x_gpu)
else:
    print(&quot;Pas de GPU disponible, utilisation du CPU.&quot;)</code></pre>
</div><div class="slide ">
<a id="title.1.9"></a><h2>üìñ 8.  Manipulation avanc√©e des tenseurs</h2>
<p>Une fois cr√©√©s, les tenseurs peuvent √™tre transform√©s et r√©arrang√©s. PyTorch fournit de nombreuses fonctions pour modifier leur forme, leurs dimensions ou leur ordre.</p>
<a id="title.1.9.1"></a><h3>8.1. Changer la forme avec <code>view</code> et <code>reshape</code></h3>
<ul><li class="dash"><code>view</code> : retourne un nouveau tenseur qui partage la m√™me m√©moire que l‚Äôoriginal. Cela implique que le tenseur soit contigu. Un tenseur est dit contigu lorsque ses donn√©es sont stock√©es de mani√®re cons√©cutive en m√©moire, c‚Äôest-√†-dire que PyTorch peut lire tous les √©l√©ments dans l‚Äôordre sans sauts.
Certaines op√©rations, comme la transposition (`t()`), rendent le tenseur non contigu, et dans ce cas <code>view</code> √©choue.</li>
<li class="dash"><code>reshape</code> : similaire √† <code>view</code>, mais plus flexible car il tente d‚Äôutiliser la m√©moire existante, mais cr√©e une copie si n√©cessaire. <code>reshape</code> fonctionne dans tous les cas de figures.</li>
</ul>

<pre><code class="python hljs">x = torch.arange(12)   # tenseur 1D [0, 1, ..., 11]
print(&quot;x :&quot;, x)

# Transformer en matrice 3x4
x_view = x.view(3, 4)
print(&quot;view en 3x4 :\n&quot;, x_view)

# Transformer en matrice 2x6
x_reshape = x.reshape(2, 6)
print(&quot;reshape en 2x6 :\n&quot;, x_reshape)</code></pre>
</div><div class="slide ">
<p>Autre exemple pour illustrer la diff√©rence entre <code>view</code> et <code>reshape</code> :</p>
<pre><code class="python hljs"># Cr√©ation d&#039;un tenseur 2x3
x = torch.arange(6).view(2, 3)
print(&quot;x :\n&quot;, x)
print(&quot;Contigu :&quot;, x.is_contiguous())

# Transposition ‚Üí rend le tenseur non contigu
y = x.t()
print(&quot;\ny (transpos√©) :\n&quot;, y)
print(&quot;Contigu :&quot;, y.is_contiguous())

# view √©choue sur un tenseur non contigu
try:
    z = y.view(6)
except Exception as e:
    print(&quot;\nErreur avec view :&quot;, e)

# reshape fonctionne toujours
z2 = y.reshape(6)
print(&quot;\nreshape fonctionne :&quot;, z2)</code></pre>
</div><div class="slide ">
<a id="title.1.9.2"></a><h3>8.2. Changer l‚Äôordre des dimensions : <code>permute</code></h3>
<ul><li class="dash"><code>permute</code> r√©arrange les dimensions dans un nouvel ordre.</li>
<li class="dash">Tr√®s utile pour manipuler les donn√©es d‚Äôimages ou de s√©quences.</li>
</ul>

<pre><code class="python hljs"># Exemple avec un tenseur 3D (batch, hauteur, largeur)
t = torch.randn(2, 3, 4)  # forme (2, 3, 4)
print(&quot;Tenseur original :&quot;, t.shape)

# Inverser l&#039;ordre (largeur, hauteur, batch)
p = t.permute(2, 1, 0)
print(&quot;Apr√®s permute :&quot;, p.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.9.3"></a><h3>8.3. Ajouter ou supprimer des dimensions : <code>unsqueeze</code> et <code>squeeze</code></h3>
<ul><li class="dash"><code>unsqueeze(dim)</code> : ajoute une dimension de taille 1 √† la position <code>dim</code>.</li>
<li class="dash"><code>squeeze()</code> : supprime toutes les dimensions de taille 1.</li>
</ul>

<pre><code class="python hljs">v = torch.tensor([1, 2, 3])
print(&quot;Forme initiale :&quot;, v.shape)

v_unsq = v.unsqueeze(0)  # ajoute une dimension au d√©but
print(&quot;Apr√®s unsqueeze(0) :&quot;, v_unsq.shape)

v_sq = v_unsq.squeeze()  # supprime les dimensions de taille 1
print(&quot;Apr√®s squeeze() :&quot;, v_sq.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.9.4"></a><h3>8.4. Concat√©ner ou empiler des tenseurs</h3>
<ul><li class="dash"><code>torch.cat</code> : concat√®ne le long d‚Äôune dimension existante.</li>
<li class="dash"><code>torch.stack</code> : empile en ajoutant une nouvelle dimension.</li>
</ul>

<pre><code class="python hljs">a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

cat = torch.cat((a, b), dim=0)
print(&quot;torch.cat :&quot;, cat)

stack = torch.stack((a, b), dim=0)
print(&quot;torch.stack :&quot;, stack)
print(&quot;Forme de stack :&quot;, stack.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.10"></a><h2>üìñ 9. Autograd avec PyTorch</h2>
<p>En Deep Learning, nous travaillons souvent avec des fonctions compliqu√©es d√©pendant de plusieurs variables. Pour entra√Æner un mod√®le, nous avons besoin de calculer automatiquement les d√©riv√©es de ces fonctions. C&#039;est l√† qu&#039;intervient Autograd qui est le moteur de diff√©rentiation automatique de PyTorch. </p>
<a id="title.1.10.1"></a><h3>9.1. Cr√©ation d&#039;un tenseur suivi</h3>
<p>Pour qu&#039;un tenseur suive les op√©rations et calcule les gradients automatiquement, il faut d√©finir <code>requires_grad=True</code> :</p>
<pre><code class="python hljs">x = torch.tensor([2.0, 3.0], requires_grad=True)
print(x)</code></pre>
<p>Ici, <code>x</code> est maintenant un tenseur avec suivi des gradients. Toutes les op√©rations futures sur ce tenseur seront enregistr√©es pour pouvoir calculer les d√©riv√©es automatiquement.</p>
</div><div class="slide ">
<a id="title.1.10.2"></a><h3>9.2. Op√©rations sur les tenseurs</h3>
<p>Toutes les op√©rations effectu√©es sur ce tenseur sont automatiquement enregistr√©es dans un graphe computationnel dynamique.</p>
<pre><code class="python hljs">y = x ** 2 + 3 * x # y = [y1, y2]
print(y)</code></pre>
<p>Dans ce cas :</p>
<ul><li class="dash"><code>x</code> est la variable d&#039;entr√©e.</li>
<li class="dash"><code>y</code> est calcul√© √† partir de <code>x</code> avec les op√©rations <code>x**2</code> et <code>3*x</code>.</li>
</ul>

<p>Chaque op√©ration devient un n≈ìud du graphe et PyTorch garde la trace des d√©pendances pour pouvoir calculer les gradients.</p>
</div><div class="slide ">
<a id="title.1.11"></a><h2>üìñ 10. Graphe computationnel</h2>
<p>Un graphe computationnel est une structure qui repr√©sente toutes les op√©rations effectu√©es sur les tenseurs. Chaque n≈ìud du graphe correspond √† un tenseur ou √† une op√©ration math√©matique, et les ar√™tes indiquent les d√©pendances entre eux.</p>
<a id="title.1.11.1"></a><h3>10.1. <code>torchviz</code></h3>
<p>Pour visualiser le graphe dans PyTorch, on peut utiliser <code>torchviz</code> (qu&#039;il faudra installer avec <code>pip install torchviz</code>)  :</p>
<pre><code class="python hljs">from torchviz import make_dot

z = y.sum()
make_dot(z, params={&#039;x&#039;: x})</code></pre>
<p>Cela produira une image avec des n≈ìuds pour chaque op√©ration et des fl√®ches indiquant les d√©pendances :</p>
<ul><li class="dash">Les n≈ìuds <code>x^2</code> et <code>3*x</code> repr√©sentent les op√©rations effectu√©es sur <code>x</code>.</li>
<li class="dash">Le n≈ìud <code>y</code> combine ces deux r√©sultats.</li>
<li class="dash">Le graphe permet √† PyTorch de savoir quelles d√©riv√©es calculer et dans quel ordre.</li>
</ul>

</div><div class="slide ">
<a id="title.1.11.2"></a><h3>10.2. Note sur le graphe g√©n√©r√© par PyTorch</h3>
<p>Quand on visualise le graphe interne avec un outil comme <code>torchviz</code> :</p>
<ul><li class="dash">Le <strong>bloc jaune avec ()</strong> correspond au tenseur final (ici <code>z</code>).</li>
<li class="dash">Les <strong>blocs interm√©diaires</strong> (<code>PowBackward0</code>, <code>AddBackward0</code>, etc.) repr√©sentent
  les op√©rations qui seront diff√©renti√©es telles que <code>PowBackward0</code> est l&#039;op√©ration op√©ration inverse associ√©e √† <code>x**2</code>, <code>MulBackward0</code> celle associ√©e √† <code>3*x</code>,<br />  <code>AddBackward0</code> combine les deux r√©sultats et repr√©sente <code>y</code> et enfin <code>SumBackward0</code> correspond au <code>y.sum()</code> qui est √©gal √† <code>z</code>.</li>
<li class="dash">Le <strong>bloc <code>AccumulateGrad</code></strong> correspond √† l‚Äôendroit o√π le gradient est stock√©
  dans la variable d‚Äôentr√©e (ici <code>x.grad</code>).</li>
</ul>

</div><div class="slide ">
<a id="title.1.12"></a><h2>üìñ 11. Calcul des gradients et r√©tropropagation </h2>
<p>Autograd utilise ce graphe pour calculer automatiquement les d√©riv√©es par rapport √† <code>x</code>, en utilisant la m√©thode <code>backward()</code> :</p>
<pre><code class="python hljs">z = y.sum()  # z = y1 + y2
z.backward()
print(x.grad)</code></pre>
<ul><li class="dash"><code>backward()</code> calcule les d√©riv√©es de <code>z</code> par rapport √† chaque √©l√©ment de <code>x</code>.</li>
<li class="dash"><code>x.grad</code> contient maintenant les gradients.</li>
</ul>

<a id="title.1.12.1"></a><h3>11.1. Principe de la r√©tropropagation</h3>
<p>Le principe de la r√©tropropagation signifie PyTorch parcourt le graphe <strong>en sens inverse</strong> pour faire le calcul des d√©riv√©es.</p>
<ol><li class="">Commence par la sortie <code>z</code>.</li>
<li class="">Recule vers les n≈ìuds pr√©c√©dents (<code>y</code> puis <code>x</code>) en appliquant la r√®gle de d√©rivation.</li>
<li class="">Stocke le gradient dans <code>x.grad</code>.</li>
</ol>

</div><div class="slide ">
<a id="title.1.12.2"></a><h3>11.2. Calcul des gradients dans notre exemple</h3>
<ul><li class="dash"><code>dz/dy = 1</code> car z = y.sum()</li>
<li class="dash"><code>dy/dx = d√©riv√©e de (x^2 + 3*x) = 2*x + 3</code></li>
<li class="dash"><code>dx = dz/dy * dy/dx = 2*x + 3</code></li>
</ul>

<p>On obtient donc :</p>
<pre><code class="python hljs">print(x.grad)  # tensor([7., 9.])</code></pre>
</div><div class="slide ">
<a id="title.1.12.3"></a><h3>11.3. D√©tail du calcul des gradients</h3>
<p>On a :</p>
<blockquote><p>y = [y1, y2] = [x1¬≤ + 3x1,  x2¬≤ + 3x2]
z = y1 + y2</p>
</blockquote>
<p><strong>√âtape 1 : d√©riv√©e de z par rapport √† y</strong></p>
<p>Comme z = y1 + y2, on a :</p>
<blockquote><p>dz/dy1 = 1 et<br />dz/dy2 = 1</p>
</blockquote>
<p>On peut regrouper sous forme vectorielle :</p>
<blockquote><p>dz/dy = [dz/dy1, dz/dy2] = [1, 1]</p>
</blockquote>
<p><strong>√âtape 2 : d√©riv√©e de y par rapport √† x</strong></p>
<blockquote><p>dy1/dx1 = 2<em>x1 + 3 et<br />dy2/dx2 = 2</em>x2 + 3</p>
</blockquote>
<p><strong>√âtape 3 : application de la r√®gle de la cha√Æne</strong></p>
<p>Pour chaque variable d‚Äôentr√©e :</p>
<blockquote><p>dz/dx1 = dz/dy1 <em> dy1/dx1 = 1 </em> (2<em>x1 + 3) et<br />dz/dx2 = dz/dy2 </em> dy2/dx2 = 1 <em> (2</em>x2 + 3)</p>
</blockquote>
</div><div class="slide ">
<a id="title.1.12.4"></a><h3>11.4. R√©sultat num√©rique pour notre exemple* </h3>
<pre><code class="python hljs">print(x)       # tensor([2., 3.], requires_grad=True)
print(x.grad)  # tensor([7., 9.])</code></pre>
<p>Car :</p>
<ul><li class="dash">Pour x1 = 2 ‚Üí dz/dx1 = 2*2 + 3 = 7</li>
<li class="dash">Pour x2 = 3 ‚Üí dz/dx2 = 2*3 + 3 = 9</li>
</ul>

<p>Ainsi, Autograd reproduit automatiquement ce calcul gr√¢ce au graphe computationnel et √† la r√®gle de la cha√Æne.</p>
</div><div class="slide ">
<a id="title.1.13"></a><h2>üìñ 12. D√©sactivation du suivi des gradients</h2>
<p>Pour certaines op√©rations, par exemple lors de l&#039;√©valuation d&#039;un mod√®le, il est inutile
de calculer les gradients. On peut alors d√©sactiver le suivi avec <code>torch.no_grad()</code> :</p>
<pre><code class="python hljs">with torch.no_grad():
    z = x * 2
print(z)</code></pre>
<p>Cela permet d&#039;√©conomiser de la m√©moire et d&#039;acc√©l√©rer les calculs.</p>
</div><div class="slide ">
<a id="title.1.14"></a><h2>üìñ 13. Les fonctions de perte (Loss Functions)</h2>
<p>Lorsqu‚Äôon entra√Æne un r√©seau de neurones, l‚Äôobjectif est de minimiser l‚Äôerreur entre les pr√©dictions du mod√®le et les valeurs attendues. Cette erreur est mesur√©e par une fonction de perte (loss function en anglais).</p>
<p>Une fonction de perte prend en entr√©e :</p>
<ul><li class="dash">la sortie du mod√®le (pr√©diction),</li>
<li class="dash">la valeur cible (label attendu),</li>
</ul>

<p>et retourne un nombre r√©el qui indique &quot;√† quel point le mod√®le s&#039;est tromp√©&quot;.</p>
<p>############################# Stop ICI #################################
############################# Stop ICI #################################
############################# Stop ICI #################################
############################# Stop ICI #################################</p>
</div><div class="slide ">
<a id="title.1.15"></a><h2>üìñ 14. Erreur quadratique moyenne (MSE)</h2>
<p>La fonction MSE (<em>Mean Squared Error</em>) est tr√®s utilis√©e en r√©gression :</p>
$$L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
<ul><li class="dash"><code>yi</code> est la valeur r√©elle (target),</li>
<li class="dash"><code>y^i</code> est la pr√©diction du mod√®le.</li>
</ul>

<pre><code class="python hljs">import torch
import torch.nn as nn

# Valeurs r√©elles et pr√©dictions
y_true = torch.tensor([2.0, 3.0, 4.0])
y_pred = torch.tensor([2.5, 2.7, 4.2])

# D√©finition de la fonction de perte MSE
loss_fn = nn.MSELoss()

# Calcul de la perte
loss = loss_fn(y_pred, y_true)
print(loss)  # valeur scalaire</code></pre>
<p>Ici, la perte est un <strong>scalaire</strong> (un seul nombre) qui r√©sume l‚Äôerreur moyenne.</p>
</div><div class="slide ">
<a id="title.1.16"></a><h2>13.2 Exemple avec la classification : Cross-Entropy Loss</h2>
<p>Pour les t√¢ches de <strong>classification</strong>, la perte la plus courante est la
<strong>Cross-Entropy Loss</strong>. <br />Elle compare la distribution de probabilit√©s pr√©dite (softmax) et la vraie classe.</p>
<pre><code class="python hljs">import torch
import torch.nn as nn

# On suppose 3 classes et une pr√©diction pour un seul exemple
y_pred = torch.tensor([[1.2, 0.8, -0.5]])  # logits
y_true = torch.tensor([0])  # la classe correcte est 0

loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(y_pred, y_true)
print(loss)</code></pre>
<p>Ici encore, `loss` est un nombre qui repr√©sente &quot;combien le mod√®le s&#039;est tromp√©&quot;.</p>
</div><div class="slide ">
<a id="title.1.17"></a><h2>13.3 Pourquoi la fonction de perte est essentielle ?</h2>
<ul><li class="dash">Elle <strong>guide l‚Äôapprentissage</strong> : c‚Äôest en la minimisant que le mod√®le s‚Äôam√©liore.</li>
<li class="dash">Sans fonction de perte, le mod√®le n‚Äôaurait <strong>aucun signal</strong> pour savoir dans
  quelle direction ajuster ses param√®tres.</li>
<li class="dash">La fonction de perte est au c≈ìur de la <strong>r√©tropropagation</strong> vue pr√©c√©demment :
  c‚Äôest elle qui fournit le <strong>point de d√©part</strong> pour calculer les gradients.</li>
</ul>

</div>
<nav><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="chap0.html">&larr; Chapitre 0 - Installation des paquets et bibliothques n√©cessaires pour le cours</a></li><li class="page-item"><a class="page-link" href="index.html"><i class="bi bi-folder2"></i> Introduction aux fondamentaux de l&#039;apprentissage supervis√© et du Deep Learning</a></li></ul></nav>

</div> <!-- /contents -->
</div> <!-- /core -->

</body>
</html>
