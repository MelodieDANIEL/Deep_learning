üèãÔ∏è Travaux Pratiques 1
=========================
.. slide::
Sur cette page se trouvent des exercices de TP sur le Chapitre 1. Ils sont class√©s par niveau de difficult√© :
.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è


.. slide::
üçÄ Exercice 1 : Calculer le gradient d‚Äôune fonction simple avec PyTorch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante : $$f(a) = a^2 + a$$, avec $$a = 1.0$$.

**Consigne :** Faites les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† $$a$$ :

1) Calculez √† la main la d√©riv√©e de $$f$$ par rapport √† $$a$$. Puis √©valuez ce gradient pour $$a = 1.0$$.  

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.

**Astuce :**
.. spoiler::
    .. discoverList::
        La d√©riv√©e de $$f(a)$$ par rapport √† $$a$$ est √©gale √† $$2a + 1$$

**R√©sultat attendu :**

Le gradient est √©gal √† 3 dans les deux cas.



.. slide::
‚öñÔ∏è Exercice 2 : Trouver la droite qui passe au mieux par les donn√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter une **boucle d'entra√Ænement simple** pour ajuster les param√®tres d'une droite 
aux donn√©es fournies.

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es bruit√©es suivantes
    import numpy as np
    x = np.random.rand(1000)
    y_true = x * 1.54 + 12.5 + np.random.rand(1000)*0.2
    

**Objectif :** Trouver une droite de la forme

.. math::

    y = f(x) =a x + b

o√π : $$a$$ et $$b$$ sont des param√®tres appris automatiquement en minimisant l'erreur  entre les pr√©dictions du mod√®le et les donn√©es r√©elles.

**Consigne :** √âcrire un programme qui ajuste les param√®tres $$a$$ et $$b$$ de la droite aux donn√©es fournies en utilisant  PyTorch.

    1) Dans un premier temps, vous pouvez faire une boucle de 10000 it√©rations et coder vous-m√™me la fonction de perte.

    2) Afficher les param√®tres appris $$a$$ et $$b$$.

    3) Ensuite, trouver un moyen plus intelligent d'arr√™ter l'entra√Ænement de tel sorte √† ce que le mod√®le converge avec le minimum d'it√©rations.
    
    4) Afficher le nombre d'it√©rations n√©cessaires pour converger.
    
    5) Tracer les donn√©es r√©elles et les donn√©es pr√©dites pour comparer visuellement le r√©sultat.

    6) Utiliser la fonction de perte MSE fournie par PyTorch et afficher les param√®tres appris $$a$$ et $$b$$.

    7) V√©rifier que le r√©sultat des param√®tres et le trac√© sont similaires √† ceux obtenus avec la boucle d'entra√Ænement manuelle.


**Remarque :** Pour utiliser ``matplotlib``, vous devez l'installer avec la commande suivante :

.. code-block:: bash
    pip install matplotlib

Puis, vous pouvez l'importer dans votre code avec :

.. code-block:: python
    import matplotlib.pyplot as plt
    %matplotlib inline #√Ä ajouter si vous utilisez Jupyter Notebook



**Astuce :**
.. spoiler::
    .. discoverList::
        1. Initialiser les param√®tres : $$a$$ et $$b$$ √† z√©ro.
        2. Utiliser une fonction de perte en codant l'√©quation de la MSE (loss = torch.sum((y_pred - y_true) ** 2)).
        3. Impl√©menter une boucle d'entra√Ænement (par exemple 10000 it√©rations) avec l'optimiseur ADAM ``torch.optim.ADAM``.
        4. √Ä chaque it√©ration :
            - calculer les pr√©dictions,
            - calculer la perte,
            - effectuer la r√©tropropagation,
            - mettre √† jour les param√®tres :$$a$$ et $$b$$.

        5. Il faut arr√™ter l'entra√Ænement lorsque la perte est suffisamment faible (par exemple, inf√©rieure √† 0.01)

**R√©sultat attendu :**

Vous devez obtenir un graphique o√π :  
- les points bleus correspondent aux donn√©es r√©elles (``y_true``),  
- et une droite rouge correspond aux pr√©dictions (``y_pred``).  

Exemple d‚Äôaffichage attendu :

.. image:: images/chap1_exo_2_resultat.png
    :alt: droite ajust√©e aux points
    :align: center




################# Stop ICI #############################

################# Stop ICI #############################

################# Stop ICI #############################

################# Stop ICI #############################

