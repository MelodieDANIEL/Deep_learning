<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<link rel="stylesheet" type="text/css" href="css/style.css" />
<title>Intro Python - üèãÔ∏è Travaux Pratiques 2</title>

                <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="viewport-fit=cover, width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            

                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            
<link href="https://fonts.googleapis.com/css2?family=Gentium+Basic&display=swap" rel="stylesheet"> 
<link rel="stylesheet" type="text/css" href="slidey/bootstrap/css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" href="slidey/bootstrap-icons/font/bootstrap-icons.css" />
<link rel="stylesheet" type="text/css" href="slidey/highlight.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.menu.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.step.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.code.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.slide.css" />
<script type="text/javascript" src="slidey/js/jquery.js"></script>
<script type="text/javascript" src="slidey/js/slidey.permalink.js"></script>
<script type="text/javascript" src="slidey/js/slidey.menu.js"></script>
<script type="text/javascript" src="slidey/js/slidey.mobile.js"></script>
<script type="text/javascript" src="slidey/js/slidey.spoilers.js"></script>
<script type="text/javascript" src="slidey/js/slidey.steps.js"></script>
<script type="text/javascript" src="slidey/js/slidey.js"></script>
<script type="text/javascript" src="slidey/js/main.js"></script>
<script type="text/javascript" src="slidey/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="slidey/highlight/highlight.pack.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico" />
</head>
<body>
<!-- Modal log-in window -->
<div class="modal fade" id="loginWindow">
  <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
            <h5 class="modal-title">Log-in</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>    
        <div class="modal-body">
            <form>
                Mot de passe&nbsp;:<br />
                <input class="form-control" type="password" name="password" />
            </form>
        </div>
      </div><!-- /.modal-content -->
    </div><!-- /.modal-dialog -->
</div>

<div class="core">

    <script>hljs.highlightAll();</script>

<!-- Extra-controls for mobile device -->
<div class="mobileControls">
    <div class="btn-group">
        <a href="javascript:void(0)" class="left btn btn-light btn-lg">
            <i class="bi bi-arrow-up"></i>
        </a>
        <a href="javascript:void(0)" class="right btn btn-light btn-lg">
            <i class="bi bi-arrow-down"></i>
        </a>
        <a href="javascript:void(0)" class="up btn btn-light btn-lg">
            <i class="bi bi-arrow-left"></i>
        </a>
        <a href="javascript:void(0)" class="down btn btn-light btn-lg">
            <i class="bi bi-arrow-right"></i>
        </a>
        <a href="javascript:void(0)" class="login btn btn-light btn-lg">
            <i class="bi bi-lock"></i>
        </a>
    </div>
</div>

<!-- Controls -->
<div class="slideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg slideModeSlide"><i class="bi bi-film"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i> </a>
        <a href="index.html" class="btn btn-light btn-lg goHome"><i class="bi bi-house"></i></a>
    </div>
</div>

<div class="exitSlideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg showMobile"><i class="bi bi-phone"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg stopShow">
            <span class="currentSlideNumber">
            </span>
            <i class="bi bi-x-lg"></i>
        </a>
    </div>
</div>

<!-- Browsing menu -->
<div class="menu">
</div>

<div class="contents container">

<a id="title.1"></a><h1>üèãÔ∏è Travaux Pratiques 2</h1>
<div class="slide ">
<p>Sur cette page se trouvent des exercices de TP sur le Chapitre 2. Ils sont class√©s par niveau de difficult√© :</p>
<ul><li class="discover">Facile : üçÄ</li>
<li class="discover">Moyen : ‚öñÔ∏è</li>
<li class="discover">Difficile : üå∂Ô∏è</li>
</ul>

</div><div class="slide ">
<a id="title.1.1"></a><h2>üçÄ Exercice 1 : Approximations d‚Äôune fonction non lin√©aire</h2>
<p>Dans cet exercice, vous allez impl√©menter une boucle d&#039;entra√Ænement simple pour ajuster les param√®tres d‚Äôun mod√®le polyn√¥mial comme dans le chapitre 1, puis comparer les r√©sultats avec ceux d&#039;un mod√®le MLP.</p>
<p>On vous donne les donn√©es suivantes :</p>
<pre><code class="python hljs">torch.manual_seed(0)

X = torch.linspace(-3, 3, 100).unsqueeze(1)
y_true = torch.sin(X) + 0.1 * torch.randn(X.size())  # fonction sinuso√Ødale bruit√©e</code></pre>
<p><strong>Objectif :</strong> Comparer deux mod√®les pour approximer la fonction :</p>
<ol><li class="">Polyn√¥me cubique : \(y = f(x) = a x^3 + b x^2 + c x + d\), o√π \(a, b, c, d\) sont des param√®tres appris automatiquement en minimisant l‚Äôerreur entre les pr√©dictions et les donn√©es r√©elles comme dans le chapitre 1.</li>
<li class="">MLP simple :</li>
<ul><li class="dash">Impl√©ment√© sous forme de classe <code>nn.Module</code></li>
<li class="dash">2 couches cach√©es de 10 neurones chacune avec `ReLU`` pour l&#039;activation</li>
<li class="dash">Entr√©e : 1 feature, sortie : 1 pr√©diction</li>
</ul>
</ol>

<p><strong>Consigne :</strong> √âcrire un programme qui :</p>
<p>1) Ajuste les param√®tres du polyn√¥me cubique aux donn√©es en utilisant PyTorch. <br />2) Affiche les param√®tres appris \(a, b, c, d\). <br />3) Impl√©mente ensuite un MLP et entra√Æne-le sur les m√™mes donn√©es pendant 5000 epochs avec un learning rate de 0.01. <br />4) Compare visuellement les deux mod√®les avec les donn√©es r√©elles sur un m√™me graphique.<br />5) Que remarquez-vous sur les performances des deux mod√®les ?
6) Que se passe-t-il si vous augmentez le nombre du polyn√¥me ?</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ol><li class="discover">Initialiser les param√®tres du polyn√¥me avec <code>torch.randn(1, requires_grad=True)</code>.</li>
<li class="discover">Utiliser <code>nn.MSELoss()</code> comme fonction de perte pour les deux mod√®les.</li>
<li class="discover">Pour le MLP, cr√©er une classe h√©ritant de <code>nn.Module</code> et d√©finir <code>forward</code>.</li>
<li class="discover">Utiliser <code>optimizer.zero_grad()</code>, <code>loss.backward()</code>, <code>optimizer.step()</code> √† chaque it√©ration.</li>
<li class="discover">On voit que le MLP parvient √† mieux s&#039;adapter aux donn√©es, car il peut capturer des relations non lin√©aires plus complexes.</li>
</ol>

</div>
<p><strong>R√©sultat attendu :</strong> Vous devez obtenir un graphique similaire √† celui ci-dessous o√π :  </p>
<ul><li class="dash">les points bleus correspondent aux donn√©es r√©elles (<code>y_true</code>)</li>
<li class="dash">la courbe rouge correspond au polyn√¥me cubique</li>
<li class="dash">la courbe verte correspond au MLP</li>
</ul>

<img src="images/chap2_exo_1_resultat.png"  alt="R√©sultat Exercice 1" align="center" />
</div><div class="slide ">
<a id="title.1.2"></a><h2>‚öñÔ∏è Exercice 2 : Comparaison de l&#039;entra√Ænement d&#039;un MLP sur donn√©es brutes et standardis√©es</h2>
<p>Dans cet exercice, vous allez entra√Æner un MLP simple sur un jeu de donn√©es synth√©tiques avec deux features ayant des √©chelles diff√©rentes. Vous comparerez les performances lorsque les donn√©es sont brutes ou standardis√©es.</p>
<p>On vous donne les donn√©es suivantes :</p>
<pre><code class="python hljs"># Donn√©es synth√©tiques
N = 500
X1 = torch.linspace(0, 1, N).unsqueeze(1)      # petite amplitude
X2 = torch.linspace(0, 100, N).unsqueeze(1)    # grande amplitude
X = torch.cat([X1, X2], dim=1)
y = 3*X1 + 0.05*X2**2 + torch.randn(N,1) * 0.5</code></pre>
<p><strong>Objectif :</strong> <br />Comprendre l‚Äôimportance de la standardisation des donn√©es pour l‚Äôentra√Ænement d‚Äôun r√©seau de neurones et observer l‚Äô√©volution de la loss.</p>
<p><strong>Consigne :</strong> √âcrire un programme qui :  </p>
<p>1) D√©finit une classe MLP simple sans couches cach√©es avec : </p>
<ul><li class="dash">une couche lin√©aire d‚Äôentr√©e (2 features) vers 20 neurones</li>
<li class="dash">une fonction d‚Äôactivation <code>ReLU</code></li>
<li class="dash">une couche de sortie avec 1 pr√©diction</li>
</ul>

<p>2) Cr√©e deux mod√®les : un pour les donn√©es brutes, un pour les donn√©es standardis√©es.  </p>
<p>3) Entra√Æne les deux mod√®les avec Adam et une fonction de perte MSE pendant 1000 epochs avec un learning rate de 0.01.</p>
<p>4) Stocke et trace l‚Äô√©volution de la loss pour les deux mod√®les.  </p>
<p>5) Trace les pr√©dictions finales des deux mod√®les sur le m√™me graphique que les donn√©es r√©elles.  </p>
<p>6) Comparez les performances des deux mod√®les et notez lequel converge plus vite et donne de meilleures pr√©dictions.</p>
<p>7) A quelle epoch peut-on consid√©rer que le mod√®le sur donn√©es standardis√©es a converg√© et comment on peut faire pour le d√©terminer ?</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ol><li class="discover">N‚Äôoubliez pas d initialiser les poids du mod√®le avec <code>torch.randn()</code> pour un d√©marrage al√©atoire et de  mettre <code>optimizer.zero_grad()</code> avant <code>loss.backward()</code>.</li>
<li class="discover">Pour standardiser, utilisez <code>(X - X_mean)/X_std</code>.</li>
<li class="discover">Pour visualiser la loss : stockez <code>loss.item()</code> √† chaque epoch et utilisez <code>matplotlib.pyplot.plot()</code>.</li>
<li class="discover">Pour visualiser les pr√©dictions, utilisez un scatter plot avec les donn√©es r√©elles et les pr√©dictions des deux mod√®les.</li>
<li class="discover">Pour savoir quand stopper l&#039;entra√Ænement, vous pouvez faire du Early Stopping.</li>
<li class="discover">Pour que l‚Äôearly stopping fonctionne correctement avec ce type de donn√©es, il est conseill√© de :</li>
<ul><li class="dash discover">Mettre le param√®tre <code>patience</code> √† 20.</li>
<li class="dash discover">Comparer la perte actuelle avec la meilleure perte pr√©c√©dente en utilisant un seuil de tol√©rance. Par exemple, arrondir la perte √† 5 pour consid√©rer une am√©lioration significative (<code>if loss.item() &lt; best_loss - 5</code>)</li>
</ul>
</ol>

</div>
<p><strong>R√©sultat attendu :</strong> <br />Le graphique montre les pr√©dictions du MLP sur les donn√©es brutes (rouge) et standardis√©es (bleu) par rapport aux donn√©es r√©elles (noir).  Vous devez obtenir un r√©sultat similaire √† celui-ci avant de r√©duire le nombre d&#039;epochs :</p>
<img src="images/chap2_exo_2_resultat.png"  alt="Comparaison MLP brutes vs standardis√©es" align="center" />
</div><div class="slide ">
<a id="title.1.3"></a><h2>üå∂Ô∏è Exercice 3 : Overfitting et g√©n√©ralisation</h2>
<p>Cet exercise permet d&#039;observer l&#039;overfitting avec un MLP sur des donn√©es bruit√©es. L&#039;overfitting se produit lorsque le mod√®le apprend trop bien les d√©tails des donn√©es d&#039;entra√Ænement, au d√©triment de sa capacit√© √† g√©n√©raliser sur de nouvelles donn√©es.</p>
<p><strong>Objectif :</strong></p>
<ul><li class="dash">Comparer un MLP de petite taille et un MLP de grande taille.</li>
<li class="dash">Observer ce qui se passe si on entra√Æne trop longtemps un petit MLP.</li>
<li class="dash">Visualiser comment la complexit√© du mod√®le et le bruit des donn√©es influencent la qualit√© des pr√©dictions.</li>
<li class="dash">Tester les mod√®les sur de nouvelles donn√©es.</li>
</ul>

<p><strong>Consigne :</strong> √âcrire un programme qui :  </p>
<p>1) G√©n√©rer un jeu de donn√©es 1D avec <code>N=100</code> points :  </p>
<ul><li class="dash"><code>X</code> uniform√©ment dans \([-3,3]\).</li>
<li class="dash"><code>y = sin(X) + bruit</code> avec <code>bruit = 0.2 * torch.randn_like(y)</code>.</li>
</ul>

<p>2) D√©finir trois mod√®les MLP avec <code>Tanh</code> comme activation :  </p>
<ul><li class="dash">Petit : 2 couches cach√©es de 5 neurones chacune</li>
<li class="dash">Petit entra√Æn√© longtemps : m√™me architecture, mais entra√Æn√© avec plus d‚Äôepochs</li>
<li class="dash">Grand : 2 couches cach√©es de 50 neurones chacune</li>
</ul>

<p>3) Entra√Æner chaque mod√®le avec <code>MSELoss</code> et Adam pendant :  </p>
<ul><li class="dash">Petit : 2000 epochs</li>
<li class="dash">Petit long : 10000 epochs</li>
<li class="dash">Grand : 2000 epochs</li>
</ul>

<p>4) Tracer sur le m√™me graphique :  </p>
<ul><li class="dash">Les points de donn√©es bruit√©es</li>
<li class="dash">La fonction vraie `sin(X)`</li>
<li class="dash">Les pr√©dictions des trois MLP</li>
</ul>

<p>5) Tracer √©galement l‚Äô√©volution de la loss pour chaque mod√®le.</p>
<p>6) Tester les mod√®les sur une nouvelle valeur de X (ex. X=0.5) et afficher les pr√©dictions et la valeur vraie.</p>
<p><strong>Questions :</strong></p>
<p>7) Que remarquez-vous sur la capacit√© de g√©n√©ralisation du MLP petit vs grand ? <br />8) Que se passe-t-il si on augmente encore le nombre d‚Äôepochs pour le MLP petit ? <br />9) Quel r√¥le joue le bruit dans la difficult√© de l‚Äôapprentissage ? <br />10) Comment pourrait-on am√©liorer la g√©n√©ralisation des mod√®les (pistes) ?
11) Pouvez vous √©crire du code pour √©viter de l&#039;overfitting ?</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ol><li class="discover">Utiliser <code>torch.manual_seed(0)</code> pour la reproductibilit√©.</li>
<li class="discover">Pour l‚Äôentra√Ænement, penser √† <code>optimizer.zero_grad()</code>, <code>loss.backward()</code>, <code>optimizer.step()</code>.</li>
<li class="discover">Stocker les losses √† chaque epoch pour pouvoir les tracer ensuite.</li>
<li class="discover">Pour la nouvelle valeur de test, utiliser <code>with torch.no_grad()</code>.</li>
<li class="discover">Faire de l&#039;Early Stopping pour pr√©venir l&#039;overfitting.</li>
</ol>

</div>
<p><strong>R√©sultats attendus :</strong></p>
<ul><li class="dash">Voici un exemple de graphique attendu pour les pr√©dictions des trois mod√®les par rapport aux donn√©es bruit√©es et √† la fonction vraie :</li>
</ul>

<img src="images/chap2_exo_3_resultat.png"  alt="Comparaison MLP petit vs grand" align="center" />
<ul><li class="dash">Les pr√©dictions sur la nouvelle valeur permettent de comparer la capacit√© de g√©n√©ralisation des mod√®les. Vous devriez obtenir des r√©sultats similaires √† ceux-ci:
    Pour X = 0.50 :
        MLP petit = 0.5706, MLP petit entra√Æn√© longtemps = 0.7065, MLP grand = 0.7116 et Valeur vraie = 0.4794.</li>
</ul>

<p><strong>R√©usltat pour √©viter l&#039;overfitting :</strong></p>
<div class="spoiler"><blockquote><img src="images/chap2_exo_3_suite_resultat.png"  />
<p>:alt: Comparaison MLP petit vs grand
:align: center</p>
</blockquote>
</div>
</div><div class="slide ">
<a id="title.2"></a><h1>üèãÔ∏è Exercices suppl√©mentaires 2</h1>
<p>Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.</p>
</div><div class="slide ">
<a id="title.2.1"></a><h2>‚öñÔ∏è Exercice suppl√©mentaire 1 : Approximation d&#039;une fonction 2D avec un MLP </h2>
<p>Cet exercise propose l&#039;entra√Ænement d&#039;un MLP avec des donn√©es en 2D.</p>
<p><strong>Objectif :</strong> Entra√Æner un MLP pour approximer la fonction suivante :</p>
$$y = \sin(X_1) + \cos(X_2)$$
<p>o√π $(X_1, X_2) \in [-2,2]^2$, et visualiser la pr√©diction du mod√®le par rapport √† la fonction r√©elle.</p>
<p><strong>Consigne :</strong>  </p>
<p>1) G√©n√©rer <code>N = 800</code> points al√©atoires \((X_1, X_2)\) dans \([-2,2]\) et calculer \(y\) en suivant la fonction.</p>
<p>2) Standardiser les entr√©es pour le MLP.</p>
<p>3) Cr√©er un MLP simple :</p>
<ul><li class="dash">Entr√©e : 2 features</li>
<li class="dash">2 couches cach√©es de 64 neurones avec activation <code>Tanh</code></li>
<li class="dash">Sortie : 1 pr√©diction</li>
</ul>

<p>4) Entra√Æner le mod√®le avec Adam et MSE loss pendant 1000 epochs.</p>
<p>5) Ajouter early stopping avec <code>patience = 20</code> et <code>tolerance = 0.1</code>.</p>
<p>6) Pr√©parer une grille 2D pour visualiser la fonction r√©elle et la pr√©diction du mod√®le.</p>
<p>7) Afficher sur une seule figure 3D* :</p>
<ul><li class="dash">Surface r√©elle en vert transparent</li>
<li class="dash">Surface pr√©dite par le MLP en orange semi-transparent</li>
<li class="dash">Ajouter une l√©gende pour distinguer les surfaces</li>
</ul>

<p>8) Tracer l&#039;√©volution de la loss pendant l&#039;entra√Ænement pour v√©rifier la convergence.</p>
<p>9) Refaire un test avec des donn√©es bruit√©es (ajouter un bruit gaussien de moyenne 0 et √©cart-type 0.6 √† y) et observer l&#039;impact sur la pr√©diction du MLP.</p>
<p><strong>Questions :</strong>  </p>
<p>10) Que remarquez-vous sur la capacit√© du MLP √† approximer la fonction sous-jacente malgr√© le bruit‚ÄØ? <br />11) Que se passe-t-il si vous augmentez ou diminuez le niveau de bruit‚ÄØ? <br />12) Comment l‚Äôearly stopping impacte-t-il l‚Äôapprentissage‚ÄØ?</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ol><li class="discover">Pour l&#039;early stopping, stocker la meilleure loss et un compteur d&#039;epochs sans am√©lioration.</li>
<li class="discover">Pour la visualisation, utiliser <code>ax.plot_surface</code> pour les surfaces et <code>Patch</code> pour la l√©gende.</li>
<li class="discover">La standardisation permet au MLP de mieux converger.</li>
<li class="discover">V√©rifier la loss finale pour s&#039;assurer que le mod√®le a appris correctement la fonction.</li>
<li class="discover">Pour g√©n√©rer le bruit, utilisez <code>0.6 * torch.randn_like(y_clean)</code>.</li>
</ol>

</div>
<p><strong>Astuce avanc√©e :</strong>        </p>
<div class="spoiler"><blockquote><p><strong>Voici le code pour la visualisation 3D avec Matplotlib :</strong></p>
<pre><code class="python hljs">x1g, x2g = torch.meshgrid(
torch.linspace(-2, 2, 80),
torch.linspace(-2, 2, 80),
indexing=&quot;ij&quot;
)

Xg = torch.cat([x1g.reshape(-1,1), x2g.reshape(-1,1)], dim=1)
Xg_std = (Xg - X_mean) / X_std

with torch.no_grad():
    y_true_grid = (torch.sin(x1g) + torch.cos(x2g))
    y_pred_grid = model(Xg_std).reshape(80, 80)
    y_pred_train = model(X_stdized)

fig = plt.figure(figsize=(9,7))
ax = fig.add_subplot(111, projection=&#039;3d&#039;)
ax.set_title(&quot;MLP 2D avec Early Stopping&quot;)
ax.set_xlabel(&quot;X1&quot;); ax.set_ylabel(&quot;X2&quot;); ax.set_zlabel(&quot;y&quot;)
ax.set_xlim(-2, 2); ax.set_ylim(-2, 2); ax.set_zlim(-2, 2)
try:
    ax.set_box_aspect((1,1,1))
except Exception:
    pass
ax.view_init(elev=25, azim=35)

ax.plot_surface(x1g.numpy(), x2g.numpy(), y_true_grid.numpy(),
                cmap=&quot;Greens&quot;, alpha=0.45, linewidth=0)
ax.plot_surface(x1g.numpy(), x2g.numpy(), y_pred_grid.numpy(),
                cmap=&quot;Oranges&quot;, alpha=0.70, linewidth=0)
legend_elements = [
    Patch(facecolor=&quot;tab:green&quot;, alpha=0.45, label=&quot;Surface vraie&quot;),
    Patch(facecolor=&quot;tab:orange&quot;, alpha=0.70, label=&quot;Surface MLP&quot;)
]
ax.legend(handles=legend_elements, loc=&quot;upper left&quot;)


plt.tight_layout()
plt.show()</code></pre>
</blockquote>
</div>
<p><strong>R√©sultats attendus :</strong></p>
<ul><li class="dash">Voici un exemple de la figure 3D attendue pour les points 1 √† 8 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :</li>
</ul>

<img src="images/chap2_exo_sup_1_resultat.png"  alt="R√©sultat attendu MLP 2D" align="center" />
<ul><li class="dash">Voici un exemple de la figure 3D attendue pour le point 9 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :</li>
</ul>

<img src="images/chap2_exo_sup_1_suite_resultat.png"  alt="R√©sultat attendu MLP 2D" align="center" />
</div><div class="slide ">
<a id="title.2.2"></a><h2>‚öñÔ∏è Exercice suppl√©mentaire 2 : Comparaison de deux MLP avec torchsummary</h2>
<p>Dans cet exercice, vous allez comparer deux MLP pour approximer une fonction non lin√©aire. L&#039;objectif est d&#039;observer l&#039;impact de la taille du r√©seau sur la performance et de comprendre comment <code>torchsummary</code> permet d&#039;√©valuer la structure du mod√®le.</p>
<p><strong>Objectif</strong> :</p>
<ul><li class="dash">Comprendre comment la taille et la complexit√© d&#039;un MLP influencent la qualit√© des pr√©dictions.</li>
<li class="dash">Utiliser <code>torchsummary</code> pour visualiser le nombre de param√®tres et la structure du r√©seau.</li>
<li class="dash">Comparer deux MLP sur une m√™me fonction et interpr√©ter leurs r√©sultats.</li>
</ul>

<p><strong>Consignes</strong> :</p>
<p>1) G√©n√©rer un jeu de donn√©es avec la fonction non lin√©aire suivante : </p>
<blockquote><pre><code class="python hljs">import torch
torch.manual_seed(0)
N = 200
X = torch.linspace(0, 3, N).unsqueeze(1)
y = torch.exp(X) + 0.1*torch.randn_like(X)  # fonction exponentielle bruit√©e</code></pre>
</blockquote>
<p>2) D√©finir deux MLP avec <code>nn.Module</code> et une activation <code>Tanh</code> :</p>
<ul><li class="dash"><strong>Petit MLP</strong> : 2 couches cach√©es de 5 neurones chacune</li>
<li class="dash"><strong>Grand MLP</strong> : 2 couches cach√©es de 50 neurones chacune</li>
</ul>

<p>3) Entra√Æner les deux mod√®les avec Adam et <code>nn.MSELoss()</code> pendant 2000 epochs et learning rate 0.01.</p>
<p>4) Utiliser <code>torchsummary</code> pour afficher la structure et le nombre de param√®tres de chaque mod√®le.</p>
<p>5) Tracer les pr√©dictions des deux MLP sur le m√™me graphique ainsi que la fonction vraie.</p>
<p>6) Comparer les performances et interpr√©ter les r√©sultats √† l‚Äôaide du r√©sum√© des mod√®les.</p>
<p><strong>Astuce avanc√©e :</strong>        </p>
<div class="spoiler"><ul><li class="dash discover">Pour <code>torchsummary</code>, vous pouvez faire :</li>
</ul>

<pre><code class="python hljs">from torchsummary import summary
summary(model, input_size=(1,))</code></pre>
<ul><li class="dash">Stockez les pertes √† chaque epoch pour tracer l&#039;√©volution et v√©rifier la convergence.</li>
<li class="dash">Le petit MLP a moins de param√®tres et risque moins de sur-apprentissage, mais peut √™tre limit√© pour des fonctions tr√®s complexes.</li>
<li class="dash">Le grand MLP peut sur-apprendre le bruit si le dataset est petit ou bruit√©.</li>
</ul>

</div>
<p><strong>R√©sultats attendus :</strong></p>
<ul><li class="dash">Une figure montrant les pr√©dictions des deux MLP et la fonction vraie comme celle ci-dessous.</li>
<li class="dash">Le r√©sum√© des mod√®les avec le nombre de param√®tres et la structure (torchsummary).</li>
<li class="dash">Discussion : quel MLP capture mieux la fonction ?</li>
</ul>

<img src="images/chap2_exo_sup_2_resultat.png"  alt="R√©sultat attendu MLP" align="center" />
</div>
<nav><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="chap2.html"><i class="bi bi-folder2"></i> Chapitre 2 ‚Äî Perceptron multi-couches </a></li></ul></nav>

</div> <!-- /contents -->
</div> <!-- /core -->

</body>
</html>
