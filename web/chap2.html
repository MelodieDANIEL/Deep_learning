<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<link rel="stylesheet" type="text/css" href="css/style.css" />
<title>Intro Python - Chapitre 2 ‚Äî Perceptron multi-couches </title>

                <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="viewport-fit=cover, width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            

                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            
<link href="https://fonts.googleapis.com/css2?family=Gentium+Basic&display=swap" rel="stylesheet"> 
<link rel="stylesheet" type="text/css" href="slidey/bootstrap/css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" href="slidey/bootstrap-icons/font/bootstrap-icons.css" />
<link rel="stylesheet" type="text/css" href="slidey/highlight.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.menu.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.step.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.code.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.slide.css" />
<script type="text/javascript" src="slidey/js/jquery.js"></script>
<script type="text/javascript" src="slidey/js/slidey.permalink.js"></script>
<script type="text/javascript" src="slidey/js/slidey.menu.js"></script>
<script type="text/javascript" src="slidey/js/slidey.mobile.js"></script>
<script type="text/javascript" src="slidey/js/slidey.spoilers.js"></script>
<script type="text/javascript" src="slidey/js/slidey.steps.js"></script>
<script type="text/javascript" src="slidey/js/slidey.js"></script>
<script type="text/javascript" src="slidey/js/main.js"></script>
<script type="text/javascript" src="slidey/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="slidey/highlight/highlight.pack.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico" />
</head>
<body>
<!-- Modal log-in window -->
<div class="modal fade" id="loginWindow">
  <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
            <h5 class="modal-title">Log-in</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>    
        <div class="modal-body">
            <form>
                Mot de passe&nbsp;:<br />
                <input class="form-control" type="password" name="password" />
            </form>
        </div>
      </div><!-- /.modal-content -->
    </div><!-- /.modal-dialog -->
</div>

<div class="core">

    <script>hljs.highlightAll();</script>

<!-- Extra-controls for mobile device -->
<div class="mobileControls">
    <div class="btn-group">
        <a href="javascript:void(0)" class="left btn btn-light btn-lg">
            <i class="bi bi-arrow-up"></i>
        </a>
        <a href="javascript:void(0)" class="right btn btn-light btn-lg">
            <i class="bi bi-arrow-down"></i>
        </a>
        <a href="javascript:void(0)" class="up btn btn-light btn-lg">
            <i class="bi bi-arrow-left"></i>
        </a>
        <a href="javascript:void(0)" class="down btn btn-light btn-lg">
            <i class="bi bi-arrow-right"></i>
        </a>
        <a href="javascript:void(0)" class="login btn btn-light btn-lg">
            <i class="bi bi-lock"></i>
        </a>
    </div>
</div>

<!-- Controls -->
<div class="slideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg slideModeSlide"><i class="bi bi-film"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i> </a>
        <a href="index.html" class="btn btn-light btn-lg goHome"><i class="bi bi-house"></i></a>
    </div>
</div>

<div class="exitSlideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg showMobile"><i class="bi bi-phone"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg stopShow">
            <span class="currentSlideNumber">
            </span>
            <i class="bi bi-x-lg"></i>
        </a>
    </div>
</div>

<!-- Browsing menu -->
<div class="menu">
</div>

<div class="contents container">

<div class="slide ">
<a id="title.1"></a><h1>Chapitre 2 ‚Äî Perceptron multi-couches </h1>
<a id="title.1.1"></a><h2>üéØ Objectifs du Chapitre</h2>
<div class="alert alert-light p-2 fs-5 text-center"><p>√Ä la fin de cette section,  vous saurez :  </p>
<ul><li class="dash">Le fonctionnement du perceptron simple.</li>
<li class="dash">Utiliser une fonction d&#039;activation adapt√©e.</li>
<li class="dash">L‚Äôimportance de la normalisation / standardisation des donn√©es et l&#039;usage des epochs.</li>
<li class="dash">Construire un r√©seau de neurones avec <code>torch.nn</code>.</li>
<li class="dash">Faire un entra√Ænement simple d‚Äôun MLP pour un probl√®me de r√©gression.</li>
<li class="dash">Suivre l‚Äô√©volution de la loss et interpr√©ter les r√©sultats.</li>
<li class="dash">Utiliser <code>torch-summary</code> pour inspecter l‚Äôarchitecture du r√©seau.</li>
</ul>

</div>
</div><div class="slide ">
<a id="title.1.2"></a><h2>üìñ 1. Rappels sur les perceptrons</h2>
<p>Le perceptron multi-couches (MLP de Multi-Layers Perceptron en anglais) est la brique de base des r√©seaux de neurones modernes. Dans ce chapitre, nous allons l‚Äôappliquer √† des probl√®mes de r√©gression simple. Avant de commencer, voici quelques rappels.</p>
<a id="title.1.2.1"></a><h3>1.1. Perceptron simple</h3>
<p>Le perceptron est le bloc de base d‚Äôun r√©seau de neurones. Il r√©alise une transformation lin√©aire suivie (ou pas) d‚Äôune fonction d‚Äôactivation telle que :  </p>
<blockquote>$$y = \sigma(Wx + b)$$
<p>o√π : <br /> - \(y\) est la sortie du perceptron,<br /> - \(\sigma\) est une fonction d‚Äôactivation,<br /> - \(W\) est la matrice des poids, <br /> - \(b\) est le biais et <br /> - \(x\) est l&#039;ensemble des entr√©es du perceptron.</p>
</blockquote>
</div><div class="slide ">
<a id="title.1.2.2"></a><h3>1.2. Perceptron intuition</h3>
<img src="images/chap2_perceptron.png"  alt="perceptron" align="center" width="40%" />
<p>avec \(y= \sigma(x_1*w_1 + x_2*w_2 + ...+ x_i*w_i + ... + x_n*w_n + b)\)</p>
<p>üí° <strong>Intuition :</strong></p>
<ul><li class="dash">Chaque poids \(w_i\) mesure l‚Äôimportance de la caract√©ristique \(x_i\).</li>
<li class="dash">Le biais \(b\) d√©place la fronti√®re de d√©cision.</li>
<li class="dash">La fonction d‚Äôactivation permet d‚Äôintroduire de la non-lin√©arit√©, indispensable pour mod√©liser des relations complexes mais nous en parleront plus en d√©tails par la suite.</li>
</ul>

</div><div class="slide ">
<a id="title.1.2.3"></a><h3>1.3. Mise √† jour des param√®tres</h3>
<p>Un perceptron poss√®de deux types de <strong>param√®tres</strong> : les <strong>poids</strong> et le <strong>biais</strong>.  </p>
<p>Lors de l‚Äôentra√Ænement, on souhaite ajuster ces param√®tres pour am√©liorer les pr√©dictions du mod√®le.  Pour cela, il faut mettre √† jour les poids apr√®s avoir calcul√© la loss gr√¢ce √† la fonction de perte et le gradient gr√¢ce √† l&#039;optimiseur comme expliqu√© dans le chapitre pr√©c√©dent.  </p>
<p>Pour rappel, on met √† jours les param√®tres du mod√®le gr√¢ce √† l&#039;√©quation introduite dans le chapitre pr√©c√©dent. </p>
$$\theta \leftarrow \theta - \eta \, \nabla_\theta \mathcal{L}(\theta)$$
<p>o√π : <br />- \(\theta\) repr√©sente l‚Äôensemble des param√®tres du mod√®le (ici \(W\) et \(b\)), <br />- \(\mathcal{L}\) est la fonction de perte, <br />- \(\nabla_\theta \mathcal{L}\) est le gradient de la perte par rapport aux param√®tres, <br />- \(\eta\) est le taux d‚Äôapprentissage (learning rate en anglais).</p>
</div><div class="slide ">
<a id="title.1.2.4"></a><h3>1.4. Exemples d&#039;applications du perceptron simple</h3>
<p>Un perceptron simple ne peut r√©soudre que les probl√®mes lin√©airement s√©parables puisque en trouvant les param√®tres du mod√®le, le perceptron trace une droite dans le plan des entr√©es et s√©pare les points selon qu‚Äôils sont au-dessus ou en dessous de cette droite.</p>
<p><strong>Exemple 1 : porte logique ET</strong></p>
<table class="table table-striped"><tbody><tr><th>x‚ÇÅ</th><th>x‚ÇÇ</th><th>y=ET</th></tr><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table>
<p>Dans ce cas, une droite s√©pare bien les deux classes : <br />- la classe \(0\) (points en bas √† gauche, en haut √† gauche, en bas √† droite), <br />- la classe \(1\) (point en haut √† droite).  </p>
<p>Un perceptron simple peut donc apprendre cette fonction.</p>
</div><div class="slide ">
<p><strong>Exemple 2 : porte logique XOR</strong></p>
<table class="table table-striped"><tbody><tr><th>x‚ÇÅ</th><th>x‚ÇÇ</th><th>y=XOR</th></tr><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table>
<p>Ici, il est impossible de tracer une seule droite qui s√©pare correctement les classes. Autrement dit, XOR n‚Äôest pas lin√©airement s√©parable.  </p>
<img src="images/chap2_et_vs_xor.png"  alt="Repr√©sentation du XOR dans le plan (non-s√©parable lin√©airement)" align="center" width="300%" />
<p><strong>Conclusion :</strong> </p>
<ul><li class="dash">Le perceptron simple suffit pour des t√¢ches lin√©aires (comme ET, OU).</li>
<li class="dash">Pour r√©soudre des probl√®mes plus complexes comme XOR, il faut introduire plusieurs couches de neurones et des fonctions d‚Äôactivation non-lin√©aires : c‚Äôest le principe du <strong>perceptron multi-couches (MLP)</strong>.</li>
</ul>

</div><div class="slide ">
<a id="title.1.2.5"></a><h3>1.5. Faire un perceptron dans PyTorch</h3>
<p>Pour cr√©er un perceptron simple dans PyTorch, on peut utiliser la fonction <code>Linear</code> de <code>torch.nn</code>, qui impl√©mente une couche lin√©aire (ou affine) : \(y = Wx + b\). La fonction <code>Linear</code> prend en entr√©e le nombre d&#039;entr√©e \(x\) et le nombre de sortie \(y\).</p>
<pre><code class="python hljs">import torch
import torch.nn as nn

# Donn√©es ET
X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)
y = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)

# Mod√®le lin√©aire (perceptron)
model = nn.Linear(2,1)

# Loss function et optimiseur
loss_fc = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# Entra√Ænement
for _ in range(500):
    optimizer.zero_grad()
    loss = loss_fc(model(X), y)
    loss.backward()
    optimizer.step()

# R√©sultat
with torch.no_grad():
    print((model(X)).round())
    print(model.weight, model.bias)</code></pre>
<p><strong>Remarque</strong> : si maintenant on change les entr√©es et sorties pour le XOR, le mod√®le ne pourra pas apprendre correctement la fonction (les \(W\) restent √† 0 comme √† l&#039;initialisation). Vous pouvez faire le test pour v√©rifier.</p>
</div><div class="slide ">
<p>################################ STOP ICI ################################</p>
<p>################################ STOP ICI ################################</p>
<p>################################ STOP ICI ################################</p>
</div><div class="slide ">
<a id="title.1.3"></a><h2>üìñ 2. Fonction d&#039;activation</h2>
<p>Les fonctions d‚Äôactivation introduisent de la non-lin√©arit√© dans le mod√®le, ce qui permet de mieux capturer des relations complexes dans les donn√©es. Voici quelques fonctions d‚Äôactivation couramment utilis√©es :</p>
<ol><li class=""><strong>Sigmo√Øde</strong> : \(\sigma(x) = \frac{1}{1 + e^{-x}}\)</li>
<ul><li class="dash">Sortie entre 0 et 1.</li>
<li class="dash">Utilis√©e pour les probl√®mes de classification binaire.</li>
</ul>
<li class=""><strong>Tanh</strong> : \(\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</li>
<ul><li class="dash">Sortie entre -1 et 1.</li>
<li class="dash">Souvent utilis√©e dans les r√©seaux de neurones cach√©s.</li>
</ul>
<li class=""><strong>ReLU (Rectified Linear Unit)</strong> : \(\text{ReLU}(x) = \max(0, x)\)</li>
<ul><li class="dash">Sortie nulle pour les entr√©es n√©gatives.</li>
<li class="dash">Tr√®s utilis√©e dans les r√©seaux de neurones profonds en raison de sa simplicit√© et de son efficacit√©.</li>
</ul>
<li class=""><strong>Softmax</strong> : \(\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\)</li>
<ul><li class="dash">Transforme un vecteur en une distribution de probabilit√©.</li>
<li class="dash">Utilis√©e en sortie des mod√®les de classification multi-classes.</li>
</ul>
</ol>

</div><div class="slide ">
<a id="title.1.4"></a><h2>üìñ 3. Epoch</h2>
<p>Lorsqu‚Äôon entra√Æne un mod√®le, on doit pr√©senter plusieurs fois l‚Äôensemble des donn√©es d‚Äôapprentissage \(x\).</p>
<a id="title.1.4.1"></a><h3>3.1 D√©finitions importantes</h3>
<ul><li class="dash">It√©ration : mise √† jour des param√®tres apr√®s avoir trait√© un seul exemple ou un mini-batch.</li>
<li class="dash">Batch / mini-batch : sous-ensemble d‚Äôexemples utilis√©s pour calculer la mise √† jour.</li>
<li class="dash"> Epoch : passage complet sur toutes les donn√©es d‚Äôapprentissage.</li>
</ul>

<p><strong>Exemple</strong> :</p>
<p>Si vous avez 1000 exemples et que vous utilisez des mini-batchs de 100 : Une epoch correspond √† 10 it√©rations (1000 √∑ 100).</p>
<p>Apr√®s chaque epoch, chaque exemple a √©t√© utilis√© exactement une fois pour mettre √† jour les param√®tres.</p>
<a id="title.1.4.2"></a><h3>3.2 Pourquoi plusieurs epochs‚ÄØ?</h3>
<p>Au d√©but de l‚Äôentra√Ænement, le mod√®le fait souvent de grandes erreurs.</p>
<p>Chaque epoch permet aux poids et biais de s‚Äôajuster progressivement pour mieux pr√©dire les sorties.</p>
<p>En g√©n√©ral, plusieurs dizaines ou centaines d‚Äôepochs sont n√©cessaires pour que la loss se stabilise.</p>
<p>üí° Intuition : imaginez un perceptron comme un √©l√®ve qui apprend : il ne retient pas tout parfaitement du premier coup ; il faut plusieurs passages sur le m√™me exercice pour ma√Ætriser.</p>
</div><div class="slide ">
<ol><li class="">Normaliser / standardiser les donn√©es
-----------------------------</li>
</ol>

<p>Pourquoi normaliser ?  </p>
<ul><li class="dash">Les entr√©es de grande amplitude ralentissent l‚Äôapprentissage.</li>
<li class="dash">Normaliser permet de mettre toutes les features √† la m√™me √©chelle.</li>
</ul>

<p>Deux approches classiques :  </p>
<ul><li class="dash"><strong>Normalisation</strong> : valeurs entre 0 et 1.</li>
<li class="dash"><strong>Standardisation</strong> : moyenne 0, variance 1.</li>
</ul>

<p>Exemple avec scikit-learn :  </p>
<pre><code class="python hljs">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)</code></pre>
<ol><li class="">Utiliser <code>torch.nn</code> pour construire un MLP
------------------------------------------------</li>
<li class="dash"><code>Sequential</code> : permet d‚Äôempiler les couches facilement.</li>
<li class="dash"><code>Linear</code> : couche affine (Wx+b).</li>
<li class="dash">Fonctions d‚Äôactivation : donnent la non-lin√©arit√© (ex. <code>nn.ReLU()</code>).</li>
</ol>

<p>Exemple minimal d‚Äôun r√©seau :  </p>
<pre><code class="python hljs">import torch.nn as nn

model = nn.Sequential(
    nn.Linear(1, 10),   # entr√©e 1D -&gt; couche cach√©e 10 neurones
    nn.ReLU(),          
    nn.Linear(10, 1)    # sortie 1D (r√©gression)
)</code></pre>
<ol><li class="">Suivi de la loss et visualisation
-------------------------------------</li>
<li class="dash">Pendant l‚Äôentra√Ænement, enregistrer la loss √† chaque epoch pour voir si elle diminue.</li>
<li class="dash">Comparer <code>y_pred</code> et <code>y_true</code> avec Matplotlib.</li>
</ol>

<pre><code class="python hljs">import matplotlib.pyplot as plt

plt.plot(losses)              # courbe de la loss
plt.scatter(x, y_true)        # donn√©es r√©elles
plt.scatter(x, y_pred)        # pr√©dictions</code></pre>
<ol><li class="">Inspecter le mod√®le avec <code>torch-summary</code>
----------------------------------------------</li>
</ol>

<p>Permet de voir le nombre de param√®tres par couche et la structure du r√©seau.  </p>
<pre><code class="python hljs">from torchsummary import summary
summary(model, input_size=(1,))</code></pre>
<p>################################ Activation fonction ######################################
Parler de la softmax, relu , etc.
##########################################################################################</p>
<p>################################ MLP ######################################</p>
<p>faire une classe avec fonction pour les couches et une pour le forward comme :<br />..code-block:: python  <br />    class MLP(nn.Module):
        def __<a href="">init</a>(self, input_size, hidden_size, output_size):
            super(MLP, self).__<a href="">init</a>()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, output_size)</p>
<blockquote><p>def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x</p>
</blockquote>
<p>parler de dataset loader et parler de broadcasting ?</p>
<ul><li class="dash">parler de .detach() et .clone() ?</li>
<li class="dash">parler de autograd profiler.profile</li>
<li class="dash">parler de la gestion des outliers
##########################################################################################</li>
</ul>

</div>
<nav><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="chap1.html">&larr; Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</a></li><li class="page-item"><a class="page-link" href="index.html"><i class="bi bi-folder2"></i> Introduction aux fondamentaux de l&#039;apprentissage supervis√© et du Deep Learning</a></li></ul></nav>

</div> <!-- /contents -->
</div> <!-- /core -->

</body>
</html>
