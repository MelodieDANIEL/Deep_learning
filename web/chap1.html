<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<link rel="stylesheet" type="text/css" href="css/style.css" />
<title>Intro Python - Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</title>

                <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="viewport-fit=cover, width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            

                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            
<link href="https://fonts.googleapis.com/css2?family=Gentium+Basic&display=swap" rel="stylesheet"> 
<link rel="stylesheet" type="text/css" href="slidey/bootstrap/css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" href="slidey/bootstrap-icons/font/bootstrap-icons.css" />
<link rel="stylesheet" type="text/css" href="slidey/highlight.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.menu.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.step.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.code.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.slide.css" />
<script type="text/javascript" src="slidey/js/jquery.js"></script>
<script type="text/javascript" src="slidey/js/slidey.permalink.js"></script>
<script type="text/javascript" src="slidey/js/slidey.menu.js"></script>
<script type="text/javascript" src="slidey/js/slidey.mobile.js"></script>
<script type="text/javascript" src="slidey/js/slidey.spoilers.js"></script>
<script type="text/javascript" src="slidey/js/slidey.steps.js"></script>
<script type="text/javascript" src="slidey/js/slidey.js"></script>
<script type="text/javascript" src="slidey/js/main.js"></script>
<script type="text/javascript" src="slidey/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="slidey/highlight/highlight.pack.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico" />
</head>
<body>
<!-- Modal log-in window -->
<div class="modal fade" id="loginWindow">
  <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
            <h5 class="modal-title">Log-in</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>    
        <div class="modal-body">
            <form>
                Mot de passe&nbsp;:<br />
                <input class="form-control" type="password" name="password" />
            </form>
        </div>
      </div><!-- /.modal-content -->
    </div><!-- /.modal-dialog -->
</div>

<div class="core">

    <script>hljs.highlightAll();</script>

<!-- Extra-controls for mobile device -->
<div class="mobileControls">
    <div class="btn-group">
        <a href="javascript:void(0)" class="left btn btn-light btn-lg">
            <i class="bi bi-arrow-up"></i>
        </a>
        <a href="javascript:void(0)" class="right btn btn-light btn-lg">
            <i class="bi bi-arrow-down"></i>
        </a>
        <a href="javascript:void(0)" class="up btn btn-light btn-lg">
            <i class="bi bi-arrow-left"></i>
        </a>
        <a href="javascript:void(0)" class="down btn btn-light btn-lg">
            <i class="bi bi-arrow-right"></i>
        </a>
        <a href="javascript:void(0)" class="login btn btn-light btn-lg">
            <i class="bi bi-lock"></i>
        </a>
    </div>
</div>

<!-- Controls -->
<div class="slideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg slideModeSlide"><i class="bi bi-film"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i> </a>
        <a href="index.html" class="btn btn-light btn-lg goHome"><i class="bi bi-house"></i></a>
    </div>
</div>

<div class="exitSlideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg showMobile"><i class="bi bi-phone"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg stopShow">
            <span class="currentSlideNumber">
            </span>
            <i class="bi bi-x-lg"></i>
        </a>
    </div>
</div>

<!-- Browsing menu -->
<div class="menu">
</div>

<div class="contents container">

<div class="slide ">
<a id="title.1"></a><h1>Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</h1>
<a id="title.1.1"></a><h2>üéØ Objectifs du Chapitre</h2>
<div class="alert alert-light p-2 fs-5 text-center"><p>√Ä la fin de ce chapitre, vous saurez : </p>
<ul><li class="dash">Cr√©er et manipuler des tenseurs PyTorch sur CPU et GPU.</li>
<li class="dash">Calculer automatiquement les gradients √† l‚Äôaide de <code>autograd</code>.</li>
<li class="dash">D√©finir une fonction de co√ªt.</li>
<li class="dash">Utiliser un optimiseur pour ajuster les param√®tres d‚Äôun mod√®le.</li>
<li class="dash">Impl√©menter une boucle d&#039;entra√Ænement simple.</li>
</ul>

</div>
</div><div class="slide ">
<a id="title.1.2"></a><h2>üìñ 1. Qu&#039;est-ce que PyTorch ? </h2>
<p>PyTorch est une biblioth√®que Python de machine learning open-source d√©velopp√©e par Facebook (FAIR). Elle est con√ßue pour faciliter la cr√©ation et l&#039;entra√Ænement de mod√®les, en particulier dans le domaine du deep learning. </p>
<p>Elle repose principalement sur deux √©l√©ments :</p>
<p>A) Les <em>tenseurs</em>, des structures de donn√©es similaires aux tableaux NumPy (<code>ndarray</code>), mais avec des fonctionnalit√©s suppl√©mentaires pour :</p>
<ul><li class="dash">le calcul diff√©rentiel automatique,</li>
<li class="dash">l&#039;acc√©l√©ration GPU,</li>
<li class="dash">l‚Äôentra√Ænement de r√©seaux de neurones.</li>
</ul>

<p>B) Le module <code>autograd</code> permet de calculer automatiquement les gradients n√©cessaires √† l&#039;entra√Ænement des mod√®les, en suivant toutes les op√©rations effectu√©es sur les tenseurs.</p>
</div><div class="slide ">
<p>D&#039;autres biblioth√®ques Python similaires existent, comme :</p>
<ul><li class="dash">TensorFlow : d√©velopp√© par Google, tr√®s utilis√© pour des d√©ploiements √† grande √©chelle.</li>
<li class="dash">Keras : interface haut niveau de TensorFlow, plus simple mais moins flexible.</li>
<li class="dash">JAX : plus r√©cent, optimis√© pour la recherche et les calculs scientifiques √† haute performance.</li>
</ul>

</div><div class="slide ">
<p>Dans le cadre de ce cours, nous utiliserons PyTorch car :</p>
<ul><li class="dash">elle est largement adopt√©e par la communaut√© de la recherche en deep learning,</li>
<li class="dash">elle est plus lisible et plus facile √† d√©boguer que TensorFlow et JAX,</li>
<li class="dash">elle offre plus de possibilit√©s que Keras,</li>
<li class="dash">elle est bien document√©e et est l&#039;une des biblioth√®ques les plus utilis√©es en science des donn√©es (Data Science en anglais) et en apprentissage machine (Machine Learning en anglais).</li>
</ul>

</div><div class="slide ">
<a id="title.1.3"></a><h2>üìñ 2. Qu&#039;est-ce qu&#039;un tenseur ?</h2>
<p>Les <strong>tenseurs</strong> sont la structure de base de PyTorch. Ce sont des tableaux multidimensionnels similaires aux <code>ndarray</code> de NumPy, mais avec des fonctionnalit√©s suppl√©mentaires pour le GPU et le calcul automatique des gradients. Un tenseur est une structure de donn√©es qui g√©n√©ralise les matrices √† un nombre quelconque de dimensions:</p>
<ul><li class="dash">Un scalaire est un tenseur 0D.</li>
<li class="dash">Un vecteur est un tenseur 1D.</li>
<li class="dash">Une matrice est un tenseur 2D.</li>
<li class="dash">On peut avoir des tenseurs 3D, 4D, etc.</li>
</ul>

<p>Les tenseurs √† haute dimensions sont tr√®s utilis√©s en deep learning (par exemple pour les images ou les vid√©os). Nous allons voir comment cr√©er et manipuler des tenseurs dans PyTorch. Vous pouvez copier-coller les exemples de code ci-dessous dans un notebook Jupyter pour les tester et voir les affichages. Pour utiliserles fonctions de PyTorch, il faut d&#039;abord l&#039;importer :</p>
<pre><code class="python hljs">import torch</code></pre>
</div><div class="slide ">
<a id="title.1.4"></a><h2>üìñ 3. Cr√©ation de tenseurs</h2>
<p>Il existe plusieurs mani√®res de cr√©er un tenseur en PyTorch.</p>
<a id="title.1.4.1"></a><h3>3.1 √Ä partir de donn√©es Python (listes ou tuples)</h3>
<pre><code class="python hljs"># Depuis une liste
a = torch.tensor([1, 2, 3])
print(a)

# Depuis une liste de listes (matrice)
b = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(b)

# On peut aussi sp√©cifier le type de donn√©es
c = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
print(c, c.dtype)</code></pre>
</div><div class="slide ">
<a id="title.1.4.2"></a><h3>3.2 Avec des fonctions de construction</h3>
<pre><code class="python hljs"># Tenseur rempli de z√©ros
z = torch.zeros(2, 3)
print(z)

# Tenseur rempli de uns
o = torch.ones(2, 3)
print(o)

# Tenseur vide (valeurs non initialis√©es)
e = torch.empty(2, 3)
print(e)

# Identit√© (matrice diagonale)
eye = torch.eye(3)
print(eye)</code></pre>
</div><div class="slide ">
<a id="title.1.4.3"></a><h3>3.3 Avec des suites r√©guli√®res</h3>
<p>PyTorch permet de g√©n√©rer facilement des suites de nombres avec des pas r√©guliers. Deux fonctions sont particuli√®rement utiles :</p>
<ol><li class=""><strong>torch.arange(debut, fin, pas)</strong></li>
<ul><li class="dash">Cr√©e une suite en commen√ßant √† <code>debut</code></li>
<li class="dash">S‚Äôarr√™te <em>avant</em> <code>fin</code> (attention, la borne sup√©rieure est exclue !)</li>
<li class="dash">Utilise le <code>pas</code> indiqu√©</li>
</ul>
</ol>

<pre><code class="python hljs"># De 0 √† 8 inclus, avec un pas de 2
r = torch.arange(0, 10, 2)
print(&quot;torch.arange(0, 10, 2) :&quot;, r)

# De 5 √† 20 exclu, avec un pas de 3
r2 = torch.arange(5, 20, 3)
print(&quot;torch.arange(5, 20, 3) :&quot;, r2)

# ‚ö†Ô∏è Remarque : la borne sup√©rieure (ici 10 ou 20) n&#039;est jamais incluse</code></pre>
</div><div class="slide ">
<ol><li class=""><strong>torch.linspace(debut, fin, steps)</strong></li>
<ul><li class="dash">Cr√©e une suite de <code>steps</code> valeurs r√©guli√®rement espac√©es</li>
<li class="dash">Inclut <strong>√† la fois</strong> <code>debut</code> et <code>fin</code></li>
</ul>
</ol>

<pre><code class="python hljs"># 5 valeurs entre 0 et 1 inclus
l = torch.linspace(0, 1, steps=5)
print(&quot;torch.linspace(0, 1, steps=5) :&quot;, l)

# 4 valeurs entre -1 et 1 inclus
l2 = torch.linspace(-1, 1, steps=4)
print(&quot;torch.linspace(-1, 1, steps=4) :&quot;, l2)</code></pre>
<p><strong>R√©sum√© des diff√©rences</strong></p>
<ul><li class="dash"><code>arange</code> ‚Üí on fixe le <strong>pas</strong> entre les valeurs, la fin est exclue.</li>
<li class="dash"><code>linspace</code> ‚Üí on fixe le <strong>nombre de valeurs</strong>, la fin est incluse.</li>
</ul>

<p>Exemple comparatif :</p>
<pre><code class="python hljs">print(torch.arange(0, 1, 0.25))   # [0.00, 0.25, 0.50, 0.75]
print(torch.linspace(0, 1, 5))    # [0.00, 0.25, 0.50, 0.75, 1.00]</code></pre>
</div><div class="slide ">
<a id="title.1.4.4"></a><h3>3.4 Avec des nombres al√©atoires</h3>
<pre><code class="python hljs"># Attention dans les exemples suivants, les crochets [] veulent dire que la valeur de la borne est incluse, contrairement √† aux parenth√®ses () qui signifient que la borne est exclue.
# Uniforme entre [0, 1)
u = torch.rand(2, 2)
print(&quot;Uniforme [0,1) :\n&quot;, u)

# Distribution normale (moyenne=0, √©cart-type=1)
n = torch.randn(2, 2)
print(&quot;Normale standard (0,1) :\n&quot;, n)

# Distribution normale avec moyenne (mean) et √©cart-type (std) choisis
custom = torch.normal(mean=2.0, std=3.0, size=(2,2))
print(&quot;Normale (moyenne=10, √©cart-type=2) :\n&quot;, custom)

# Fixer la graine pour la reproductibilit√©
torch.manual_seed(42)
print(&quot;Reproductibilit√© :\n&quot;, torch.rand(2, 2))  # toujours le m√™me r√©sultat</code></pre>
</div><div class="slide ">
<a id="title.1.5"></a><h2>üìñ 4. Conna√Ætre la forme d&#039;un tenseur</h2>
<p>Un tenseur peut avoir n‚Äôimporte quelle dimension. La m√©thode <code>.shape</code> permet de conna√Ætre sa taille.</p>
<pre><code class="python hljs"># Scalaire (0D)
s = torch.tensor(5)
print(&quot;Scalaire :&quot;, s, &quot;shape =&quot;, s.shape)

# Vecteur (1D)
v = torch.tensor([1, 2, 3, 4])
print(&quot;Vecteur :&quot;, v, &quot;shape =&quot;, v.shape)

# Matrice (2D)
m = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(&quot;Matrice :\n&quot;, m, &quot;shape =&quot;, m.shape)

# Tenseur 3D (par exemple, 2 matrices de taille 3x3)
t3 = torch.zeros(2, 3, 3)
print(&quot;Tenseur 3D shape =&quot;, t3.shape)

# Tenseur 4D (par exemple, un mini-batch de 10 images RGB de 32x32)
t4 = torch.zeros(10, 3, 32, 32)
print(&quot;Tenseur 4D shape =&quot;, t4.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.6"></a><h2>üìñ 5. Types de tenseurs et conversion</h2>
<ul><li class="dash">Vous pouvez sp√©cifier le type de donn√©es (<code>dtype</code>) lors de la cr√©ation :</li>
</ul>

<pre><code class="python hljs">x = torch.tensor([1.2, 3.4, 5.6])
print(x.dtype)     # float32 par d√©faut

x_int = x.to(torch.int32)
print(x_int, x_int.dtype)

x_float64 = x.double()
print(x_float64, x_float64.dtype)</code></pre>
<ul><li class="dash">Conversion d‚Äôun tenseur existant :</li>
</ul>

<pre><code class="python hljs">x_int = x.to(torch.int32)
print(x_int.dtype)</code></pre>
</div><div class="slide ">
<a id="title.1.7"></a><h2>üìñ 6. Op√©rations de base</h2>
<p>PyTorch supporte de nombreuses op√©rations sur les tenseurs :</p>
<pre><code class="python hljs">a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# Addition
print(a + b)

# Multiplication √©l√©ment par √©l√©ment
print(a * b)

# Produit matriciel
mat1 = torch.rand(2, 3)
mat2 = torch.rand(3, 4)
print(torch.mm(mat1, mat2))</code></pre>
</div><div class="slide ">
<a id="title.1.8"></a><h2>üìñ 7. Tenseurs sur GPU</h2>
<p>Pour profiter de l‚Äôacc√©l√©ration GPU, il suffit de d√©placer un tenseur sur le device CUDA :</p>
<pre><code class="python hljs">if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    x_gpu = x.to(device)
    print(&quot;Tenseur sur GPU :&quot;, x_gpu)
else:
    print(&quot;Pas de GPU disponible, utilisation du CPU.&quot;)</code></pre>
</div><div class="slide ">
<a id="title.1.9"></a><h2>üìñ 8.  Manipulation avanc√©e des tenseurs</h2>
<p>Une fois cr√©√©s, les tenseurs peuvent √™tre transform√©s et r√©arrang√©s. PyTorch fournit de nombreuses fonctions pour modifier leur forme, leurs dimensions ou leur ordre.</p>
<a id="title.1.9.1"></a><h3>8.1 Changer la forme avec <code>view</code> et <code>reshape</code></h3>
<ul><li class="dash"><code>view</code> : retourne un nouveau tenseur qui partage la m√™me m√©moire que l‚Äôoriginal. Cela implique que le tenseur soit contigu. Un tenseur est dit contigu lorsque ses donn√©es sont stock√©es de mani√®re cons√©cutive en m√©moire, c‚Äôest-√†-dire que PyTorch peut lire tous les √©l√©ments dans l‚Äôordre sans sauts.
Certaines op√©rations, comme la transposition (`t()`), rendent le tenseur non contigu, et dans ce cas <code>view</code> √©choue.</li>
<li class="dash"><code>reshape</code> : similaire √† <code>view</code>, mais plus flexible car il tente d‚Äôutiliser la m√©moire existante, mais cr√©e une copie si n√©cessaire. <code>reshape</code> fonctionne dans tous les cas de figures.</li>
</ul>

<pre><code class="python hljs">x = torch.arange(12)   # tenseur 1D [0, 1, ..., 11]
print(&quot;x :&quot;, x)

# Transformer en matrice 3x4
x_view = x.view(3, 4)
print(&quot;view en 3x4 :\n&quot;, x_view)

# Transformer en matrice 2x6
x_reshape = x.reshape(2, 6)
print(&quot;reshape en 2x6 :\n&quot;, x_reshape)</code></pre>
<p>Autre exemple pour illustrer la diff√©rence entre <code>view</code> et <code>reshape</code> :</p>
<pre><code class="python hljs"># Cr√©ation d&#039;un tenseur 2x3
x = torch.arange(6).view(2, 3)
print(&quot;x :\n&quot;, x)
print(&quot;Contigu :&quot;, x.is_contiguous())

# Transposition ‚Üí rend le tenseur non contigu
y = x.t()
print(&quot;\ny (transpos√©) :\n&quot;, y)
print(&quot;Contigu :&quot;, y.is_contiguous())

# view √©choue sur un tenseur non contigu
try:
    z = y.view(6)
except Exception as e:
    print(&quot;\nErreur avec view :&quot;, e)

# reshape fonctionne toujours
z2 = y.reshape(6)
print(&quot;\nreshape fonctionne :&quot;, z2)</code></pre>
</div><div class="slide ">
<a id="title.1.9.2"></a><h3>8.2 Changer l‚Äôordre des dimensions : <code>permute</code></h3>
<ul><li class="dash"><code>permute</code> r√©arrange les dimensions dans un nouvel ordre.</li>
<li class="dash">Tr√®s utile pour manipuler les donn√©es d‚Äôimages ou de s√©quences.</li>
</ul>

<pre><code class="python hljs"># Exemple avec un tenseur 3D (batch, hauteur, largeur)
t = torch.randn(2, 3, 4)  # forme (2, 3, 4)
print(&quot;Tenseur original :&quot;, t.shape)

# Inverser l&#039;ordre (largeur, hauteur, batch)
p = t.permute(2, 1, 0)
print(&quot;Apr√®s permute :&quot;, p.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.9.3"></a><h3>8.3 Ajouter ou supprimer des dimensions : <code>unsqueeze</code> et <code>squeeze</code></h3>
<ul><li class="dash"><code>unsqueeze(dim)</code> : ajoute une dimension de taille 1 √† la position <code>dim</code>.</li>
<li class="dash"><code>squeeze()</code> : supprime toutes les dimensions de taille 1.</li>
</ul>

<pre><code class="python hljs">v = torch.tensor([1, 2, 3])
print(&quot;Forme initiale :&quot;, v.shape)

v_unsq = v.unsqueeze(0)  # ajoute une dimension au d√©but
print(&quot;Apr√®s unsqueeze(0) :&quot;, v_unsq.shape)

v_sq = v_unsq.squeeze()  # supprime les dimensions de taille 1
print(&quot;Apr√®s squeeze() :&quot;, v_sq.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.9.4"></a><h3>8.4 Concat√©ner ou empiler des tenseurs</h3>
<ul><li class="dash"><code>torch.cat</code> : concat√®ne le long d‚Äôune dimension existante.</li>
<li class="dash"><code>torch.stack</code> : empile en ajoutant une nouvelle dimension.</li>
</ul>

<pre><code class="python hljs">a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

cat = torch.cat((a, b), dim=0)
print(&quot;torch.cat :&quot;, cat)

stack = torch.stack((a, b), dim=0)
print(&quot;torch.stack :&quot;, stack)
print(&quot;Forme de stack :&quot;, stack.shape)</code></pre>
</div><div class="slide ">
<a id="title.1.10"></a><h2>üìñ 9. Autograd avec PyTorch</h2>
<p>En Deep Learning, nous travaillons souvent avec des fonctions compliqu√©es d√©pendant de plusieurs variables. Pour entra√Æner un mod√®le, nous avons besoin de calculer automatiquement les d√©riv√©es de ces fonctions. C&#039;est l√† qu&#039;intervient Autograd qui est le moteur de diff√©rentiation automatique de PyTorch. </p>
<a id="title.1.10.1"></a><h3>9.1 Cr√©ation d&#039;un tenseur suivi</h3>
<p>Pour qu&#039;un tenseur suive les op√©rations et calcule les gradients automatiquement, il faut d√©finir <code>requires_grad=True</code> :</p>
<pre><code class="python hljs">import torch

x = torch.tensor([2.0, 3.0], requires_grad=True)
print(x)</code></pre>
<p>Ici, <code>x</code> est maintenant un tenseur avec suivi des gradients. Toutes les op√©rations futures sur ce tenseur seront enregistr√©es pour pouvoir calculer les d√©riv√©es automatiquement.</p>
</div><div class="slide ">
<a id="title.1.10.2"></a><h3>9.2 Op√©rations sur les tenseurs</h3>
<p>Toutes les op√©rations effectu√©es sur ce tenseur sont automatiquement enregistr√©es dans un graphe computationnel dynamique.</p>
<pre><code class="python hljs">y = x ** 2 + 3 * x # y = [y1, y2]
print(y)</code></pre>
<p>Dans ce cas :</p>
<ul><li class="dash"><code>x</code> est la variable d&#039;entr√©e.</li>
<li class="dash"><code>y</code> est calcul√© √† partir de <code>x</code> avec les op√©rations <code>x**2</code> et <code>3*x</code>.</li>
</ul>

<p>Chaque op√©ration devient un n≈ìud du graphe et PyTorch garde la trace des d√©pendances pour pouvoir calculer les gradients.</p>
<p>############################## Stop ICI ##############################
############################## Stop ICI ##############################
############################## Stop ICI ##############################
############################## Stop ICI ##############################</p>
</div><div class="slide ">
<a id="title.1.11"></a><h2>üìñ 10. Graphique computationnel</h2>
<p>Un graphe computationnel est une structure qui repr√©sente toutes les op√©rations effectu√©es sur les tenseurs.  </p>
<ul><li class="dash">Chaque n≈ìud correspond √† un tenseur ou √† une op√©ration math√©matique.</li>
<li class="dash">Chaque fl√®che indique une d√©pendance : le r√©sultat d&#039;une op√©ration d√©pend d&#039;un ou plusieurs tenseurs d&#039;entr√©e.</li>
</ul>

<p><em>Illustration ASCII pour l&#039;exemple pr√©c√©dent</em> :</p>
<pre><code class="no-highlight hljs">x
 \
   3*x
   /
y</code></pre>
<p>Ici :</p>
<ul><li class="dash">Les n≈ìuds <code>x^2</code> et <code>3*x</code> repr√©sentent les op√©rations effectu√©es sur <code>x</code>.</li>
<li class="dash">Le n≈ìud <code>y</code> combine ces deux r√©sultats.</li>
<li class="dash">Le graphe permet √† PyTorch desavoir quelles d√©riv√©es calculer et dans quel ordre.</li>
</ul>

</div><div class="slide ">
<a id="title.1.12"></a><h2>üìñ 11. Calcul des gradients et r√©tropropagation </h2>
<p>Autograd utilise ce graphe pour calculer automatiquement les d√©riv√©es par rapport √† <code>x</code>, en utilisant la m√©thode <code>backward()</code> :</p>
<pre><code class="python hljs">z = y.sum()  # z = y1 + y2
z.backward()
print(x.grad)</code></pre>
<ul><li class="dash"><code>backward()</code> calcule les d√©riv√©es de <code>y.sum()</code> par rapport √† chaque √©l√©ment de <code>x</code>.</li>
<li class="dash"><code>x.grad</code> contient maintenant les gradients.</li>
</ul>

<p>PyTorch parcourt le graphe <strong>en sens inverse</strong> :</p>
<ol><li class="">Commence par la sortie <code>z</code>.</li>
<li class="">Recule vers les n≈ìuds pr√©c√©dents (<code>y</code> puis <code>x</code>) en appliquant la r√®gle de d√©rivation.</li>
<li class="">Stocke le gradient dans <code>x.grad</code>.</li>
</ol>

<p>Calcul des gradients dans notre exemple :</p>
<ul><li class="dash"><code>dz/dy = 1</code> car z = y.sum()</li>
<li class="dash"><code>dy/dx = d√©riv√©e de (x^2 + 3*x) = 2*x + 3</code></li>
<li class="dash"><code>dx = dz/dy * dy/dx = 2*x + 3</code></li>
</ul>

<p>On obtient donc :</p>
<pre><code class="python hljs">print(x.grad)  # tensor([7., 9.])</code></pre>
</div><div class="slide ">
<a id="title.1.12.1"></a><h3>üìñ 12. D√©sactivation du suivi des gradients</h3>
<p>Pour certaines op√©rations, par exemple lors de l&#039;√©valuation d&#039;un mod√®le, il est inutile
de calculer les gradients. On peut alors d√©sactiver le suivi avec <code>torch.no_grad()</code> :</p>
<pre><code class="python hljs">with torch.no_grad():
    z = x * 2
print(z)</code></pre>
<p>Cela permet d&#039;√©conomiser de la m√©moire et d&#039;acc√©l√©rer les calculs.</p>
<p>################################ Partie LOSS ################################</p>
<p>Voici un exemple concret :</p>
<pre><code class="python hljs">import torch

# On cr√©e un tenseur avec require_grad=True pour suivre les gradients
x = torch.tensor([2.0, 3.0], requires_grad=True)

# On effectue des op√©rations
y = x ** 2 + 3 * x
z = y.sum()</code></pre>
<p>Dans ce cas :</p>
<ul><li class="dash"><code>x</code> est la variable d&#039;entr√©e.</li>
<li class="dash"><code>y</code> est calcul√© √† partir de <code>x</code> avec les op√©rations <code>x**2</code> et <code>3*x</code>.</li>
<li class="dash"><code>z</code> est la somme des √©l√©ments de <code>y</code> et correspond √† la <strong>fonction de perte</strong>.</li>
</ul>

<p>################################ Partie LOSS ################################</p>
<a id="title.1.13"></a><h2>Exemple concret : petite boucle d&#039;entra√Ænement</h2>
<p>On peut illustrer l&#039;utilisation d&#039;Autograd pour entra√Æner un r√©seau tr√®s simple
(une seule couche lin√©aire) :</p>
<pre><code class="python hljs"># Cr√©ation de donn√©es factices
X = torch.randn(5, 1, requires_grad=False)
y_true = 2 * X + 1

# Param√®tres √† apprendre
w = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)

# Boucle d&#039;entra√Ænement simple
learning_rate = 0.1
for epoch in range(10):
    y_pred = X * w + b
    loss = ((y_pred - y_true) ** 2).mean()

    loss.backward()  # calcul des gradients

    # Mise √† jour des param√®tres
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad

        # r√©initialisation des gradients
        w.grad.zero_()
        b.grad.zero_()

    print(f&quot;Epoch {epoch+1}, loss: {loss.item()}&quot;)</code></pre>
<a id="title.1.14"></a><h2>Conclusion</h2>
<p>Autograd permet de calculer automatiquement les d√©riv√©es et de mettre √† jour les
param√®tres lors de l&#039;entra√Ænement d&#039;un r√©seau de neurones. La combinaison de
<code>requires_grad=True</code>, <code>backward()</code> et <code>no_grad()</code> constitue le coeur de la
programmation avec PyTorch.</p>
<p>################################# POUR LE TP #####################</p>
</div>
<nav><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="chap0.html">&larr; Chapitre 0 - Installation des paquets et bibliothques n√©cessaires pour le cours</a></li><li class="page-item"><a class="page-link" href="index.html"><i class="bi bi-folder2"></i> Introduction aux fondamentaux de l&#039;apprentissage supervis√© et du Deep Learning</a></li></ul></nav>

</div> <!-- /contents -->
</div> <!-- /core -->

</body>
</html>
