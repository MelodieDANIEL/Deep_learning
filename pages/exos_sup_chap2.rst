.. slide::
üèãÔ∏è Exercices suppl√©mentaires
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.

.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 1 : Approximation d'une fonction 2D avec un MLP 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercise propose l'entra√Ænement d'un MLP avec des donn√©es en 2D.

**Objectif :** Entra√Æner un MLP pour approximer la fonction suivante :

.. math::

    y = \sin(X_1) + \cos(X_2)

o√π $$(X_1, X_2) \in [-2,2]^2$$, et visualiser la pr√©diction du mod√®le par rapport √† la fonction r√©elle.

**Consigne :**  

1) G√©n√©rer ``N = 800`` points al√©atoires $$(X_1, X_2)$$ dans $$[-2,2]$$ et calculer $$y$$ en suivant la fonction.

2) Standardiser les entr√©es pour le MLP.

3) Cr√©er un MLP simple :

   - Entr√©e : 2 features  
   - 2 couches cach√©es de 64 neurones avec activation ``Tanh``  
   - Sortie : 1 pr√©diction

4) Entra√Æner le mod√®le avec Adam et MSE loss pendant 1000 epochs.

5) Ajouter early stopping avec ``patience = 20`` et ``tolerance = 0.1``.

6) Pr√©parer une grille 2D pour visualiser la fonction r√©elle et la pr√©diction du mod√®le.

7) Afficher sur une seule figure 3D :

   - Surface r√©elle en vert transparent  
   - Surface pr√©dite par le MLP en orange semi-transparent  
   - Ajouter une l√©gende pour distinguer les surfaces

8) Tracer l'√©volution de la loss pendant l'entra√Ænement pour v√©rifier la convergence.

9) Refaire un test avec des donn√©es bruit√©es (ajouter un bruit gaussien de moyenne 0 et √©cart-type 0.6 √† y) et observer l'impact sur la pr√©diction du MLP.

**Questions :**  

10) Que remarquez-vous sur la capacit√© du MLP √† approximer la fonction sous-jacente malgr√© le bruit‚ÄØ?  
11) Que se passe-t-il si vous augmentez ou diminuez le niveau de bruit‚ÄØ?  
12) Comment l‚Äôearly stopping impacte-t-il l‚Äôapprentissage‚ÄØ?

**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour l'early stopping, stocker la meilleure loss et un compteur d'epochs sans am√©lioration.  
        2. Pour la visualisation, utiliser ``ax.plot_surface`` pour les surfaces et ``Patch`` pour la l√©gende.  
        3. La standardisation permet au MLP de mieux converger.  
        4. V√©rifier la loss finale pour s'assurer que le mod√®le a appris correctement la fonction.
        5. Pour g√©n√©rer le bruit, utilisez ``0.6 * torch.randn_like(y_clean)``.

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Voici le code pour la visualisation 3D avec Matplotlib :**
        .. code-block:: python
            x1g, x2g = torch.meshgrid(
            torch.linspace(-2, 2, 80),
            torch.linspace(-2, 2, 80),
            indexing="ij"
            )

            Xg = torch.cat([x1g.reshape(-1,1), x2g.reshape(-1,1)], dim=1)
            Xg_std = (Xg - X_mean) / X_std

            with torch.no_grad():
                y_true_grid = (torch.sin(x1g) + torch.cos(x2g))
                y_pred_grid = model(Xg_std).reshape(80, 80)
                y_pred_train = model(X_stdized)

            fig = plt.figure(figsize=(9,7))
            ax = fig.add_subplot(111, projection='3d')
            ax.set_title("MLP 2D avec Early Stopping")
            ax.set_xlabel("X1"); ax.set_ylabel("X2"); ax.set_zlabel("y")
            ax.set_xlim(-2, 2); ax.set_ylim(-2, 2); ax.set_zlim(-2, 2)
            try:
                ax.set_box_aspect((1,1,1))
            except Exception:
                pass
            ax.view_init(elev=25, azim=35)

            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_true_grid.numpy(),
                            cmap="Greens", alpha=0.45, linewidth=0)
            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_pred_grid.numpy(),
                            cmap="Oranges", alpha=0.70, linewidth=0)
            legend_elements = [
                Patch(facecolor="tab:green", alpha=0.45, label="Surface vraie"),
                Patch(facecolor="tab:orange", alpha=0.70, label="Surface MLP")
            ]
            ax.legend(handles=legend_elements, loc="upper left")


            plt.tight_layout()
            plt.show()


**R√©sultats attendus :**

- Voici un exemple de la figure 3D attendue pour les points 1 √† 8 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center

- Voici un exemple de la figure 3D attendue pour le point 9 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_suite_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center

.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 2 : Comparaison de deux MLP avec torchsummary
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez comparer deux MLP pour approximer une fonction non lin√©aire. L'objectif est d'observer l'impact de la taille du r√©seau sur la performance et de comprendre comment ``torchsummary`` permet d'√©valuer la structure du mod√®le.

**Objectif** :

- Comprendre comment la taille et la complexit√© d'un MLP influencent la qualit√© des pr√©dictions.
- Utiliser ``torchsummary`` pour visualiser le nombre de param√®tres et la structure du r√©seau.
- Comparer deux MLP sur une m√™me fonction et interpr√©ter leurs r√©sultats.

**Consignes** :

1) G√©n√©rer un jeu de donn√©es avec la fonction non lin√©aire suivante : 

   .. code-block:: python

       import torch
       torch.manual_seed(0)
       N = 200
       X = torch.linspace(0, 3, N).unsqueeze(1)
       y = torch.exp(X) + 0.1*torch.randn_like(X)  # fonction exponentielle bruit√©e

2) D√©finir deux MLP avec ``nn.Module`` et une activation ``Tanh`` :

   - **Petit MLP** : 2 couches cach√©es de 5 neurones chacune
   - **Grand MLP** : 2 couches cach√©es de 50 neurones chacune

3) Entra√Æner les deux mod√®les avec Adam et ``nn.MSELoss()`` pendant 2000 epochs et learning rate 0.01.

4) Utiliser ``torchsummary`` pour afficher la structure et le nombre de param√®tres de chaque mod√®le.

5) Tracer les pr√©dictions des deux MLP sur le m√™me graphique ainsi que la fonction vraie.

6) Comparer les performances et interpr√©ter les r√©sultats √† l‚Äôaide du r√©sum√© des mod√®les.

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
    - Pour ``torchsummary``, vous pouvez faire :

    .. code-block:: python

        from torchsummary import summary
        summary(model, input_size=(1,))

    - Stockez les pertes √† chaque epoch pour tracer l'√©volution et v√©rifier la convergence.
    - Le petit MLP a moins de param√®tres et risque moins de sur-apprentissage, mais peut √™tre limit√© pour des fonctions tr√®s complexes.
    - Le grand MLP peut sur-apprendre le bruit si le dataset est petit ou bruit√©.


**R√©sultats attendus :**

- Une figure montrant les pr√©dictions des deux MLP et la fonction vraie comme celle ci-dessous.
- Le r√©sum√© des mod√®les avec le nombre de param√®tres et la structure (torchsummary).
- Discussion : quel MLP capture mieux la fonction ? 


.. image:: images/chap2_exo_sup_2_resultat.png
    :alt: R√©sultat attendu MLP 
    :align: center