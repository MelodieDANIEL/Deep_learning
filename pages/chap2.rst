
.. slide::

Chapitre 2 - Introduction 
================

üéØ Objectifs du Chapitre
----------------------


.. important::

   √Ä la fin de ce chapitre, vous saurez : 

   - Cr√©er et manipuler des tenseurs PyTorch sur CPU et GPU.
   - Calculer automatiquement les gradients √† l‚Äôaide de ``autograd``.
   - D√©finir une fonction de perte.
   - Utiliser un optimiseur pour ajuster les param√®tres d‚Äôun mod√®le.
   - Impl√©menter une boucle d'entra√Ænement simple.

.. slide::

üìñ 1. Qu'est-ce que PyTorch ? 
----------------------


################################ Activation fonction ######################################
Parler de la softmax, relu , etc.
##########################################################################################



.. slide::
üìñ 20. Optimisation dans PyTorch
-----------------------

PyTorch fournit le module ``torch.optim`` qui impl√©mente plusieurs algorithmes d‚Äôoptimisation (SGD, Adam, etc.).  

Exemple avec la descente de gradient stochastique (SGD) :

.. code-block:: python

    import torch
    import torch.nn as nn
    import torch.optim as optim

    # Exemple : un mod√®le tr√®s simple (une seule couche lin√©aire)
    model = nn.Linear(1, 1)

    # Fonction de perte
    loss_fn = nn.MSELoss()

    # Optimiseur : SGD avec un learning rate de 0.01
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # Exemple de donn√©es
    x = torch.tensor([[1.0], [2.0], [3.0]])
    y = torch.tensor([[2.0], [4.0], [6.0]])  # y = 2x

    # √âtape d‚Äôentra√Ænement
    y_pred = model(x)            # 1. pr√©diction
    loss = loss_fn(y_pred, y)    # 2. calcul de la perte

    optimizer.zero_grad()        # 3. r√©initialise les gradients
    loss.backward()              # 4. r√©tropropagation
    optimizer.step()             # 5. mise √† jour des poids

---










