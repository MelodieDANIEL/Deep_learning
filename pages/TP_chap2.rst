üèãÔ∏è Travaux Pratiques 2
=========================
.. slide::
Sur cette page se trouvent des exercices de TP sur le Chapitre 2. Ils sont class√©s par niveau de difficult√© :
.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è




.. slide::
üçÄ Exercice 1 : Approximations d‚Äôune fonction non lin√©aire
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter une boucle d'entra√Ænement simple pour ajuster les param√®tres d‚Äôun mod√®le polyn√¥mial comme dans le chapitre 1, puis comparer les r√©sultats avec ceux d'un mod√®le MLP.

On vous donne les donn√©es suivantes :

.. code-block:: python

    torch.manual_seed(0)

    X = torch.linspace(-3, 3, 100).unsqueeze(1)
    y_true = torch.sin(X) + 0.1 * torch.randn(X.size())  # fonction sinuso√Ødale bruit√©e

**Objectif :** Comparer deux mod√®les pour approximer la fonction :

1. Polyn√¥me cubique : $$y = f(x) = a x^3 + b x^2 + c x + d$$, o√π $$a, b, c, d$$ sont des param√®tres appris automatiquement en minimisant l‚Äôerreur entre les pr√©dictions et les donn√©es r√©elles comme dans le chapitre 1.

2. MLP simple :  

    - Impl√©ment√© sous forme de classe ``nn.Module``  
    - 2 couches cach√©es de 10 neurones chacune avec ``ReLU`` pour l'activation
    - Entr√©e : 1 feature, sortie : 1 pr√©diction

**Consigne :** √âcrire un programme qui :

1) Ajuste les param√®tres du polyn√¥me cubique aux donn√©es en utilisant PyTorch.  
2) Affiche les param√®tres appris $$a, b, c, d$$.  
3) Impl√©mente ensuite un MLP et entra√Æne-le sur les m√™mes donn√©es pendant 5000 epochs avec un learning rate de 0.01.  
4) Compare visuellement les deux mod√®les avec les donn√©es r√©elles sur un m√™me graphique. 
5) Que remarquez-vous sur les performances des deux mod√®les ?
6) Que se passe-t-il si vous augmentez le nombre du polyn√¥me ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. Initialiser les param√®tres du polyn√¥me avec ``torch.randn(1, requires_grad=True)``.  
        2. Utiliser ``nn.MSELoss()`` comme fonction de perte pour les deux mod√®les.  
        3. Pour le MLP, cr√©er une classe h√©ritant de ``nn.Module`` et d√©finir ``forward``.  
        4. Utiliser ``optimizer.zero_grad()``, ``loss.backward()``, ``optimizer.step()`` √† chaque it√©ration.  
        5. On voit que le MLP parvient √† mieux s'adapter aux donn√©es, car il peut capturer des relations non lin√©aires plus complexes.

**R√©sultat attendu :** Vous devez obtenir un graphique similaire √† celui ci-dessous o√π :  

- les points bleus correspondent aux donn√©es r√©elles (``y_true``)  
- la courbe rouge correspond au polyn√¥me cubique  
- la courbe verte correspond au MLP  

.. image:: images/chap2_exo_1_resultat.png
    :alt: R√©sultat Exercice 1
    :align: center


.. slide::
‚öñÔ∏è Exercice 2 : Comparaison de l'entra√Ænement d'un MLP sur donn√©es brutes et standardis√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez entra√Æner un MLP simple sur un jeu de donn√©es synth√©tiques avec deux features ayant des √©chelles diff√©rentes. Vous comparerez les performances lorsque les donn√©es sont brutes ou standardis√©es.

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es synth√©tiques
    N = 500
    X1 = torch.linspace(0, 1, N).unsqueeze(1)      # petite amplitude
    X2 = torch.linspace(0, 100, N).unsqueeze(1)    # grande amplitude
    X = torch.cat([X1, X2], dim=1)
    y = 3*X1 + 0.05*X2**2 + torch.randn(N,1) * 0.5


**Objectif :**  
Comprendre l‚Äôimportance de la standardisation des donn√©es pour l‚Äôentra√Ænement d‚Äôun r√©seau de neurones et observer l‚Äô√©volution de la loss.


**Consigne :** √âcrire un programme qui :  

1) D√©finit une classe MLP simple sans couches cach√©es avec : 

   - une couche lin√©aire d‚Äôentr√©e (2 features) vers 20 neurones  
   - une fonction d‚Äôactivation ``ReLU``  
   - une couche de sortie avec 1 pr√©diction 

2) Cr√©e deux mod√®les : un pour les donn√©es brutes, un pour les donn√©es standardis√©es.  

3) Entra√Æne les deux mod√®les avec Adam et une fonction de perte MSE pendant 1000 epochs avec un learning rate de 0.01.

4) Stocke et trace l‚Äô√©volution de la loss pour les deux mod√®les.  

5) Trace les pr√©dictions finales des deux mod√®les sur le m√™me graphique que les donn√©es r√©elles.  

6) Comparez les performances des deux mod√®les et notez lequel converge plus vite et donne de meilleures pr√©dictions.

7) A quelle epoch peut-on consid√©rer que le mod√®le sur donn√©es standardis√©es a converg√© et comment on peut faire pour le d√©terminer ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. N‚Äôoubliez pas d'initialiser les poids du mod√®le avec ``torch.randn()`` pour un d√©marrage al√©atoire et de  mettre ``optimizer.zero_grad()`` avant ``loss.backward()``.  
        2. Pour standardiser, utilisez ``(X - X_mean)/X_std``.  
        3. Pour visualiser la loss : stockez ``loss.item()`` √† chaque epoch et utilisez ``matplotlib.pyplot.plot()``.  
        4. Pour visualiser les pr√©dictions, utilisez un scatter plot avec les donn√©es r√©elles et les pr√©dictions des deux mod√®les.
        5. Pour savoir quand stopper l'entra√Ænement, vous pouvez faire du Early Stopping.
        6. Pour que l‚Äôearly stopping fonctionne correctement avec ce type de donn√©es, il est conseill√© de :

            - Mettre le param√®tre ``patience`` √† 20.  
            - Comparer la perte actuelle avec la meilleure perte pr√©c√©dente en utilisant un seuil de tol√©rance. Par exemple, arrondir la perte √† 5 pour consid√©rer une am√©lioration significative (``if loss.item() < best_loss - 5``)

**R√©sultat attendu :**  
Le graphique montre les pr√©dictions du MLP sur les donn√©es brutes (rouge) et standardis√©es (bleu) par rapport aux donn√©es r√©elles (noir).  Vous devez obtenir un r√©sultat similaire √† celui-ci avant de r√©duire le nombre d'epochs :

.. image:: images/chap2_exo_2_resultat.png
    :alt: Comparaison MLP brutes vs standardis√©es
    :align: center


.. slide::
üå∂Ô∏è Exercice 3 : Overfitting et g√©n√©ralisation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercise permet d'observer l'overfitting avec un MLP sur des donn√©es bruit√©es. L'overfitting se produit lorsque le mod√®le apprend trop bien les d√©tails des donn√©es d'entra√Ænement, au d√©triment de sa capacit√© √† g√©n√©raliser sur de nouvelles donn√©es.

**Objectif :**

    - Comparer un MLP de petite taille et un MLP de grande taille.
    - Observer ce qui se passe si on entra√Æne trop longtemps un petit MLP.
    - Visualiser comment la complexit√© du mod√®le et le bruit des donn√©es influencent la qualit√© des pr√©dictions.
    - Tester les mod√®les sur de nouvelles donn√©es.

**Consigne :** √âcrire un programme qui :  

1) G√©n√®re un jeu de donn√©es 1D avec ``N=100`` points :  

   - ``X`` uniform√©ment dans $$[-3,3]$$.
   - ``y = sin(X) + bruit`` avec ``bruit = 0.2 * torch.randn_like(y)``.

2) D√©finit trois mod√®les MLP avec ``Tanh`` comme activation :  

   - Petit : 2 couches cach√©es de 5 neurones chacune  
   - Petit entra√Æn√© longtemps : m√™me architecture, mais entra√Æn√© avec plus d‚Äôepochs  
   - Grand : 2 couches cach√©es de 50 neurones chacune

3) Entra√Æne chaque mod√®le avec ``MSELoss`` et Adam pendant :  

   - Petit : 2000 epochs  
   - Petit long : 10000 epochs  
   - Grand : 2000 epochs

4) Trace sur le m√™me graphique :  

   - Les points de donn√©es bruit√©es  
   - La fonction vraie `sin(X)`  
   - Les pr√©dictions des trois MLP  

5) Trace √©galement l‚Äô√©volution de la loss pour chaque mod√®le.

6) Teste les mod√®les sur une nouvelle valeur de X (ex. X=0.5) et affiche les pr√©dictions et la valeur vraie.

**Questions :**

7) Que remarquez-vous sur la capacit√© de g√©n√©ralisation du MLP petit vs grand ?  
8) Que se passe-t-il si on augmente encore le nombre d‚Äôepochs pour le MLP petit ?  
9) Quel r√¥le joue le bruit dans la difficult√© de l‚Äôapprentissage ?  
10) Comment pourrait-on am√©liorer la g√©n√©ralisation des mod√®les (pistes) ?
11) Pouvez-vous √©crire du code pour √©viter de l'overfitting ?

**Astuce :**
.. spoiler::
    .. discoverList::
    1. Utiliser ``torch.manual_seed(0)`` pour la reproductibilit√©.  
    2. Pour l‚Äôentra√Ænement, penser √† ``optimizer.zero_grad()``, ``loss.backward()``, ``optimizer.step()``.  
    3. Stocker les losses √† chaque epoch pour pouvoir les tracer ensuite.  
    4. Pour la nouvelle valeur de test, utiliser ``with torch.no_grad()``.
    5. Faire de l'Early Stopping pour pr√©venir l'overfitting.


**R√©sultats attendus :**

- Voici un exemple de graphique attendu pour les pr√©dictions des trois mod√®les par rapport aux donn√©es bruit√©es et √† la fonction vraie :

.. image:: images/chap2_exo_3_resultat.png
    :alt: Comparaison MLP petit vs grand
    :align: center

- Les pr√©dictions sur la nouvelle valeur permettent de comparer la capacit√© de g√©n√©ralisation des mod√®les. Vous devriez obtenir des r√©sultats similaires √† ceux-ci: 
    Pour X = 0.50 :
        MLP petit = 0.5706, MLP petit entra√Æn√© longtemps = 0.7065, MLP grand = 0.7116 et Valeur vraie = 0.4794.

**R√©usltat pour √©viter l'overfitting :**
.. spoiler::
    .. discoverList::
        .. image:: images/chap2_exo_3_suite_resultat.png
        :alt: Comparaison MLP petit vs grand
        :align: center


.. slide::
üèãÔ∏è Exercices suppl√©mentaires 2
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.

.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 1 : Approximation d'une fonction 2D avec un MLP 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cet exercise propose l'entra√Ænement d'un MLP avec des donn√©es en 2D.

**Objectif :** Entra√Æner un MLP pour approximer la fonction suivante :

.. math::

    y = \sin(X_1) + \cos(X_2)

o√π $(X_1, X_2) \in [-2,2]^2$, et visualiser la pr√©diction du mod√®le par rapport √† la fonction r√©elle.

**Consigne :**  

1) G√©n√©rer ``N = 800`` points al√©atoires $$(X_1, X_2)$$ dans $$[-2,2]$$ et calculer $$y$$ en suivant la fonction.

2) Standardiser les entr√©es pour le MLP.

3) Cr√©er un MLP simple :

   - Entr√©e : 2 features  
   - 2 couches cach√©es de 64 neurones avec activation ``Tanh``  
   - Sortie : 1 pr√©diction

4) Entra√Æner le mod√®le avec Adam et MSE loss pendant 1000 epochs.

5) Ajouter early stopping avec ``patience = 20`` et ``tolerance = 0.1``.

6) Pr√©parer une grille 2D pour visualiser la fonction r√©elle et la pr√©diction du mod√®le.

7) Afficher sur une seule figure 3D* :

   - Surface r√©elle en vert transparent  
   - Surface pr√©dite par le MLP en orange semi-transparent  
   - Ajouter une l√©gende pour distinguer les surfaces

8) Tracer l'√©volution de la loss pendant l'entra√Ænement pour v√©rifier la convergence.

9) Refaire un test avec des donn√©es bruit√©es (ajouter un bruit gaussien de moyenne 0 et √©cart-type 0.6 √† y) et observer l'impact sur la pr√©diction du MLP.

**Questions :**  

10) Que remarquez-vous sur la capacit√© du MLP √† approximer la fonction sous-jacente malgr√© le bruit‚ÄØ?  
11) Que se passe-t-il si vous augmentez ou diminuez le niveau de bruit‚ÄØ?  
12) Comment l‚Äôearly stopping impacte-t-il l‚Äôapprentissage‚ÄØ?

**Astuce :**
.. spoiler::
    .. discoverList::
        1. Pour l'early stopping, stocker la meilleure loss et un compteur d'epochs sans am√©lioration.  
        2. Pour la visualisation, utiliser ``ax.plot_surface`` pour les surfaces et ``Patch`` pour la l√©gende.  
        3. La standardisation permet au MLP de mieux converger.  
        4. V√©rifier la loss finale pour s'assurer que le mod√®le a appris correctement la fonction.
        5. Pour g√©n√©rer le bruit, utilisez ``0.6 * torch.randn_like(y_clean)``.

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Voici le code pour la visualisation 3D avec Matplotlib :**
        .. code-block:: python
            x1g, x2g = torch.meshgrid(
            torch.linspace(-2, 2, 80),
            torch.linspace(-2, 2, 80),
            indexing="ij"
            )

            Xg = torch.cat([x1g.reshape(-1,1), x2g.reshape(-1,1)], dim=1)
            Xg_std = (Xg - X_mean) / X_std

            with torch.no_grad():
                y_true_grid = (torch.sin(x1g) + torch.cos(x2g))
                y_pred_grid = model(Xg_std).reshape(80, 80)
                y_pred_train = model(X_stdized)

            fig = plt.figure(figsize=(9,7))
            ax = fig.add_subplot(111, projection='3d')
            ax.set_title("MLP 2D avec Early Stopping")
            ax.set_xlabel("X1"); ax.set_ylabel("X2"); ax.set_zlabel("y")
            ax.set_xlim(-2, 2); ax.set_ylim(-2, 2); ax.set_zlim(-2, 2)
            try:
                ax.set_box_aspect((1,1,1))
            except Exception:
                pass
            ax.view_init(elev=25, azim=35)

            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_true_grid.numpy(),
                            cmap="Greens", alpha=0.45, linewidth=0)
            ax.plot_surface(x1g.numpy(), x2g.numpy(), y_pred_grid.numpy(),
                            cmap="Oranges", alpha=0.70, linewidth=0)
            legend_elements = [
                Patch(facecolor="tab:green", alpha=0.45, label="Surface vraie"),
                Patch(facecolor="tab:orange", alpha=0.70, label="Surface MLP")
            ]
            ax.legend(handles=legend_elements, loc="upper left")


            plt.tight_layout()
            plt.show()


**R√©sultats attendus :**

- Voici un exemple de la figure 3D attendue pour les points 1 √† 8 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center

- Voici un exemple de la figure 3D attendue pour le point 9 de la consigne avec la surface r√©elle (verte) et la surface pr√©dite par le MLP (orange) :

.. image:: images/chap2_exo_sup_1_suite_resultat.png
    :alt: R√©sultat attendu MLP 2D
    :align: center

.. slide::
‚öñÔ∏è Exercice suppl√©mentaire 2 : Comparaison de deux MLP avec torchsummary
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez comparer deux MLP pour approximer une fonction non lin√©aire. L'objectif est d'observer l'impact de la taille du r√©seau sur la performance et de comprendre comment ``torchsummary`` permet d'√©valuer la structure du mod√®le.

**Objectif** :

- Comprendre comment la taille et la complexit√© d'un MLP influencent la qualit√© des pr√©dictions.
- Utiliser ``torchsummary`` pour visualiser le nombre de param√®tres et la structure du r√©seau.
- Comparer deux MLP sur une m√™me fonction et interpr√©ter leurs r√©sultats.

**Consignes** :

1) G√©n√©rer un jeu de donn√©es avec la fonction non lin√©aire suivante : 

   .. code-block:: python

       import torch
       torch.manual_seed(0)
       N = 200
       X = torch.linspace(0, 3, N).unsqueeze(1)
       y = torch.exp(X) + 0.1*torch.randn_like(X)  # fonction exponentielle bruit√©e

2) D√©finir deux MLP avec ``nn.Module`` et une activation ``Tanh`` :

   - **Petit MLP** : 2 couches cach√©es de 5 neurones chacune
   - **Grand MLP** : 2 couches cach√©es de 50 neurones chacune

3) Entra√Æner les deux mod√®les avec Adam et ``nn.MSELoss()`` pendant 2000 epochs et learning rate 0.01.

4) Utiliser ``torchsummary`` pour afficher la structure et le nombre de param√®tres de chaque mod√®le.

5) Tracer les pr√©dictions des deux MLP sur le m√™me graphique ainsi que la fonction vraie.

6) Comparer les performances et interpr√©ter les r√©sultats √† l‚Äôaide du r√©sum√© des mod√®les.

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
    - Pour ``torchsummary``, vous pouvez faire :

    .. code-block:: python

        from torchsummary import summary
        summary(model, input_size=(1,))

    - Stockez les pertes √† chaque epoch pour tracer l'√©volution et v√©rifier la convergence.
    - Le petit MLP a moins de param√®tres et risque moins de sur-apprentissage, mais peut √™tre limit√© pour des fonctions tr√®s complexes.
    - Le grand MLP peut sur-apprendre le bruit si le dataset est petit ou bruit√©.


**R√©sultats attendus :**

- Une figure montrant les pr√©dictions des deux MLP et la fonction vraie comme celle ci-dessous.
- Le r√©sum√© des mod√®les avec le nombre de param√®tres et la structure (torchsummary).
- Discussion : quel MLP capture mieux la fonction ? 


.. image:: images/chap2_exo_sup_2_resultat.png
    :alt: R√©sultat attendu MLP 
    :align: center