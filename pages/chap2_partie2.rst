.. slide::
Chapitre 2 ‚Äî Perceptron multi-couches (partie 2)
===========================================

üéØ Objectifs du Chapitre
----------------------


.. important::

    √Ä la fin de cette section,  vous saurez :  

    - Le fonctionnement du perceptron simple.
    - Utiliser une fonction d'activation adapt√©e.  
    - L‚Äôimportance de la normalisation / standardisation des donn√©es et l'usage des epochs.  
    - Construire un r√©seau de neurones avec ``torch.nn``. 
    - Faire un entra√Ænement simple d‚Äôun MLP pour un probl√®me de r√©gression.   
    - Suivre l‚Äô√©volution de la loss et interpr√©ter les r√©sultats.  
    - Utiliser ``torch-summary`` pour inspecter l‚Äôarchitecture du r√©seau.  


.. slide::

üìñ 6. Broadcasting
----------------------------

6.1 Qu'est-ce que le broadcasting ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Le broadcasting est un m√©canisme qui permet √† PyTorch de faire des op√©rations entre tenseurs de dimensions diff√©rentes sans avoir √† √©crire de boucles. C'est comme cela qu'est fait l'op√©ration de centrage des donn√©es (soustraction de la moyenne) dans la standardisation des donn√©es.

üí° Id√©e principale :

- Si les dimensions des tenseurs sont compatibles, PyTorch r√©plique automatiquement le tenseur de plus petite dimension pour correspondre √† la taille du tenseur le plus grand.
- Cela permet de vectoriser les calculs et de rendre le code plus simple et rapide.

.. slide::
6.2 Exemple de broadcasting pour centrer des donn√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import torch

   # Matrice 3x2
   X = torch.tensor([[1., 2.],
                     [3., 4.],
                     [5., 6.]])

   # Moyenne de chaque colonne
   mean = X.mean(dim=0)  # dimension (2,)

   # On soustrait la moyenne √† chaque ligne
   X_centered = X - mean  # broadcasting

   print("X centr√© :", X_centered)

üí° Conclusion : M√™me si ``mean`` est un vecteur (dimension 2), PyTorch l‚Äôapplique √† toutes les lignes de ``X``. Le tenseur ``mean`` est automatiquement ‚Äú√©tendu‚Äù pour correspondre √† ``X``.  

‚úÖ R√©sultat : On peut centrer toutes les lignes d‚Äôun coup, sans boucle.



.. slide::
üìñ 7. Observer la loss et d√©terminer le nombre d‚Äôepochs
------------------------------------------------------
Lorsqu‚Äôon entra√Æne un mod√®le, il est essentiel de suivre l‚Äô√©volution de la loss pour savoir si le mod√®le apprend correctement et converge vers une solution. Dans l‚Äôexemple pr√©c√©dent, nous avons compar√© l‚Äôimpact de la standardisation sur les pr√©dictions finales. Nous allons maintenant observer l‚Äô√©volution de la loss pendant l‚Äôentra√Ænement pour mieux comprendre la convergence et d√©terminer un nombre d‚Äôepochs appropri√©. Nous allons continuer √† utiliser les donn√©es suivantes pour entra√Æner le mod√®le :

.. code-block:: python

   # Donn√©es d'entra√Ænement
   X = torch.tensor([[0.],[10.],[20.],[30.],[40.],[50.]])
   y = 2*X + 1

7.1. Suivi de la loss
~~~~~~~~~~~~~~~~~~~~~

Pour suivre la loss pour le mod√®le avec et sans standardisation il faut d'abord cr√©er deux listes pour stocker les valeurs de la loss √† chaque epoch. Pour cela, il suffit d'ajouter le code suivant avant la classe de cr√©ation du mod√®le : 

.. code-block:: python

    ...

    # Listes pour stocker l'√©volution de la loss
    losses_no_std = []
    losses_std = []

    ...


.. slide::
Ensuite, pendant l‚Äôentra√Ænement, on ajoute la valeur de la loss dans les listes √† chaque epoch. Cela se fait comme suit : 

.. code-block:: python

    ...

    # Sans standardisation
    pred_no_std = model_no_std(X)
    
    ...

    optimizer_no_std.step()
    losses_no_std.append(loss_no_std.item()) # Ligne √† ajouter

    # Avec standardisation
    pred_std = model_std(X_stdized)
    
    ...

    optimizer_std.step()
    losses_std.append(loss_std.item()) # Ligne √† ajouter

    ...

.. slide::
Enfin on ajoute les lignes de code suivantes pour tracer les loss √† la fin du code : 

.. code-block:: python

    ...

    # Visualisation de la loss
    plt.plot(losses_no_std, label='Sans standardisation')
    plt.plot(losses_std, label='Avec standardisation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss MSE')
    plt.title("√âvolution de la loss pendant l'entra√Ænement")
    plt.legend()
    plt.show()


.. slide::
7.2. Interpr√©tation du r√©sultat
~~~~~~~~~~~~~~~~~~

- **Convergence** :  
  - Si la loss diminue et se stabilise autour d‚Äôune valeur faible, le mod√®le converge.  
  - Si la loss reste tr√®s √©lev√©e ou diverge, le mod√®le ne converge pas correctement.

- **Choix du nombre d‚Äôepochs** :  
  - En regardant le graphique, on peut d√©terminer √† partir de quel epoch la loss se stabilise.  
  - Cela permet de choisir un nombre d‚Äôepochs suffisant sans sur-entra√Æner inutilement le mod√®le.
  - Dans cet exemple, on d√©couvre que pour le mod√®le qui s'entra√Æne avec standardisation, la loss se stabilise √† 0 autour de 500 epochs. Vous pouvez r√©duire le nombre d'epochs et v√©rifier que 500 epochs suffisent.

.. note::
    **Remarque** : Si vous relancer l'entra√Ænement, le graphique de la loss peut varier √† cause de l'initialisation al√©atoire des poids sauf si vous utilisez un ``seed`` fixe.

.. slide::
7.3. Early Stopping
~~~~~~~~~~~~~~~~~~~~

Pour √©viter de trop entra√Æner le mod√®le, on peut surveiller la loss et arr√™ter l‚Äôentra√Ænement lorsque la perte ne diminue plus. Cela s‚Äôappelle l‚Äôearly stopping. On peut automatiser le processus avec PyTorch. Tout d'abord, il faut remettre le nombre d'epoch √† 5000. Ensuite il faut cr√©er les variables suivantes et les ajouter avant la classe qui construit le mod√®le :

.. code-block:: python

    ...

    # Param√®tres pour l'early stopping
    patience = 50       # nombre d'epochs sans am√©lioration avant arr√™t
    best_loss_std = float('inf') # meilleure loss observ√©e pour le mod√®le avec standardisation (initialis√©e √† l'infini pour que la premi√®re am√©lioration soit toujours accept√©e)
    counter_std = 0 # compteur d'epochs sans am√©lioration

    patience_no_std = 50
    best_loss_no_std = float('inf')    
    counter_no_std = 0

    ...

.. slide::
Ensuite, il faut ajouter le code suivant √† la fin de chaque boucle d'entra√Ænement pour v√©rifier si la loss s'est am√©lior√©e ou non. Si elle ne s'am√©liore pas pendant un certain nombre d'epochs (d√©fini par ``patience``), l'entra√Ænement s'arr√™te automatiquement. Voici le code √† ajouter :

.. code-block:: python

    ...

    # Sans standardisation

    ...


    losses_no_std.append(loss_no_std.item())

    # Early stopping pour le mod√®le sans standardisation (code √† ajouter)
    if loss_no_std.item() < best_loss_no_std:
        best_loss_no_std = loss_no_std.item()
        counter_no_std = 0
    else:
        counter_no_std += 1
    if counter_no_std >= patience_no_std:
        print(f"Arr√™t anticip√© (sans std) √† l'epoch {epoch}, loss = {best_loss_no_std:.4f}")
        break

    # Avec standardisation
   
    ...

    losses_std.append(loss_std.item())

    # Early stopping pour le mod√®le standardis√© (code √† ajouter)
    if loss_std.item() < best_loss_std:
        best_loss_std = loss_std.item()
        counter_std = 0
    else:
        counter_std += 1
    if counter_std >= patience:
        print(f"Arr√™t anticip√© (avec std) √† l'epoch {epoch}, loss = {best_loss_std:.4f}")
        break

    ...

.. slide::

üí° **Remarque** :  

- Cette m√©thode simple permet de d√©terminer un nombre d‚Äôepochs appropri√© automatiquement.  
- Pour cet exemple, le mod√®le sans standardisation des donn√©es ne converge jamais avec une loss $$\approx 0$$ tandis que le mod√®le avec standardisation des donn√©es converge √† partir d'environ 200 epochs.
- Dans la pratique, on combine souvent early stopping avec un jeu de validation pour √©viter le surapprentissage.

.. slide::
üìñ 8. Observer le mod√®le avec ``torch-summary`` et la performance des gradients avec autograd profiler
-------------------

Il existe plusieurs outils PyTorch qui permettent d'inspecter et de profiler les mod√®les. Le but √©tant de parvenir √† identifier les goulots d'√©tranglement et √† optimiser les performances. Parmi eux, on trouve :

- ``torchsummary`` : pour visualiser la structure du mod√®le et le nombre de param√®tres par couche.
- ``torch.autograd.profiler`` : pour profiler le calcul des gradients et identifier les op√©rations co√ªteuses.

8.1. Utiliser ``torchsummary``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``torchsummary`` permet de visualiser la structure du mod√®le et le nombre de param√®tres par couche avant l'entra√Ænement. Pour l'utiliser, il faut d'abord l'installer :

.. code-block:: bash

    pip install torch-summary

Ensuite, juste apr√®s la d√©finition de votre mod√®le, vous pouvez faire un r√©sum√© du mod√®le :

.. code-block:: python

    from torchsummary import summary

    # Mod√®le standardis√© d√©fini pr√©c√©demment
    # Cr√©er une copie sur CPU pour torchsummary
    model_std_cpu = MLP().to("cpu")

    # R√©sum√© du mod√®le
    # input_size correspond aux dimensions d'un √©chantillon (hors batch)
    # Ici, chaque √©chantillon a 1 feature (scalaire)
    summary(model_std_cpu, input_size=(1,), device="cpu")

.. slide::
Explications¬†:

- ``input_size`` : dimensions d‚Äôun √©chantillon (hors batch).  
  Dans notre exemple, chaque √©chantillon est un scalaire (1 feature), donc ``input_size=(1,)``.  
- ``device`` : est √©gal √† ``"cpu"`` pour √©viter tout conflit CUDA si le mod√®le ou PyTorch envoie certains tenseurs sur GPU.  

- R√©sultat¬†: pour chaque couche, on voit :

  - le type de couche (Linear, ReLU‚Ä¶)
  - la taille des tenseurs interm√©diaires
  - le nombre de param√®tres
  - le nombre de param√®tres entra√Ænables



.. slide::
8.2. R√¥le du profiler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Pour encore plus am√©liorer la performance de votre mod√®le, PyTorch fournit ``torch.autograd.profiler.profile`` pour profiler le calcul des gradients ce qui permet de :

- Mesurer le temps et la m√©moire consomm√©s par chaque op√©ration.
- Identifier les goulots d'√©tranglement dans le r√©seau.
- Optimiser et d√©bugger les mod√®les complexes.


.. slide::
8.3. Exemple d'utilisation du profiler pour l'exemple de r√©gression
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pour tester le profiler, il suffit d'ajouter le code suivant juste apr√®s le code de ``torchsummary`` :

.. code-block:: python

    ...

    # torch.autograd.profiler est utilis√© dans ce chapitre pour la simplicit√©
    # Pour des usages avanc√©s (timeline, TensorBoard), on peut utiliser torch.profiler
    import torch.autograd.profiler as profiler

    # Faire un profiling sur une seule passe avant la boucle d'entra√Ænement
    with profiler.profile(use_cuda=True, profile_memory=True) as prof_dummy:
        # Forward + backward sur le mod√®le standardis√©
        pred_std = model_std(X_stdized)
        loss_std = ((pred_std - y)**2).mean()
        optimizer_std.zero_grad()
        loss_std.backward()

    # Afficher le profil CPU (temps d'ex√©cution)
    print("Profil CPU pour le mod√®le standardis√© (une seule passe avant entra√Ænement) :")
    print(prof_dummy.key_averages().table(sort_by="cpu_time_total"))

    # Afficher le profil GPU (m√©moire consomm√©e)
    print(prof_dummy.key_averages().table(sort_by="self_cuda_memory_usage", row_limit=10))

    ...

.. slide::
**Conclusion** : 

    - On peut profiler √† la fois le **temps CPU** et la **m√©moire GPU**.
    - On utilise :
        - ``cpu_time_total`` pour identifier les op√©rations co√ªteuses en calcul,
        - ``self_cuda_memory_usage`` pour rep√©rer celles qui consomment le plus de m√©moire GPU.
    - Le profiler ralentit fortement l'ex√©cution : il ne doit pas √™tre utilis√© pendant tout l‚Äôentra√Ænement, mais seulement ponctuellement pour analyser ou optimiser.

    - Chaque op√©ration ex√©cut√©e sur le CPU par PyTorch y est list√©e avec :
        - ``Self CPU %`` : temps pass√© directement dans l‚Äôop√©ration.
        - ``CPU total %`` : temps total incluant les sous-op√©rations.
        - ``# of Calls`` : nombre d‚Äôappels √† l‚Äôop√©ration.

    - Chaque op√©ration ex√©cut√©e sur le GPU par PyTorch y est list√©e avec :
        - ``Self CUDA Memory Usage`` : m√©moire GPU utilis√©e directement par l‚Äôop√©ration.
        - ``CUDA Memory Usage`` : m√©moire totale incluant les sous-op√©rations.
        - ``# of Calls`` : nombre d‚Äôappels √† l‚Äôop√©ration.

    - Les **couches lin√©aires** (``aten::linear``) prennent la majeure partie du temps : multiplication matricielle + bias.
    - Les **activations** (``ReLU``, ``Tanh``) et les calculs de **loss** (``mean``, ``pow``) consomment moins de temps mais sont n√©cessaires pour propager les gradients.
    - Les op√©rations comme ``detach`` ou ``clone`` apparaissent lorsqu‚Äôon fait des copies ou qu‚Äôon d√©tache un tenseur du graphe pour ne pas calculer de gradient dessus.
    - Ce profilage permet de **visualiser les goulots d‚Äô√©tranglement** et d‚Äôoptimiser l‚Äôentra√Ænement si n√©cessaire.

    - Pour un petit MLP, le plus co√ªteux est le calcul des couches lin√©aires et du backward. Sur des mod√®les plus grands ou avec GPU, ces informations sont cruciales pour comprendre et am√©liorer les performances.

.. slide::
‚öñÔ∏è Exercice 2 : Comparaison de l'entra√Ænement d'un MLP sur donn√©es brutes et standardis√©es
---------------------------

Dans cet exercice, vous allez entra√Æner un MLP simple sur un jeu de donn√©es synth√©tiques avec deux features ayant des √©chelles diff√©rentes. Vous comparerez les performances lorsque les donn√©es sont brutes ou standardis√©es.

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es synth√©tiques
    N = 500
    X1 = torch.linspace(0, 1, N).unsqueeze(1)      # petite amplitude
    X2 = torch.linspace(0, 100, N).unsqueeze(1)    # grande amplitude
    X = torch.cat([X1, X2], dim=1)
    y = 3*X1 + 0.05*X2**2 + torch.randn(N,1) * 0.5


**Objectif :**  
Comprendre l‚Äôimportance de la standardisation des donn√©es pour l‚Äôentra√Ænement d‚Äôun r√©seau de neurones et observer l‚Äô√©volution de la loss.


**Consigne :** √âcrire un programme qui :  

.. step::

    1) D√©finit une classe MLP simple sans couches cach√©es avec : 

        - une couche lin√©aire d‚Äôentr√©e (2 features) vers 20 neurones  
        - une fonction d‚Äôactivation ``ReLU``  
        - une couche de sortie avec 1 pr√©diction 

.. step::
    2) Cr√©e deux mod√®les : un pour les donn√©es brutes, un pour les donn√©es standardis√©es.  

.. step::
    3) Entra√Æne les deux mod√®les avec Adam et une fonction de perte MSE pendant 1000 epochs avec un learning rate de 0.01.

.. step::
    4) Stocke et trace l‚Äô√©volution de la loss pour les deux mod√®les.  

.. step::
    5) Trace les pr√©dictions finales des deux mod√®les sur le m√™me graphique que les donn√©es r√©elles.  

.. step::
    6) Comparez les performances des deux mod√®les et notez lequel converge plus vite et donne de meilleures pr√©dictions.

.. step::
    7) A quelle epoch peut-on consid√©rer que le mod√®le sur donn√©es standardis√©es a converg√© et comment on peut faire pour le d√©terminer ?


**Astuce :**
.. spoiler::
    .. discoverList::
        1. N‚Äôoubliez pas d'initialiser les poids du mod√®le avec ``torch.randn()`` pour un d√©marrage al√©atoire et de  mettre ``optimizer.zero_grad()`` avant ``loss.backward()``.  
        2. Pour standardiser, utilisez ``(X - X_mean)/X_std``.  
        3. Pour visualiser la loss : stockez ``loss.item()`` √† chaque epoch et utilisez ``matplotlib.pyplot.plot()``.  
        4. Pour visualiser les pr√©dictions, utilisez un scatter plot avec les donn√©es r√©elles et les pr√©dictions des deux mod√®les.
        5. Pour savoir quand stopper l'entra√Ænement, vous pouvez faire du Early Stopping.
        6. Pour que l‚Äôearly stopping fonctionne correctement avec ce type de donn√©es, il est conseill√© de :

            - Mettre le param√®tre ``patience`` √† 20.  
            - Comparer la perte actuelle avec la meilleure perte pr√©c√©dente en utilisant un seuil de tol√©rance. Par exemple, arrondir la perte √† 5 pour consid√©rer une am√©lioration significative (``if loss.item() < best_loss - 5``)

**R√©sultat attendu :**  
Le graphique montre les pr√©dictions du MLP sur les donn√©es brutes (rouge) et standardis√©es (bleu) par rapport aux donn√©es r√©elles (noir). Vous devez obtenir un r√©sultat similaire √† celui-ci avant de r√©duire le nombre d'epochs :

.. image:: images/chap2_exo_2_resultat.png
    :alt: Comparaison MLP brutes vs standardis√©es
    :align: center

.. slide::

üå∂Ô∏è Exercice 3 : Overfitting et g√©n√©ralisation
---------------------

Cet exercise permet d'observer l'overfitting avec un MLP sur des donn√©es bruit√©es. L'overfitting se produit lorsque le mod√®le apprend trop bien les d√©tails des donn√©es d'entra√Ænement, au d√©triment de sa capacit√© √† g√©n√©raliser sur de nouvelles donn√©es.

**Objectif :**

    - Comparer un MLP de petite taille et un MLP de grande taille.
    - Observer ce qui se passe si on entra√Æne trop longtemps un petit MLP.
    - Visualiser comment la complexit√© du mod√®le et le bruit des donn√©es influencent la qualit√© des pr√©dictions.
    - Tester les mod√®les sur de nouvelles donn√©es.

**Consigne :** √âcrire un programme qui :  

.. step::
    1) G√©n√®re un jeu de donn√©es 1D avec ``N=100`` points :  

        - ``X`` uniform√©ment dans $$[-3,3]$$.
        - ``y = sin(X) + bruit`` avec ``bruit = 0.2 * torch.randn_like(y)``.

.. step::
    2) D√©finit trois mod√®les MLP avec ``Tanh`` comme activation :  

        - Petit : 2 couches cach√©es de 5 neurones chacune  
        - Petit entra√Æn√© longtemps : m√™me architecture, mais entra√Æn√© avec plus d‚Äôepochs  
        - Grand : 2 couches cach√©es de 50 neurones chacune

.. step::
    3) Entra√Æne chaque mod√®le avec ``MSELoss`` et Adam pendant :  

        - Petit : 2000 epochs  
        - Petit long : 10000 epochs  
        - Grand : 2000 epochs

.. step::
    4) Trace sur le m√™me graphique :  

        - Les points de donn√©es bruit√©es  
        - La fonction vraie `sin(X)`  
        - Les pr√©dictions des trois MLP  

.. step::
    5) Trace √©galement l‚Äô√©volution de la loss pour chaque mod√®le.

.. step::
    6) Teste les mod√®les sur une nouvelle valeur de X (ex. X=0.5) et affiche les pr√©dictions et la valeur vraie.

**Questions :**

.. step::
    7) Que remarquez-vous sur la capacit√© de g√©n√©ralisation du MLP petit vs grand ?  

.. step::
    8) Que se passe-t-il si on augmente encore le nombre d‚Äôepochs pour le MLP petit ?  

.. step::
    9) Quel r√¥le joue le bruit dans la difficult√© de l‚Äôapprentissage ?  

.. step::
    10) Comment pourrait-on am√©liorer la g√©n√©ralisation des mod√®les (pistes) ?

.. step::
    11) Pouvez-vous √©crire du code pour √©viter de l'overfitting ?

**Astuce :**
.. spoiler::
    .. discoverList::
    1. Utiliser ``torch.manual_seed(0)`` pour la reproductibilit√©.  
    2. Pour l‚Äôentra√Ænement, penser √† ``optimizer.zero_grad()``, ``loss.backward()``, ``optimizer.step()``.  
    3. Stocker les losses √† chaque epoch pour pouvoir les tracer ensuite.  
    4. Pour la nouvelle valeur de test, utiliser ``with torch.no_grad()``.
    5. Faire de l'Early Stopping pour pr√©venir l'overfitting.


**R√©sultats attendus :**

- Voici un exemple de graphique attendu pour les pr√©dictions des trois mod√®les par rapport aux donn√©es bruit√©es et √† la fonction vraie :

.. image:: images/chap2_exo_3_resultat.png
    :alt: Comparaison MLP petit vs grand
    :align: center

- Les pr√©dictions sur la nouvelle valeur permettent de comparer la capacit√© de g√©n√©ralisation des mod√®les. Vous devriez obtenir des r√©sultats similaires √† ceux-ci: 
    Pour X = 0.50 :
        MLP petit = 0.5706, MLP petit entra√Æn√© longtemps = 0.7065, MLP grand = 0.7116 et Valeur vraie = 0.4794.

**R√©usltat pour √©viter l'overfitting :**
.. spoiler::
    .. discoverList::
        .. image:: images/chap2_exo_3_suite_resultat.png
        :alt: Comparaison MLP petit vs grand
        :align: center


.. slide::
üèãÔ∏è Exercices Suppl√©mentaires
--------------------

.. toctree::

    exos_sup_chap2


