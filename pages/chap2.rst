.. slide::
Chapitre 2 ‚Äî Perceptron multi-couches 
===========================================

üéØ Objectifs du Chapitre
----------------------


.. important::

    √Ä la fin de cette section,  vous saurez :  

    - Le fonctionnement du perceptron simple.
    - Utiliser une fonction d'activation adapt√©e.  
    - L‚Äôimportance de la normalisation / standardisation des donn√©es et l'usage des epochs.  
    - Construire un r√©seau de neurones avec ``torch.nn``. 
    - Faire un entra√Ænement simple d‚Äôun MLP pour un probl√®me de r√©gression.   
    - Suivre l‚Äô√©volution de la loss et interpr√©ter les r√©sultats.  
    - Utiliser ``torch-summary`` pour inspecter l‚Äôarchitecture du r√©seau.  


.. slide::

üìñ 1. Rappels sur les perceptrons
----------------------

Le perceptron multi-couches (MLP de Multi-Layers Perceptron en anglais) est la brique de base des r√©seaux de neurones modernes. Dans ce chapitre, nous allons l‚Äôappliquer √† des probl√®mes de r√©gression simple. Avant de commencer, voici quelques rappels.

1.1. Perceptron simple
~~~~~~~~~~~~~~~~~~~~~~

Le perceptron est le bloc de base d‚Äôun r√©seau de neurones. Il r√©alise une transformation lin√©aire suivie (ou pas) d‚Äôune fonction d‚Äôactivation telle que :  

  .. math::

     y = \sigma(Wx + b)

  o√π :  
   - $$y$$ est la sortie du perceptron, 
   - $$\sigma$$ est une fonction d‚Äôactivation, 
   - $$W$$ est la matrice des poids,  
   - $$b$$ est le biais et  
   - $$x$$ est l'ensemble des entr√©es du perceptron.  

.. slide::
1.2. Perceptron intuition
~~~~~~~~~~~~~~~~~~~~~~
.. image:: images/chap2_perceptron.png
    :alt: perceptron
    :align: center
    :width: 40%

avec $$y= \sigma(x_1*w_1 + x_2*w_2 + ...+ x_i*w_i + ... + x_n*w_n + b)$$

üí° **Intuition :**

    - Chaque poids $$w_i$$ mesure l‚Äôimportance de la caract√©ristique $$x_i$$.  
    - Le biais $$b$$ d√©place la fronti√®re de d√©cision.  
    - La fonction d‚Äôactivation permet d‚Äôintroduire de la non-lin√©arit√©, indispensable pour mod√©liser des relations complexes mais nous en parleront plus en d√©tails par la suite.  


.. slide::
1.3. Mise √† jour des param√®tres
~~~~~~~~~~~~~~~~~~~~~~

Un perceptron poss√®de deux types de **param√®tres** : les **poids** et le **biais**.  

Lors de l‚Äôentra√Ænement, on souhaite ajuster ces param√®tres pour am√©liorer les pr√©dictions du mod√®le.  Pour cela, il faut mettre √† jour les poids apr√®s avoir calcul√© la loss gr√¢ce √† la fonction de perte et le gradient gr√¢ce √† l'optimiseur comme expliqu√© dans le chapitre pr√©c√©dent.  

Pour rappel, on met √† jours les param√®tres du mod√®le gr√¢ce √† l'√©quation introduite dans le chapitre pr√©c√©dent. 

.. math::

    \theta \leftarrow \theta - \eta \, \nabla_\theta \mathcal{L}(\theta)

o√π :  

    - $$\theta$$ repr√©sente l‚Äôensemble des param√®tres du mod√®le (ici $$W$$ et $$b$$),  
    - $$\mathcal{L}$$ est la fonction de perte,  
    - $$\nabla_\theta \mathcal{L}$$ est le gradient de la perte par rapport aux param√®tres,  
    - $$\eta$$ est le taux d‚Äôapprentissage (learning rate en anglais).


.. slide::
1.4. Exemples d'applications du perceptron simple
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Un perceptron simple ne peut r√©soudre que les probl√®mes lin√©airement s√©parables puisque en trouvant les param√®tres du mod√®le, le perceptron trace une droite dans le plan des entr√©es et s√©pare les points selon qu‚Äôils sont au-dessus ou en dessous de cette droite.

**Exemple 1 : porte logique ET**

+-----+-----+-------+
| x‚ÇÅ  | x‚ÇÇ  | y=ET  |
+=====+=====+=======+
|  0  |  0  |   0   |
+-----+-----+-------+
|  0  |  1  |   0   |
+-----+-----+-------+
|  1  |  0  |   0   |
+-----+-----+-------+
|  1  |  1  |   1   |
+-----+-----+-------+

Dans ce cas, une droite s√©pare bien les deux classes :  

    - la classe $$0$$ (points en bas √† gauche, en haut √† gauche, en bas √† droite),  
    - la classe $$1$$ (point en haut √† droite).  

Un perceptron simple peut donc apprendre cette fonction.

.. slide::
**Exemple 2 : porte logique XOR**

+-----+-----+--------+
| x‚ÇÅ  | x‚ÇÇ  | y=XOR  |
+=====+=====+========+
|  0  |  0  |   0    |
+-----+-----+--------+
|  0  |  1  |   1    |
+-----+-----+--------+
|  1  |  0  |   1    |
+-----+-----+--------+
|  1  |  1  |   0    |
+-----+-----+--------+

Ici, il est impossible de tracer une seule droite qui s√©pare correctement les classes. Autrement dit, XOR n‚Äôest pas lin√©airement s√©parable.  

.. image:: images/chap2_et_vs_xor.png
   :alt: Repr√©sentation du XOR dans le plan (non-s√©parable lin√©airement)
   :align: center
   :width: 300%

**Conclusion :** 

    - Le perceptron simple suffit pour des t√¢ches lin√©aires (comme ET, OU).  
    - Pour r√©soudre des probl√®mes plus complexes comme XOR, il faut introduire plusieurs couches de neurones et des fonctions d‚Äôactivation non-lin√©aires : c‚Äôest le principe du **perceptron multi-couches (MLP)**. 

.. slide::
1.5. Faire un perceptron dans PyTorch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pour cr√©er un perceptron simple dans PyTorch, on peut utiliser la fonction ``Linear`` de ``torch.nn``, qui impl√©mente une couche lin√©aire (ou affine) : $$y = Wx + b$$. La fonction ``Linear`` prend en entr√©e le nombre d'entr√©e $$x$$ et le nombre de sortie $$y$$.

.. code-block:: python

    import torch
    import torch.nn as nn

    # Donn√©es ET
    X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)
    y = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)

    # Mod√®le lin√©aire (perceptron)
    model = nn.Linear(2, 1, bias=True)

    # Loss function et optimiseur
    loss_fc = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    # Entra√Ænement
    for _ in range(500):
        optimizer.zero_grad()
        loss = loss_fc(model(X), y)
        loss.backward()
        optimizer.step()

    # R√©sultat
    with torch.no_grad():
        print((model(X)).round())
        print(model.weight, model.bias)

**Remarque** : si maintenant on change les entr√©es et sorties pour le XOR, le mod√®le ne pourra pas apprendre correctement la fonction (les $$W$$ restent √† 0 comme √† l'initialisation). Vous pouvez faire le test pour v√©rifier.

.. slide::

üìñ 2. Fonction d'activation
-----------

Les fonctions d‚Äôactivation introduisent de la non-lin√©arit√© dans le mod√®le, ce qui permet de mieux capturer des relations complexes dans les donn√©es. Sans une fonction d'activation, un perceptron (ou m√™me plusieurs formant un r√©seau de neurones de plusieurs couches) ne ferait que des combinaisons lin√©aires et ne pourrait pas r√©soudre des probl√®mes non lin√©aires comme XOR. 

.. slide::
2.1. √âquations des fonctions d'activation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Voici quatres fonctions d‚Äôactivation couramment utilis√©es :

1. **Sigmo√Øde** : $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
   - Sortie comprise entre 0 et 1.
   - Utilis√©e pour les probl√®mes de classification binaire.

2. **Tanh** : $$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
   - Sortie comprise entre -1 et 1.
   - Souvent utilis√©e dans les couches cach√©es des r√©seaux de neurones.

3. **ReLU (de Rectified Linear Unit en anglais)** : $$\text{ReLU}(x) = \max(0, x)$$
   - Sortie nulle pour les entr√©es n√©gatives.
   - La plus utilis√©e dans les r√©seaux de neurones profonds en raison de sa simplicit√© et de son efficacit√©.

4. **Softmax** : $$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$
   - Transforme un vecteur en une distribution de probabilit√© (chaque sortie est comprise entre 0 et 1 et la somme vaut 1).
   - Utilis√©e en sortie des mod√®les de classification multi-classes.

.. slide::
2.2. Repr√©sentation graphique des fonctions d'activation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. image:: images/chap2_fonctions_d_activation.png
   :alt: Repr√©sentation des fonctions d'activation
   :align: center
   :width: 200%

.. slide::
2.3. Les fonctions d'activation dans PyTorch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans PyTorch, les fonctions d'activation sont disponibles dans la biblioth√®que `torch.nn`. Voici quelques exemples :

1. **Sigmo√Øde** : ``nn.sigmoid(x)``
2. **Tanh** : ``nn.tanh(x)``
3. **ReLU** : ``nn.relu(x)``
4. **Softmax** : ``nn.softmax(x, dim=1)``


.. slide::
2.4. R√¥le de la fonction d‚Äôactivation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reprenons le probl√®me ET avec un perceptron.

- **Sans fonction d‚Äôactivation** :  
  Le perceptron calcule une combinaison lin√©aire des entr√©es : $$ z = w_1 x_1 + w_2 x_2 + b $$.

  La sortie est un nombre r√©el, positif ou n√©gatif. Pour classer les donn√©es, on fixe un seuil arbitraire (par exemple : si $$z > 0$$ alors classe 1, sinon 0). La fronti√®re de d√©cision reste **lin√©aire**.

- **Avec une fonction d'activation (la fonction sigmo√Øde par exemple)** :  
  On applique une transformation non lin√©aire : $$\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}$$.

  La sortie est toujours comprise entre 0 et 1. On peut alors l‚Äôinterpr√©ter comme une **probabilit√©** qui mesure la confiance du mod√®le dans sa pr√©diction : proche de 0 ‚Üí classe 0 et proche de 1 ‚Üí classe 1. Le seuil devient naturel : **0.5**.

.. note::
Remarque : Dans le cas o√π le probl√®me √† r√©soudre est non lin√©airement s√©parable (comme XOR), une fonction d‚Äôactivation seule ne suffit pas. Il faut empiler plusieurs couches de neurones avec des fonctions d‚Äôactivation entre chaque couche pour capturer la complexit√© des donn√©es.


.. slide::
2.5. Exemple d'utilisation des fonctions d'activation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Voici un exemple d'utilisation des fonctions d'activation pour le probl√®me ET avec un perceptron :

.. code-block:: python

    import torch
    import torch.nn as nn

    # Donn√©es ET
    X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)
    y = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)

    # --- Cas 1 : Perceptron sans activation ---
    linear = nn.Linear(2, 1, bias=True)
    with torch.no_grad():
        linear.weight[:] = torch.tensor([[1., 1.]])  # w1=1, w2=1
        linear.bias[:] = torch.tensor([-1.5])        # b=-1.5

    z = linear(X)  # sortie brute
    print("Sorties sans activation :")
    print(z)

    # --- Cas 2 : Perceptron avec sigmo√Øde ---
    sigmoid = nn.Sigmoid()
    y_hat = sigmoid(z)
    print("\nSorties avec sigmo√Øde :")
    print(y_hat)

Une sortie brute comme -1.5 devient 0.18 apr√®s sigmo√Øde, et 0.5 devient 0.62 : la sigmoid transforme les nombres en valeurs entre 0 et 1, les rendant interpr√©tables comme des probabilit√©s.

.. slide::
2.6. Choisir la fonction d'activation adapt√©e
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
On peut choisir la fonction d‚Äôactivation en fonction de plusieurs crit√®res: le probl√®me √† r√©soudre ou la convergence de l'entra√Ænement.

**Choix selon le contexte** : 

  - Pour une sortie binaire, la sigmo√Øde est adapt√©e car elle renvoie une valeur entre 0 et 1, interpr√©table comme une probabilit√©.  
  - Pour une sortie multi-classes, la fonction Softmax normalise les valeurs pour obtenir une distribution de probabilit√©.  
  - Pour des sorties continues ou pour moduler les valeurs internes, ReLU ou Tanh peuvent √™tre utilis√©es.

**Impact sur l‚Äôapprentissage** :  
  Certaines fonctions d‚Äôactivation influencent la vitesse de convergence. Par exemple, la sigmo√Øde borne les sorties, ce qui peut r√©duire l‚Äôamplitude des gradients et ralentir l‚Äôapprentissage pour de grandes valeurs absolues.

.. slide::
üìñ 3. Epoch
-----------

Lorsqu‚Äôon entra√Æne un mod√®le de machine learning, il est n√©cessaire de pr√©senter plusieurs fois l‚Äôensemble des donn√©es d‚Äôapprentissage $$x$$ au mod√®le afin d‚Äôajuster correctement ses param√®tres.

3.1 D√©finitions
~~~~~~~~~~~~~~~~~~~~~~~~~~

- **It√©ration** : mise √† jour des param√®tres du mod√®le apr√®s avoir trait√© un seul exemple ou un mini-batch.  
- **Batch / mini-batch** : sous-ensemble d‚Äôexemples utilis√© pour calculer la descente de grandient et la mise √† jour des param√®tres.  
- **Epoch** : passage complet sur toutes les donn√©es d‚Äôapprentissage.  

**Exemple** :

Si vous disposez de 1000 exemples et que vous utilisez des mini-batchs de 100 exemples chacun, une epoch correspond √† 10 it√©rations (1000 √∑ 100). Apr√®s chaque epoch, chaque exemple de l‚Äôensemble d‚Äôapprentissage a √©t√© utilis√© exactement une fois pour mettre √† jour les param√®tres du mod√®le.

.. slide::
3.2 Pourquoi effectuer plusieurs epochs‚ÄØ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Au d√©but de l‚Äôentra√Ænement, le mod√®le commet souvent de grandes erreurs.  Chaque epoch permet aux poids et aux biais de s‚Äôajuster progressivement, am√©liorant ainsi les pr√©dictions. En pratique, plusieurs dizaines ou centaines d‚Äôepochs sont souvent n√©cessaires pour que la loss se stabilise et que le mod√®le converge vers une bonne solution.

üí° **Intuition** : imaginez un perceptron comme un √©l√®ve qui apprend : il ne retient pas tout parfaitement du premier coup. Il faut plusieurs passages sur les m√™mes exercices pour ma√Ætriser la t√¢che.


.. slide::

üìñ 4. Normalisation et standardisation des donn√©es
--------------------------------------------------

Avant d'entra√Æner un mod√®le, il est important de pr√©parer les donn√©es pour que l‚Äôapprentissage soit efficace. Pour cela, deux op√©rations courantes sont la normalisation et la standardisation.

4.1. Normalisation
~~~~~~~~~~~~~~~~~

La normalisation consiste √† mettre les valeurs dans une plage donn√©e, souvent entre 0 et 1. Cela est utile lorsque les donn√©es ont des √©chelles tr√®s diff√©rentes. Pour ceal , il faut appliquer la formule suivante √† chaque donn√©e:

.. math::

   x'_i = \frac{x_i - x_\text{min}}{x_\text{max} - x_\text{min}}

- $$x_\text{min}$$ et $$x_\text{max}$$ sont respectivement la valeur minimale et maximale de la variable.  
- $$x'_i$$ est la valeur normalis√©e.

.. slide::
4.2. Exemple de normalisation avec PyTorch
~~~~~~~~~~~~~~~~~

.. code-block:: python

    import torch
    X = torch.tensor([[1., 50.],[2., 60.],[3., 55.]])
    X_min = X.min(dim=0).values
    X_max = X.max(dim=0).values
    X_norm = (X - X_min) / (X_max - X_min)
    print(X_norm)


.. slide::
4.3. Standardisation
~~~~~~~~~~~~~~~~~~

La standardisation consiste √† centrer et r√©duire les variables : on soustrait la moyenne et on divise par l‚Äô√©cart-type. C‚Äôest particuli√®rement utile pour les algorithmes bas√©s sur le gradient (comme les perceptrons), car cela acc√©l√®re la convergence. Pour standardiser les donn√©es voici la formule √† appliquer pour chaque donn√©e :

.. math::

   x'_i = \frac{x_i - \mu}{\sigma}

- $$\mu$$ est la moyenne de la variable.  
- $$\sigma$$ est l‚Äô√©cart-type.  

.. slide::
4.4. Exemple de standardisation avec PyTorch
~~~~~~~~~~~~~~~~~~
 Contrairement √† la normalisation, la standardisation a une fonction dans PyTorch pr√©-impl√©ment√©e nomm√©e ``torch.nn.BatchNorm1d``. Voici comment l'impl√©menter avec PyTorch :

.. code-block:: python
    import torch
    import torch.nn as nn

    X = torch.tensor([[1., 50.],[2., 60.],[3., 55.]], dtype=torch.float32)

    # Standardisation "manuelle"
    X_mean = X.mean(dim=0)
    X_std = X.std(dim=0)
    X_stdized = (X - X_mean) / X_std
    print("Standardisation manuelle :")
    print(X_stdized)

    # Standardisation avec BatchNorm1d
    batchnorm = nn.BatchNorm1d(num_features=2, affine=False)
    X_stdized_bn = batchnorm(X)
    print("\nStandardisation avec BatchNorm1d :")
    print(X_stdized_bn)

.. slide::
4.5. Normalisation vs. Standardisation
~~~~~~~~~~~~~~~~~~
   
La standardisation est souvent pr√©f√©r√©e √† la normalisation car elle est **plus robuste aux valeurs aberrantes** et permet une **convergence plus rapide** du mod√®le.

- **Robustesse aux valeurs aberrantes** : la standardisation centre et r√©duit les donn√©es par rapport √† la moyenne et √† l‚Äô√©cart-type, plut√¥t que de les ramener dans une plage fixe comme la normalisation Min-Max. Une valeur tr√®s grande ou tr√®s petite affecte moins l‚Äô√©chelle globale et n‚Äô√©crase pas les autres donn√©es.

- **Convergence plus rapide** : la standardisation met toutes les variables sur une √©chelle comparable. Sans standardisation, une variable avec de grandes valeurs provoque de tr√®s grands gradients dans sa direction, tandis qu‚Äôune variable plus petite change lentement. Le gradient combin√© suit alors une trajectoire en zigzag, avan√ßant lentement vers le minimum. En standardisant, les gradients sont √©quilibr√©s et le mod√®le descend plus directement vers la solution optimale.

.. slide::
4.6. Ce qui est attendu apr√®s la standardisation
~~~~~~~~~~~~~~~~~~

Apr√®s avoir centr√© et r√©duit les donn√©es, la standardisation permet g√©n√©ralement d'avoir une **moyenne proche de 0** et un **√©cart-type proche de 1** pour chaque variable.

**Pourquoi ?**

  - Une moyenne proche de 0 aide les fonctions d'activation et la descente de gradient √† mieux fonctionner, sans que le mod√®le doive apprendre un biais pour d√©caler toutes les donn√©es.
  - Un √©cart-type proche de 1 met toutes les donn√©es sur une √©chelle comparable, ce qui √©vite que certaines variables dans dans les donn√©es dominent les gradients et permet une descente plus directe vers le minimum de la loss.


.. note::
   Si la standardisation est appliqu√©e sur un mini-batch (par exemple avec BatchNorm1d), la moyenne et l‚Äô√©cart-type sont calcul√©s sur ce mini-batch. Dans ce cas, la moyenne n‚Äôest pas exactement 0 et l‚Äô√©cart-type n‚Äôest pas exactement 1 pour l‚Äôensemble du dataset. De plus, certains modules comme BatchNorm peuvent apprendre un scale et un shift, modifiant l√©g√®rement ces valeurs finales.

**Est-ce grave si ce n'est pas exactement 0 et 1 ?**

  - Pas n√©cessairement : une moyenne et un √©cart-type approximatifs suffisent g√©n√©ralement pour que l'apprentissage reste efficace.
  - Par contre, si les valeurs sont tr√®s √©loign√©es de 0 ou tr√®s dispers√©es, certaines fonctions d'activation peuvent saturer et ralentir la convergence.


.. slide:: 

üìñ 5. R√©seaux de neurones multi-couches (MLP)
--------------------------------------------

Les r√©seaux de neurones multi-couches (MLP, de l'anglais Multi-Layer Perceptron) permettent de r√©soudre des probl√®mes non lin√©aires comme XOR, que le perceptron simple ne peut pas g√©rer. Un MLP se compose de **couches lin√©aires** suivies de **fonctions d'activation**, et peut √™tre construit tr√®s simplement avec ``torch.nn.Sequential``.

5.1. D√©finitions
~~~~~~~~~~~~~~~~

- **Une couche** d'un MLP se compose d'un ensemble de perceptrons. Chaque perceptron (aussi appel√© neurone) re√ßoit les m√™mes entr√©es et produit une sortie individuelle. La combinaison des sorties de tous les perceptrons forme le vecteur de sortie de la couche.

- Il existe plusieurs types de couches :
  - **La couche d'entr√©e** re√ßoit les features du dataset et les transmet √† la premi√®re couche cach√©e.
  - **Les couches cach√©es** sont situ√©es entre l'entr√©e et la sortie, elles permettent de mod√©liser des relations non lin√©aires entre les variables.
  - **La couche de sortie** produit la sortie finale du r√©seau (par exemple, une probabilit√© pour la classification binaire).

.. slide:: 
5.2. Construction d'un MLP
~~~~~~~~~~~~~~~~~~~~~~~~~~

Pour construire un MLP, il faut choisir le nombre de couches et de neurones par couche ainsi que la fonction d'activation √† utiliser apr√®s chaque couche. Il n‚Äôest g√©n√©ralement pas possible de conna√Ætre √† l‚Äôavance le nombre exact √† mettre. On teste plusieurs architectures pour trouver celle qui converge correctement et rapidement.

- Nombre de couches cach√©es : g√©n√©ralement 1 ou 2 couches suffisent pour des probl√®mes simples comme XOR. Pour des probl√®mes plus complexes, plusieurs couches peuvent √™tre n√©cessaires.  
- Nombre de neurones par couche : il n‚Äôexiste pas de r√®gle stricte. On choisit un nombre suffisant pour capturer la complexit√© du probl√®me, mais pas trop pour √©viter le surapprentissage (lorsque le mod√®le s'adapte trop aux donn√©es d'entra√Ænement et ne g√©n√©ralise pas bien sur de nouvelles donn√©es).  
- En pratique, on peut commencer par un petit nombre de neurones et augmenter si le mod√®le n‚Äôarrive pas √† converger correctement.

üí° R√©sum√© :  
Chaque couche d‚Äôun MLP est un ensemble de perceptrons. Les couches cach√©es permettent de mod√©liser la non-lin√©arit√©, et le nombre de couches et de neurones doit √™tre choisi en fonction de la complexit√© du probl√®me et de la performance souhait√©e.


.. slide:: 
5.3. Construire un MLP simple avec ``torch.nn``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pour cr√©er un MLP dans PyTorch, on utilise principalement :  

- ``Sequential`` : permet d‚Äôempiler facilement les couches les unes apr√®s les autres.  
- ``Linear`` : cr√©e une couche affine, c‚Äôest-√†-dire une transformation de la forme $$y = Wx + b$$.  
- Fonctions d‚Äôactivation : introduisent de la **non-lin√©arit√©** dans le mod√®le (par exemple ``nn.ReLU()`` ou ``nn.Sigmoid()``).

Exemple minimal d‚Äôun r√©seau de neurones pour une r√©gression 1D avec un MLP √† deux couches cach√©es :

.. code-block:: python

   import torch.nn as nn

   model = nn.Sequential(
       nn.Linear(1, 10),   # couche d'entr√©e 1D -> premi√®re couche cach√©e 10 neurones
       nn.ReLU(),           # activation non lin√©aire
       nn.Linear(10, 5),    # deuxi√®me couche cach√©e avec 5 neurones
       nn.ReLU(),           # activation non lin√©aire
       nn.Linear(5, 1)      # couche de sortie 1D
   )

üí° Remarques :  

    - La premi√®re couche transforme l‚Äôentr√©e en un vecteur de dimension 10.
    - La deuxi√®me couche r√©duit ce vecteur √† 5 dimensions, permettant au r√©seau de combiner et transformer les features.
    - Chaque couche cach√©e est suivie d‚Äôune fonction d‚Äôactivation pour capturer la non-lin√©arit√©. 
    - La couche finale produit la sortie finale du r√©seau.

.. note:: 
    **Important** : La dimension de sortie d‚Äôune couche doit correspondre √† la dimension d‚Äôentr√©e de la couche suivante.  


.. slide:: 
5.4. Construire un MLP avec une classe
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans PyTorch, il est courant de d√©finir un mod√®le en cr√©ant une classe qui h√©rite de ``nn.Module``. Cela permet de mieux organiser le code, de r√©utiliser le mod√®le facilement. Dans ce cas, la m√©thode ``forward``  d√©crit comment les donn√©es traversent le r√©seau.

Voici le m√™me exemple que pr√©c√©demment avec une classe :

.. code-block:: python

   import torch
   import torch.nn as nn

   class SimpleMLP(nn.Module):
       def __init__(self):
           super(SimpleMLP, self).__init__()
           self.fc1 = nn.Linear(1, 10)   # premi√®re couche cach√©e
           self.fc2 = nn.Linear(10, 5)   # deuxi√®me couche cach√©e
           self.fc3 = nn.Linear(5, 1)    # couche de sortie
           self.relu = nn.ReLU()         # fonction d'activation

       def forward(self, x):
           x = self.relu(self.fc1(x))
           x = self.relu(self.fc2(x))
           x = self.fc3(x)
           return x

    # Cr√©ation d'une instance du mod√®le
    model = SimpleMLP()

üí° Remarques : 

    - La m√©thode ``forward`` d√©finit comment les donn√©es passent de la couche d'entr√©e √† la sortie, en appliquant les fonctions d‚Äôactivation entre les couches.  
    - L‚Äôavantage de la classe : elle permet de s√©parer la d√©finition du mod√®le et l‚Äôentra√Ænement, ce qui rend le code plus clair et modulable.  
    - On peut facilement r√©utiliser ce mod√®le pour diff√©rentes entr√©es ou probl√®mes.


.. slide::
5.5. R√©soudre XOR avec un MLP
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Comme expliqu√© pr√©c√©demment, un perceptron simple ne peut pas r√©soudre le probl√®me XOR, m√™me avec une fonction d‚Äôactivation, car il ne fait qu‚Äôune s√©paration lin√©aire (une droite).

    - Pour le XOR, il faut un r√©seau de neurones avec au moins une couche cach√©e et une fonction d‚Äôactivation √† la sortie de la couche cach√©e.
    - La fronti√®re de d√©cision apprise n‚Äôest plus une droite mais une courbe form√©e par la combinaison des sorties de plusieurs neurones. Visuellement, cela peut ressembler √† deux demi-plans combin√©s ou √† une courbe ferm√©e entourant certains points, selon l‚Äôactivation utilis√©e (Tanh ou ReLU).


Exemple minimal en PyTorch avec une seule couche cach√©e et une activation non-lin√©aire :

.. code-block:: python

    import torch
    import torch.nn as nn
    import torch.optim as optim
    import matplotlib.pyplot as plt

    # Donn√©es XOR
    X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)
    y = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)

    # D√©finition du MLP avec une classe
    class XORMLP(nn.Module):
        def __init__(self):
            super(XORMLP, self).__init__()
            self.fc1 = nn.Linear(2, 4)  # couche cach√©e 1
            self.fc2 = nn.Linear(4, 1)  # couche de sortie
            self.activation = nn.ReLU()
            self.out_activation = nn.Sigmoid()
        
        def forward(self, x):
            x = self.activation(self.fc1(x))
            x = self.out_activation(self.fc2(x))
            return x

    # Cr√©ation du mod√®le
    model = XORMLP()

    # Optimiseur et fonction de perte
    optimizer = optim.Adam(model.parameters(), lr=0.05)
    fc_loss = nn.MSELoss()

    # Entra√Ænement
    for epoch in range(5000):
        y_pred = model(X)
        loss = fc_loss(y_pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # V√©rification num√©rique
    with torch.no_grad():
        y_pred_train = model(X)
        y_class = (y_pred_train > 0.5).float()
        print("Pr√©dictions (probabilit√©s) :\n", y_pred_train)
        print("Classes pr√©dites :\n", y_class)
        print("Classes r√©elles :\n", y)
        correct = (y_class == y).all()
        print("Toutes les pr√©dictions sont correctes :", correct)

    # Affichage de la fronti√®re de d√©cision
    xx, yy = torch.meshgrid(torch.linspace(-0.5, 1.5, 200),
                            torch.linspace(-0.5, 1.5, 200))
    grid = torch.cat([xx.reshape(-1,1), yy.reshape(-1,1)], dim=1)
    with torch.no_grad():
        zz = model(grid).reshape(xx.shape)

    plt.contourf(xx, yy, zz, levels=[0,0.5,1], alpha=0.3, colors=["red","blue"])
    plt.scatter(X[:,0], X[:,1], c=y[:,0], cmap="bwr", edgecolors="k", s=100)
    plt.title("Fronti√®re de d√©cision XOR avec MLP en classe")
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.show()

üí° Remarques :

- La fonction d‚Äôactivation dans la couche cach√©e est essentielle pour r√©soudre des probl√®mes non lin√©aires comme XOR.
- La sortie finale est transform√©e par la Sigmo√Øde, produisant une probabilit√© entre 0 et 1 pour la classification binaire.
- M√™me un petit MLP avec une seule couche cach√©e de 4 neurones suffit pour apprendre XOR gr√¢ce √† la non-lin√©arit√© introduite par ReLU.
- L‚Äôutilisation d‚Äôune classe et de la m√©thode ``forward`` rend le code plus modulable et facilite l‚Äôexp√©rimentation avec diff√©rentes architectures de MLP.
- Vous pouvez remplacer la ReLU par une Tanh et voir la diff√©rence dans l'affichage.

.. slide::

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################


.. slide::


################################ MLP ######################################
- il faut mettre en lumi√®re epoch
- Faire un exemple plus dure pour montrer l'int√©r√™t de standardiser les donn√©es



parler de dataset loader et parler de broadcasting ?

- parler de .detach() et .clone() ?

- parler de autograd profiler.profile

- parler de la gestion des outliers
##########################################################################################



6. Suivi de la loss et visualisation
-------------------------------------

- Pendant l‚Äôentra√Ænement, enregistrer la loss √† chaque epoch pour voir si elle diminue.  
- Comparer ``y_pred`` et ``y_true`` avec Matplotlib.  

.. code-block:: python

   import matplotlib.pyplot as plt

   plt.plot(losses)              # courbe de la loss
   plt.scatter(x, y_true)        # donn√©es r√©elles
   plt.scatter(x, y_pred)        # pr√©dictions


7. Inspecter le mod√®le avec ``torch-summary``
----------------------------------------------

Permet de voir le nombre de param√®tres par couche et la structure du r√©seau.  

.. code-block:: python

   from torchsummary import summary
   summary(model, input_size=(1,))




.. slide::
4. Normaliser / standardiser les donn√©es
-----------------------------


Exemple avec scikit-learn :  √Ä ajouter ?????

.. code-block:: python

   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)





############################ Il faudra penser √† cr√©er un gitlab pour le cours ##################

