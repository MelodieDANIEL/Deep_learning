.. slide::
üèãÔ∏è Exercices suppl√©mentaires
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.


.. slide::
üçÄ Exercice suppl√©mentaire 1 : Gradient d‚Äôune fonction polynomiale
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante $$f(a) = 3a^3 - 2a^2 + a$$ avec $$a = 2.0$$.

**Consigne :** Utiliser les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† $$a$$ :

1) Calculez √† la main la d√©riv√©e de $$f$$ par rapport √† $$a$$. Puis √©valuez ce gradient pour $$a = 2.0$$.

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.

**Astuce :**
.. spoiler::
    .. discoverList::
        La d√©riv√©e de $$f(a)$$ par rapport √† $$a$$ est √©gale √† $$9a¬≤ - 4a + 1$$

**R√©sultat attendu :**  
Le gradient est √©gal √† 29 dans les deux cas. 


.. slide::
üçÄ Exercice suppl√©mentaire 2 : Gradient de deux variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante $$f(a, b) = a \cdot b + a^2$$ avec $$a = 2.0$$ et $$b = 3.0$$.


**Consigne :** Utiliser les deux approches suivantes pour calculer les d√©riv√©es partielles de cette fonction par rapport √† $$a$$ et $$b$$ :

1) Calculez √† la main la d√©riv√©e partielle de $$f$$ par rapport √† $$a$$ et par rapport √† $$b$$. Puis √©valuez ces d√©riv√©es pour $$a = 2.0$$ et $$b = 3.0$$.

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez le gradient de cette fonction.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.


**Astuce :**  
.. spoiler::
    .. discoverList::
        - La d√©riv√©e de $$f$$ par rapport √† $$a$$ est $$‚àÇf/‚àÇa = b + 2a$$ et par rapport √† $$b$$ est $$‚àÇf/‚àÇb = a$$.

**R√©sultat attendu :**  
Les d√©riv√©es partielles sont, dans les deux cas, √©gales √† : $$‚àÇf/‚àÇa = 7$$ et $$‚àÇf/‚àÇb = 2$$.


‚öñÔ∏è Exercice suppl√©mentaire 3 : Comparaison des fonctions de perte MSE et MAE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es bruit√©es suivantes
    torch.manual_seed(0)
    x = torch.linspace(-3, 3, 100)
    y_true = 2 * x**2 + 3 * x + 1 + 0.5 * torch.randn(x.size())  # avec bruit
    y_true[::10] += 15  # tous les 10 points, on ajoute une grosse valeur
    

**Objectif :** Trouver une courbe 2D de la forme :

.. math::

    y = f(x) =a x^2 + b x + c

o√π : $$a$$, $$b$$ et $$c$$ sont des param√®tres appris automatiquement en minimisant l'erreur entre les pr√©dictions du mod√®le et les donn√©es r√©elles.


**Consignes** : Impl√©menter une boucle d'entra√Ænement pour ajuster les param√®tres d'une courbe d'ordre 2 aux donn√©es fournies en utilisant une fonction de perte MAE et MSE.

1) R√©utilisez la boucle d'entra√Ænement de l‚Äôexercice 3 qui s'arr√™te au bout de 1000 it√©rations et qui utilise un learning rate de 0.01.  

2) Testez la fonction de perte MSE et MAE.

3) Pour chaque fonction de perte, affichez les param√®tres appris $$a$$, $$b$$ et $$c$$.

4) Pour chaque fonction de perte, tracez les donn√©es r√©elles et les donn√©es pr√©dites et comparez visuellement les r√©sultats.

6) Quelle diff√©rence observez-vous dans la convergence et les param√®tres appris ?

7) Pourquoi la MSE et la MAE ne donnent-elles pas exactement le m√™me r√©sultat ?

8) Dans quel cas pr√©f√®reriez-vous utiliser MSE ? Dans quel cas pr√©f√®reriez-vous utiliser MAE ?

**Astuce :**
.. spoiler::
    .. discoverList::
        - La MSE p√©nalise davantage les grandes erreurs.  
        - La MAE est plus robuste aux valeurs aberrantes (outliers).


**R√©sultat attendu :**
Vous devez obtenir des valeurs pour les param√®tres proches de :

    - MSE -> a = 2.002, b = 2.866, c = 2.464
    - MAE -> a = 1.984, b = 2.997, c = 1.132

et un graphique similaire √† celui ci-dessous :

.. image:: images/chap1_exo_sup_3_resultat.png
    :alt: droite ajust√©e aux points
    :align: center


.. slide::
üå∂Ô∏è Exercice suppl√©mentaire 4 : Visualiser une surface de perte en 3D & descente de gradient
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On consid√®re la fonction suivante :  

.. math::

    f(a, b) = a^2 + b^2


**Objectif :** Comprendre la descente de gradient en visualisant la surface de la fonction et la trajectoire de convergence.  


**Consignes :**

1) Calculez √† la main le gradient de $$f(a,b)$$ et ses d√©riv√©es partielles .

2) Impl√©mentez une boucle de descente de gradient avec un point de d√©part choisi (par exemple $$a=2.5$$, $$b=-2.0$$) et un learning rate de 0.1.

3) Stockez les points de la trajectoire au cours des it√©rations.  

4) Tracez la surface 3D de $$f(a, b)$$ avec Matplotlib.  

5) Ajoutez sur la surface des fl√®ches repr√©sentant les √©tapes de la descente de gradient.  

6) Expliquez ce que repr√©sente la trajectoire observ√©e et pourquoi elle converge vers $$(a, b) = (0,0)$$.

7) Testez plusieurs learning rate (ex: 0.02, 0.1, 0.5, 2.0) pour observer convergence lente, rapide, ou divergence.


**Astuce :**
.. spoiler::
    .. discoverList::
        - Utilisez ``ax.plot_surface`` pour la surface 3D.  
        - Utilisez ``ax.quiver`` pour tracer les fl√®ches en 3D.  
        - Le minimum de la fonction est atteint en $$(0,0,0)$$. 

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Squelette de code :**
        .. code-block:: python

            import numpy as np
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D  # n√©cessaire pour la 3D

            # 1) Cr√©er une grille pour la surface
            A = np.linspace(-3, 3, 100)
            B = np.linspace(-3, 3, 100)
            AA, BB = np.meshgrid(A, B)

            # √Ä compl√©ter : calculer Z = f(a,b) = a^2 + b^2
            Z = ...

            # 2) Pr√©parer une figure 3D
            fig = plt.figure(figsize=(7, 5))
            ax = fig.add_subplot(111, projection='3d')
            ax.plot_surface(AA, BB, Z, alpha=0.5)

            # 3) Descente de gradient depuis un point de d√©part
            lr = 0.1   # learning rate
            a, b = 2.5, -2.0
            n_iter = 15

            traj = [(a, b, a**2 + b**2)]

            for _ in range(n_iter):
                # √Ä compl√©ter : calculer le gradient ga, gb
                ga, gb = ...
                
                # √Ä compl√©ter : mettre √† jour a et b avec le learning rate
                a, b = ...
                
                traj.append((a, b, a**2 + b**2))

            # 4) Repr√©senter la trajectoire (quiver pour fl√®ches)
            for (a1, b1, z1), (a2, b2, z2) in zip(traj[:-1], traj[1:]):
                # √Ä compl√©ter : dessiner une fl√®che de (a1,b1,z1) vers (a2,b2,z2)
                ax.quiver(...)

            ax.set_xlabel('a')
            ax.set_ylabel('b')
            ax.set_zlabel('f(a,b)')
            ax.set_title('Surface de perte et descente de gradient')
            plt.tight_layout()
            plt.show()


**R√©sultat attendu :**  
Un graphique 3D montrant la surface convexe de la fonction et la descente du point de d√©part vers le minimum global en $$(0,0)$$ avec ``lr=0.1`` :  

.. image:: images/chap1_exo_sup_4_resultat.png
    :alt: droite ajust√©e aux points
    :align: center








