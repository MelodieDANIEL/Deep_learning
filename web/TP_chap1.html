<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<link rel="stylesheet" type="text/css" href="css/style.css" />
<title>Intro Python - üèãÔ∏è Travaux Pratiques 1</title>

                <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="viewport-fit=cover, width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            

                <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            
<link href="https://fonts.googleapis.com/css2?family=Gentium+Basic&display=swap" rel="stylesheet"> 
<link rel="stylesheet" type="text/css" href="slidey/bootstrap/css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" href="slidey/bootstrap-icons/font/bootstrap-icons.css" />
<link rel="stylesheet" type="text/css" href="slidey/highlight.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.menu.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.step.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.code.css" />
<link rel="stylesheet" type="text/css" href="slidey/slidey.slide.css" />
<script type="text/javascript" src="slidey/js/jquery.js"></script>
<script type="text/javascript" src="slidey/js/slidey.permalink.js"></script>
<script type="text/javascript" src="slidey/js/slidey.menu.js"></script>
<script type="text/javascript" src="slidey/js/slidey.mobile.js"></script>
<script type="text/javascript" src="slidey/js/slidey.spoilers.js"></script>
<script type="text/javascript" src="slidey/js/slidey.steps.js"></script>
<script type="text/javascript" src="slidey/js/slidey.js"></script>
<script type="text/javascript" src="slidey/js/main.js"></script>
<script type="text/javascript" src="slidey/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="slidey/highlight/highlight.pack.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico" />
</head>
<body>
<!-- Modal log-in window -->
<div class="modal fade" id="loginWindow">
  <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
            <h5 class="modal-title">Log-in</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>    
        <div class="modal-body">
            <form>
                Mot de passe&nbsp;:<br />
                <input class="form-control" type="password" name="password" />
            </form>
        </div>
      </div><!-- /.modal-content -->
    </div><!-- /.modal-dialog -->
</div>

<div class="core">

    <script>hljs.highlightAll();</script>

<!-- Extra-controls for mobile device -->
<div class="mobileControls">
    <div class="btn-group">
        <a href="javascript:void(0)" class="left btn btn-light btn-lg">
            <i class="bi bi-arrow-up"></i>
        </a>
        <a href="javascript:void(0)" class="right btn btn-light btn-lg">
            <i class="bi bi-arrow-down"></i>
        </a>
        <a href="javascript:void(0)" class="up btn btn-light btn-lg">
            <i class="bi bi-arrow-left"></i>
        </a>
        <a href="javascript:void(0)" class="down btn btn-light btn-lg">
            <i class="bi bi-arrow-right"></i>
        </a>
        <a href="javascript:void(0)" class="login btn btn-light btn-lg">
            <i class="bi bi-lock"></i>
        </a>
    </div>
</div>

<!-- Controls -->
<div class="slideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg slideModeSlide"><i class="bi bi-film"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i> </a>
        <a href="index.html" class="btn btn-light btn-lg goHome"><i class="bi bi-house"></i></a>
    </div>
</div>

<div class="exitSlideMode">
    <div class="btn-group">
        <a href="javascript:void(0)" class="btn btn-light btn-lg followMode"><i class="bi bi-stopwatch"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg showMobile"><i class="bi bi-phone"></i></a>
        <a href="javascript:void(0)" class="btn btn-light btn-lg stopShow">
            <span class="currentSlideNumber">
            </span>
            <i class="bi bi-x-lg"></i>
        </a>
    </div>
</div>

<!-- Browsing menu -->
<div class="menu">
</div>

<div class="contents container">

<a id="title.1"></a><h1>üèãÔ∏è Travaux Pratiques 1</h1>
<div class="slide ">
<p>Sur cette page se trouvent des exercices de TP sur le Chapitre 1. Ils sont class√©s par niveau de difficult√© :</p>
<ul><li class="discover">Facile : üçÄ</li>
<li class="discover">Moyen : ‚öñÔ∏è</li>
<li class="discover">Difficile : üå∂Ô∏è</li>
</ul>

</div><div class="slide ">
<a id="title.1.1"></a><h2>üçÄ Exercice 1 : Calculer le gradient d‚Äôune fonction simple avec PyTorch</h2>
<p>Consid√©rons la fonction suivante : \(f(a) = a^2 + a\), avec \(a = 1.0\).</p>
<p><strong>Consigne :</strong> Utiliser les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† \(a\) :</p>
<p>1) Calculez √† la main la d√©riv√©e de \(f\) par rapport √† \(a\). Puis √©valuez ce gradient pour \(a = 1.0\).  </p>
<p>2) Faites l&#039;impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.</p>
<p>3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><blockquote><p>La d√©riv√©e de \(f(a)\) par rapport √† \(a\) est √©gale √† \(2a + 1\)</p>
</blockquote>
</div>
<p><strong>R√©sultat attendu :</strong> Le gradient est √©gal √† 3 dans les deux cas.</p>
</div><div class="slide ">
<a id="title.1.2"></a><h2>‚öñÔ∏è Exercice 2 : Trouver la droite qui passe au mieux par les donn√©es avec MSE</h2>
<p>Dans cet exercice, vous allez impl√©menter une <strong>boucle d&#039;entra√Ænement simple</strong> pour ajuster les param√®tres d&#039;une droite<br />aux donn√©es fournies.</p>
<p>On vous donne les donn√©es suivantes :</p>
<pre><code class="python hljs"># Donn√©es bruit√©es suivantes
import numpy as np
x = np.random.rand(1000)
y_true = x * 1.54 + 12.5 + np.random.rand(1000)*0.2</code></pre>
<p><strong>Objectif :</strong> Trouver une droite de la forme :</p>
$$y = f(x) =a x + b$$
<p>o√π : \(a\) et \(b\) sont des param√®tres appris automatiquement en minimisant l&#039;erreur entre les pr√©dictions du mod√®le et les donn√©es r√©elles.</p>
<p><strong>Consigne :</strong> √âcrire un programme qui ajuste les param√®tres \(a\) et \(b\) de la droite aux donn√©es fournies en utilisant  PyTorch.</p>
<blockquote><p>1) Dans un premier temps, vous pouvez faire une boucle de 10000 it√©rations et coder vous-m√™me la fonction de perte.</p>
<p>2) Afficher les param√®tres appris \(a\) et \(b\).</p>
<p>3) Ensuite, trouver un moyen plus intelligent d&#039;arr√™ter l&#039;entra√Ænement de tel sorte √† ce que le mod√®le converge avec le minimum d&#039;it√©rations.</p>
<p>4) Afficher le nombre d&#039;it√©rations n√©cessaires pour converger.</p>
<p>5) Tracer les donn√©es r√©elles et les donn√©es pr√©dites pour comparer visuellement le r√©sultat.</p>
<p>6) Utiliser la fonction de perte MSE fournie par PyTorch et afficher les param√®tres appris \(a\) et \(b\).</p>
<p>7) V√©rifier que le r√©sultat des param√®tres et le trac√© sont similaires √† ceux obtenus avec la boucle d&#039;entra√Ænement manuelle.</p>
</blockquote>
<p><strong>Remarque :</strong> Pour utiliser <code>matplotlib</code>, vous devez l&#039;installer avec la commande suivante :</p>
<pre><code class="bash hljs">pip install matplotlib</code></pre>
<p>Puis, vous pouvez l&#039;importer dans votre code avec :</p>
<pre><code class="python hljs">import matplotlib.pyplot as plt
%matplotlib inline #√Ä ajouter si vous utilisez Jupyter Notebook</code></pre>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ol><li class="discover">Initialiser les param√®tres : \(a\) et \(b\) √† z√©ro.</li>
<li class="discover">Utiliser une fonction de perte en codant l&#039;√©quation de la MSE (loss = torch.sum((y_pred - y_true) ** 2)).</li>
<li class="discover">Impl√©menter une boucle d&#039;entra√Ænement (par exemple 10000 it√©rations) avec l&#039;optimiseur ADAM <code>torch.optim.ADAM</code>.</li>
<li class="discover">√Ä chaque it√©ration :</li>
<ul><li class="dash discover">calculer les pr√©dictions,</li>
<li class="dash discover">calculer la perte,</li>
<li class="dash discover">effectuer la r√©tropropagation,</li>
<li class="dash discover">mettre √† jour les param√®tres :\(a\) et \(b\).</li>
</ul>
<li class="discover">Il faut arr√™ter l&#039;entra√Ænement lorsque la perte est suffisamment faible (par exemple, inf√©rieure √† 0.01)</li>
</ol>

</div>
<p><strong>R√©sultat attendu :</strong> Vous devez obtenir un graphique o√π : <br />    - les points bleus correspondent aux donn√©es r√©elles (<code>y_true</code>), <br />    - et une droite rouge correspond aux pr√©dictions (<code>y_pred</code>).  </p>
<p>Exemple d‚Äôaffichage attendu :</p>
<img src="images/chap1_exo_2_resultat.png"  alt="droite ajust√©e aux points" align="center" />
</div><div class="slide ">
<a id="title.1.3"></a><h2>‚öñÔ∏è Exercice 3 : Trouver la droite qui passe au mieux par les donn√©es avec une fonction de perte de type valeur absolue</h2>
<p><strong>Objectif</strong> : <br />L&#039;objectif est le m√™me que celui de l&#039;exercice pr√©c√©dent (faire de la r√©gression lin√©aire), mais cette fois-ci,  vous allez utiliser une fonction de perte de type valeur absolue (MAE de l&#039;anglais Mean Absolute Error)  au lieu de la MSE. L‚Äôid√©e de cet exercice est de comparer deux optimisateurs SGD et Adam.</p>
<p><strong>Consignes :</strong>  Impl√©menter une boucle d&#039;entra√Ænement pour ajuster les param√®tres d&#039;une droite aux donn√©es fournies dans l&#039;exercice pr√©c√©dent en utilisant une fonction de perte de type valeur absolue et en r√©utilisant l&#039;impl√©mentation de l&#039;exercice pr√©c√©dent.</p>
<p>1) R√©utilisez la boucle d&#039;entra√Ænement de l‚Äôexercice pr√©c√©dent qui s&#039;arr√™te au bout de 2500 it√©rations et qui utilise un learning rate de 0.01. <br />2) Remplacez la fonction de perte MSE par une fonction de perte de type MAE. Il faudra chercher dans la documentation comment l&#039;impl√©menter dans PyTorch. <br />3) Testez avec l‚Äôoptimiseur SGD puis avec l‚Äôoptimiseur Adam. <br />4) Pour chaque optimiseur, affichez les param√®tres appris appris \(a\) et \(b\).
5) Tracez les donn√©es r√©elles et les donn√©es pr√©dites pour comparer visuellement les r√©sultats. <br />6) Comparez les deux m√©thodes : que constatez-vous en termes de stabilit√© et de vitesse de convergence ? <br />7) Expliquez quel optimiseur est meilleur et pourquoi?   </p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ul><li class="dash discover">La valeur absolue dans PyTorch s&#039;obtient avec la fonction <code>nn.L1Loss()</code>.</li>
<li class="dash discover">Adam g√®re mieux ce type de fonction de perte non d√©rivable partout.</li>
</ul>

</div>
<p><strong>R√©sultat attendu :</strong>
Vous devez obtenir des valeurs pour les param√®tres proche de :</p>
<ul><li class="dash">Adam -&gt; a = 1.5451, b = 12.5996</li>
<li class="dash">SGD  -&gt; a = 2.3039, b = 12.1880</li>
</ul>

<p>et un graphique similaire √† celui ci-dessous :</p>
<img src="images/chap1_exo_3_resultat.png"  alt="droite ajust√©e aux points" align="center" />
</div><div class="slide ">
<a id="title.2"></a><h1>üèãÔ∏è Exercices suppl√©mentaires 1</h1>
<p>Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.</p>
</div><div class="slide ">
<a id="title.2.1"></a><h2>üçÄ Exercice suppl√©mentaire 1 : Gradient d‚Äôune fonction polynomiale</h2>
<p>Consid√©rons la fonction suivante \(f(a) = 3a^3 - 2a^2 + a\) avec \(a = 2.0\).</p>
<p><strong>Consigne :</strong> Utiliser les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† \(a\) :</p>
<p>1) Calculez √† la main la d√©riv√©e de \(f\) par rapport √† \(a\). Puis √©valuez ce gradient pour \(a = 2.0\).</p>
<p>2) Faites l&#039;impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.</p>
<p>3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><blockquote><p>La d√©riv√©e de \(f(a)\) par rapport √† \(a\) est √©gale √† \(9a¬≤ - 4a + 1\)</p>
</blockquote>
</div>
<p><strong>R√©sultat attendu :</strong> <br />Le gradient est √©gal √† 29 dans les deux cas. </p>
</div><div class="slide ">
<a id="title.2.2"></a><h2>üçÄ Exercice suppl√©mentaire 2 : Gradient de deux variables</h2>
<p>Consid√©rons la fonction suivante \(f(a, b) = a \cdot b + a^2\) avec \(a = 2.0\) et \(b = 3.0\).</p>
<p><strong>Consigne :</strong> Utiliser les deux approches suivantes pour calculer les d√©riv√©es partielles de cette fonction par rapport √† \(a\) et \(b\) :</p>
<p>1) Calculez √† la main la d√©riv√©e partielle de \(f\) par rapport √† \(a\) et par rapport √† \(b\). Puis √©valuez ces d√©riv√©es pour \(a = 2.0\) et \(b = 3.0\).</p>
<p>2) Faites l&#039;impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez le gradient de cette fonction.</p>
<p>3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.</p>
<p><strong>Astuce :</strong>  </p>
<div class="spoiler"><ul><li class="dash discover">La d√©riv√©e de \(f\) par rapport √† \(a\) est \(‚àÇf/‚àÇa = b + 2a\) et par rapport √† \(b\) est \(‚àÇf/‚àÇb = a\).</li>
</ul>

</div>
<p><strong>R√©sultat attendu :</strong> <br />Les d√©riv√©es partielles sont, dans les deux cas, √©gales √† : \(‚àÇf/‚àÇa = 7\) et \(‚àÇf/‚àÇb = 2\).</p>
<a id="title.2.3"></a><h2>‚öñÔ∏è Exercice suppl√©mentaire 3 : Comparaison de des fonctions de perte MSE et MAE</h2>
<p>On vous donne les donn√©es suivantes :</p>
<pre><code class="python hljs"># Donn√©es bruit√©es suivantes
torch.manual_seed(0)
x = torch.linspace(-3, 3, 100)
y_true = 2 * x**2 + 3 * x + 1 + 0.5 * torch.randn(x.size())  # avec bruit
y_true[::10] += 15  # tous les 10 points, on ajoute une grosse valeur</code></pre>
<p><strong>Objectif :</strong> Trouver une courbe 2D de la forme :</p>
$$y = f(x) =a x^2 + b x + c$$
<p>o√π : \(a\), \(b\) et \(c\) sont des param√®tres appris automatiquement en minimisant l&#039;erreur entre les pr√©dictions du mod√®le et les donn√©es r√©elles.</p>
<p><strong>Consignes</strong> : Impl√©menter une boucle d&#039;entra√Ænement pour ajuster les param√®tres d&#039;une courbe d&#039;ordre 2 aux donn√©es fournies en utilisant une fonction de perte MAE et MSE.</p>
<p>1) R√©utilisez la boucle d&#039;entra√Ænement de l‚Äôexercice 3 qui s&#039;arr√™te au bout de 1000 it√©rations et qui utilise un learning rate de 0.01.  </p>
<p>2) Tester la fonction de perte MSE et MAE.</p>
<p>3) Pour chaque fonction de perte, afficher les param√®tres appris \(a\), \(b\) et \(c\).</p>
<p>4) Pour chaque fonction de perte, tracer les donn√©es r√©elles et les donn√©es pr√©dites et comparer visuellement les r√©sultats. </p>
<p>6) Quelle diff√©rence observez-vous dans la convergence et les param√®tres appris ?</p>
<p>7) Pourquoi la MSE et la MAE ne donnent-elles pas exactement le m√™me r√©sultat ?</p>
<p>8) Dans quel cas pr√©f√®reriez-vous utiliser MSE ? Dans quel cas pr√©f√®reriez-vous utiliser MAE ?</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ul><li class="dash discover">La MSE p√©nalise davantage les grandes erreurs.</li>
<li class="dash discover">La MAE est plus robuste aux valeurs aberrantes (outliers).</li>
</ul>

</div>
<p><strong>R√©sultat attendu :</strong>
Vous devez obtenir des valeurs pour les param√®tres proche de :</p>
<ul><li class="dash">MSE -&gt; a = 2.002, b = 2.866, c = 2.464</li>
<li class="dash">MAE -&gt; a = 1.984, b = 2.997, c = 1.132</li>
</ul>

<p>et un graphique similaire √† celui ci-dessous :</p>
<img src="images/chap1_exo_sup_3_resultat.png"  alt="droite ajust√©e aux points" align="center" />
</div><div class="slide ">
<a id="title.2.4"></a><h2>üå∂Ô∏è Exercice suppl√©mentaire 4 : Visualiser une surface de perte en 3D &amp; descente de gradient</h2>
<p>On consid√®re la fonction suivante :  </p>
$$f(a, b) = a^2 + b^2$$
<p><strong>Objectif :</strong> Comprendre la descente de gradient en visualisant la surface de la fonction et la trajectoire de convergence.  </p>
<p><strong>Consignes :</strong></p>
<p>1) Calculer √† la main le gradient de \(f(a,b)\) et ses d√©riv√©es partielles .</p>
<p>2) Impl√©menter une boucle de descente de gradient avec un point de d√©part choisi (par exemple \(a=2.5\), \(b=-2.0\)) et un learning rate de 0.1.</p>
<p>3) Stocker les points de la trajectoire au cours des it√©rations.  </p>
<p>4) Tracer la surface 3D de \(f(a, b)\) avec Matplotlib.  </p>
<p>5) Ajouter sur la surface des fl√®ches repr√©sentant les √©tapes de la descente de gradient.  </p>
<p>6) Expliquer ce que repr√©sente la trajectoire observ√©e et pourquoi elle converge vers \((a, b) = (0,0)\).</p>
<p>7) Testez plusieurs learning rate (ex: 0.02, 0.1, 0.5, 2.0) pour observer convergence lente, rapide, ou divergence.</p>
<p><strong>Astuce :</strong></p>
<div class="spoiler"><ul><li class="dash discover">Utilisez <code>ax.plot_surface</code> pour la surface 3D.</li>
<li class="dash discover">Utilisez <code>ax.quiver</code> pour tracer les fl√®ches en 3D.</li>
<li class="dash discover">Le minimum de la fonction est atteint en \((0,0,0)\).</li>
</ul>

</div>
<p><strong>Astuce avanc√©e :</strong>        </p>
<div class="spoiler"><blockquote><p><strong>Squelette de code :</strong></p>
<pre><code class="python hljs">import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # n√©cessaire pour la 3D

# 1) Cr√©er une grille pour la surface
A = np.linspace(-3, 3, 100)
B = np.linspace(-3, 3, 100)
AA, BB = np.meshgrid(A, B)

# √Ä compl√©ter : calculer Z = f(a,b) = a^2 + b^2
Z = ...

# 2) Pr√©parer une figure 3D
fig = plt.figure(figsize=(7, 5))
ax = fig.add_subplot(111, projection=&#039;3d&#039;)
ax.plot_surface(AA, BB, Z, alpha=0.5)

# 3) Descente de gradient depuis un point de d√©part
lr = 0.1   # learning rate
a, b = 2.5, -2.0
n_iter = 15

traj = [(a, b, a**2 + b**2)]

for _ in range(n_iter):
    # √Ä compl√©ter : calculer le gradient ga, gb
    ga, gb = ...
    
    # √Ä compl√©ter : mettre √† jour a et b avec le learning rate
    a, b = ...
    
    traj.append((a, b, a**2 + b**2))

# 4) Repr√©senter la trajectoire (quiver pour fl√®ches)
for (a1, b1, z1), (a2, b2, z2) in zip(traj[:-1], traj[1:]):
    # √Ä compl√©ter : dessiner une fl√®che de (a1,b1,z1) vers (a2,b2,z2)
    ax.quiver(...)

ax.set_xlabel(&#039;a&#039;)
ax.set_ylabel(&#039;b&#039;)
ax.set_zlabel(&#039;f(a,b)&#039;)
ax.set_title(&#039;Surface de perte et descente de gradient&#039;)
plt.tight_layout()
plt.show()</code></pre>
</blockquote>
</div>
<p><strong>R√©sultat attendu :</strong> <br />Un graphique 3D montrant la surface convexe de la fonction et la descente du point de d√©part vers le minimum global en \((0,0)\) avec <code>lr=0.1</code> :  </p>
<img src="images/chap1_exo_sup_4_resultat.png"  alt="droite ajust√©e aux points" align="center" />
</div>
<nav><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="chap1.html"><i class="bi bi-folder2"></i> Chapitre 1 - Introduction √† PyTorch et Optimisation de Mod√®les</a></li></ul></nav>

</div> <!-- /contents -->
</div> <!-- /core -->

</body>
</html>
