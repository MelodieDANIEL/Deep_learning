üèãÔ∏è Travaux Pratiques 1
=========================
.. slide::
Sur cette page se trouvent des exercices de TP sur le Chapitre 1. Ils sont class√©s par niveau de difficult√© :
.. discoverList::
    * Facile : üçÄ
    * Moyen : ‚öñÔ∏è
    * Difficile : üå∂Ô∏è


.. slide::
üçÄ Exercice 1 : Calculer le gradient d‚Äôune fonction simple avec PyTorch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante : $$f(a) = a^2 + a$$, avec $$a = 1.0$$.

**Consigne :** Utiliser les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† $$a$$ :

1) Calculez √† la main la d√©riv√©e de $$f$$ par rapport √† $$a$$. Puis √©valuez ce gradient pour $$a = 1.0$$.  

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.

**Astuce :**
.. spoiler::
    .. discoverList::
        La d√©riv√©e de $$f(a)$$ par rapport √† $$a$$ est √©gale √† $$2a + 1$$

**R√©sultat attendu :** Le gradient est √©gal √† 3 dans les deux cas.



.. slide::
‚öñÔ∏è Exercice 2 : Trouver la droite qui passe au mieux par les donn√©es avec MSE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dans cet exercice, vous allez impl√©menter une **boucle d'entra√Ænement simple** pour ajuster les param√®tres d'une droite 
aux donn√©es fournies.

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es bruit√©es suivantes
    import numpy as np
    x = np.random.rand(1000)
    y_true = x * 1.54 + 12.5 + np.random.rand(1000)*0.2
    

**Objectif :** Trouver une droite de la forme :

.. math::

    y = f(x) =a x + b

o√π : $$a$$ et $$b$$ sont des param√®tres appris automatiquement en minimisant l'erreur entre les pr√©dictions du mod√®le et les donn√©es r√©elles.

**Consigne :** √âcrire un programme qui ajuste les param√®tres $$a$$ et $$b$$ de la droite aux donn√©es fournies en utilisant  PyTorch.

    1) Dans un premier temps, vous pouvez faire une boucle de 10000 it√©rations et coder vous-m√™me la fonction de perte.

    2) Afficher les param√®tres appris $$a$$ et $$b$$.

    3) Ensuite, trouver un moyen plus intelligent d'arr√™ter l'entra√Ænement de tel sorte √† ce que le mod√®le converge avec le minimum d'it√©rations.
    
    4) Afficher le nombre d'it√©rations n√©cessaires pour converger.
    
    5) Tracer les donn√©es r√©elles et les donn√©es pr√©dites pour comparer visuellement le r√©sultat.

    6) Utiliser la fonction de perte MSE fournie par PyTorch et afficher les param√®tres appris $$a$$ et $$b$$.

    7) V√©rifier que le r√©sultat des param√®tres et le trac√© sont similaires √† ceux obtenus avec la boucle d'entra√Ænement manuelle.


**Remarque :** Pour utiliser ``matplotlib``, vous devez l'installer avec la commande suivante :

.. code-block:: bash
    pip install matplotlib

Puis, vous pouvez l'importer dans votre code avec :

.. code-block:: python
    import matplotlib.pyplot as plt
    %matplotlib inline #√Ä ajouter si vous utilisez Jupyter Notebook



**Astuce :**
.. spoiler::
    .. discoverList::
        1. Initialiser les param√®tres : $$a$$ et $$b$$ √† z√©ro.
        2. Utiliser une fonction de perte en codant l'√©quation de la MSE (loss = torch.sum((y_pred - y_true) ** 2)).
        3. Impl√©menter une boucle d'entra√Ænement (par exemple 10000 it√©rations) avec l'optimiseur ADAM ``torch.optim.ADAM``.
        4. √Ä chaque it√©ration :
            - calculer les pr√©dictions,
            - calculer la perte,
            - effectuer la r√©tropropagation,
            - mettre √† jour les param√®tres :$$a$$ et $$b$$.

        5. Il faut arr√™ter l'entra√Ænement lorsque la perte est suffisamment faible (par exemple, inf√©rieure √† 0.01)

**R√©sultat attendu :** Vous devez obtenir un graphique o√π :  
    - les points bleus correspondent aux donn√©es r√©elles (``y_true``),  
    - et une droite rouge correspond aux pr√©dictions (``y_pred``).  

Exemple d‚Äôaffichage attendu :

.. image:: images/chap1_exo_2_resultat.png
    :alt: droite ajust√©e aux points
    :align: center


.. slide::
‚öñÔ∏è Exercice 3 : Trouver la droite qui passe au mieux par les donn√©es avec une fonction de perte de type valeur absolue
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Objectif** :  
L'objectif est le m√™me que celui de l'exercice pr√©c√©dent (faire de la r√©gression lin√©aire), mais cette fois-ci,  vous allez utiliser une fonction de perte de type valeur absolue (MAE de l'anglais Mean Absolute Error)  au lieu de la MSE. L‚Äôid√©e de cet exercice est de comparer deux optimisateurs SGD et Adam.

**Consignes :**  Impl√©menter une boucle d'entra√Ænement pour ajuster les param√®tres d'une droite aux donn√©es fournies dans l'exercice pr√©c√©dent en utilisant une fonction de perte de type valeur absolue et en r√©utilisant l'impl√©mentation de l'exercice pr√©c√©dent.


1) R√©utilisez la boucle d'entra√Ænement de l‚Äôexercice pr√©c√©dent qui s'arr√™te au bout de 2500 it√©rations et qui utilise un learning rate de 0.01.  
2) Remplacez la fonction de perte MSE par une fonction de perte de type MAE. Il faudra chercher dans la documentation comment l'impl√©menter dans PyTorch.  
3) Testez avec l‚Äôoptimiseur SGD puis avec l‚Äôoptimiseur Adam.  
4) Pour chaque optimiseur, affichez les param√®tres appris appris $$a$$ et $$b$$.
5) Tracez les donn√©es r√©elles et les donn√©es pr√©dites pour comparer visuellement les r√©sultats.  
6) Comparez les deux m√©thodes : que constatez-vous en termes de stabilit√© et de vitesse de convergence ?  
7) Expliquez quel optimiseur est meilleur et pourquoi?   

**Astuce :**
.. spoiler::
    .. discoverList::
        - La valeur absolue dans PyTorch s'obtient avec la fonction ``nn.L1Loss()``.
        - Adam g√®re mieux ce type de fonction de perte non d√©rivable partout.


**R√©sultat attendu :**
Vous devez obtenir des valeurs pour les param√®tres proche de :

    - Adam -> a = 1.5451, b = 12.5996
    - SGD  -> a = 2.3039, b = 12.1880


et un graphique similaire √† celui ci-dessous :

.. image:: images/chap1_exo_3_resultat.png
    :alt: droite ajust√©e aux points
    :align: center


.. slide::
üèãÔ∏è Exercices suppl√©mentaires 1
===============================
Dans cette section, il y a des exercices suppl√©mentaires pour vous entra√Æner. Ils suivent le m√™me classement de difficult√© que pr√©c√©demment.


.. slide::
üçÄ Exercice suppl√©mentaire 1 : Gradient d‚Äôune fonction polynomiale
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante $$f(a) = 3a^3 - 2a^2 + a$$ avec $$a = 2.0$$.

**Consigne :** Utiliser les deux approches suivantes pour calculer le gradient de cette fonction par rapport √† $$a$$ :

1) Calculez √† la main la d√©riv√©e de $$f$$ par rapport √† $$a$$. Puis √©valuez ce gradient pour $$a = 2.0$$.

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez son gradient.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.

**Astuce :**
.. spoiler::
    .. discoverList::
        La d√©riv√©e de $$f(a)$$ par rapport √† $$a$$ est √©gale √† $$9a¬≤ - 4a + 1$$

**R√©sultat attendu :**  
Le gradient est √©gal √† 29 dans les deux cas. 


.. slide::
üçÄ Exercice suppl√©mentaire 2 : Gradient de deux variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consid√©rons la fonction suivante $$f(a, b) = a \cdot b + a^2$$ avec $$a = 2.0$$ et $$b = 3.0$$.


**Consigne :** Utiliser les deux approches suivantes pour calculer les d√©riv√©es partielles de cette fonction par rapport √† $$a$$ et $$b$$ :

1) Calculez √† la main la d√©riv√©e partielle de $$f$$ par rapport √† $$a$$ et par rapport √† $$b$$. Puis √©valuez ces d√©riv√©es pour $$a = 2.0$$ et $$b = 3.0$$.

2) Faites l'impl√©mentation de la m√™me fonction avec PyTorch, calculez et √©valuez le gradient de cette fonction.

3) Comparez le r√©sultat obtenu par PyTorch avec le calcul manuel.


**Astuce :**  
.. spoiler::
    .. discoverList::
        - La d√©riv√©e de $$f$$ par rapport √† $$a$$ est $$‚àÇf/‚àÇa = b + 2a$$ et par rapport √† $$b$$ est $$‚àÇf/‚àÇb = a$$.

**R√©sultat attendu :**  
Les d√©riv√©es partielles sont, dans les deux cas, √©gales √† : $$‚àÇf/‚àÇa = 7$$ et $$‚àÇf/‚àÇb = 2$$.


‚öñÔ∏è Exercice suppl√©mentaire 3 : Comparaison de des fonctions de perte MSE et MAE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On vous donne les donn√©es suivantes :

.. code-block:: python

    # Donn√©es bruit√©es suivantes
    torch.manual_seed(0)
    x = torch.linspace(-3, 3, 100)
    y_true = 2 * x**2 + 3 * x + 1 + 0.5 * torch.randn(x.size())  # avec bruit
    y_true[::10] += 15  # tous les 10 points, on ajoute une grosse valeur
    

**Objectif :** Trouver une courbe 2D de la forme :

.. math::

    y = f(x) =a x^2 + b x + c

o√π : $$a$$, $$b$$ et $$c$$ sont des param√®tres appris automatiquement en minimisant l'erreur entre les pr√©dictions du mod√®le et les donn√©es r√©elles.


**Consignes** : Impl√©menter une boucle d'entra√Ænement pour ajuster les param√®tres d'une courbe d'ordre 2 aux donn√©es fournies en utilisant une fonction de perte MAE et MSE.

1) R√©utilisez la boucle d'entra√Ænement de l‚Äôexercice 3 qui s'arr√™te au bout de 1000 it√©rations et qui utilise un learning rate de 0.01.  

2) Tester la fonction de perte MSE et MAE.

3) Pour chaque fonction de perte, afficher les param√®tres appris $$a$$, $$b$$ et $$c$$.

4) Pour chaque fonction de perte, tracer les donn√©es r√©elles et les donn√©es pr√©dites et comparer visuellement les r√©sultats. 

6) Quelle diff√©rence observez-vous dans la convergence et les param√®tres appris ?

7) Pourquoi la MSE et la MAE ne donnent-elles pas exactement le m√™me r√©sultat ?

8) Dans quel cas pr√©f√®reriez-vous utiliser MSE ? Dans quel cas pr√©f√®reriez-vous utiliser MAE ?

**Astuce :**
.. spoiler::
    .. discoverList::
        - La MSE p√©nalise davantage les grandes erreurs.  
        - La MAE est plus robuste aux valeurs aberrantes (outliers).


**R√©sultat attendu :**
Vous devez obtenir des valeurs pour les param√®tres proche de :

    - MSE -> a = 2.002, b = 2.866, c = 2.464
    - MAE -> a = 1.984, b = 2.997, c = 1.132

et un graphique similaire √† celui ci-dessous :

.. image:: images/chap1_exo_sup_3_resultat.png
    :alt: droite ajust√©e aux points
    :align: center


.. slide::
üå∂Ô∏è Exercice suppl√©mentaire 4 : Visualiser une surface de perte en 3D & descente de gradient
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On consid√®re la fonction suivante :  

.. math::

    f(a, b) = a^2 + b^2


**Objectif :** Comprendre la descente de gradient en visualisant la surface de la fonction et la trajectoire de convergence.  


**Consignes :**

1) Calculer √† la main le gradient de $$f(a,b)$$ et ses d√©riv√©es partielles .

2) Impl√©menter une boucle de descente de gradient avec un point de d√©part choisi (par exemple $$a=2.5$$, $$b=-2.0$$) et un learning rate de 0.1.

3) Stocker les points de la trajectoire au cours des it√©rations.  

4) Tracer la surface 3D de $$f(a, b)$$ avec Matplotlib.  

5) Ajouter sur la surface des fl√®ches repr√©sentant les √©tapes de la descente de gradient.  

6) Expliquer ce que repr√©sente la trajectoire observ√©e et pourquoi elle converge vers $$(a, b) = (0,0)$$.

7) Testez plusieurs learning rate (ex: 0.02, 0.1, 0.5, 2.0) pour observer convergence lente, rapide, ou divergence.


**Astuce :**
.. spoiler::
    .. discoverList::
        - Utilisez ``ax.plot_surface`` pour la surface 3D.  
        - Utilisez ``ax.quiver`` pour tracer les fl√®ches en 3D.  
        - Le minimum de la fonction est atteint en $$(0,0,0)$$. 

**Astuce avanc√©e :**        
.. spoiler::
    .. discoverList:: 
        **Squelette de code :**
        .. code-block:: python

            import numpy as np
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D  # n√©cessaire pour la 3D

            # 1) Cr√©er une grille pour la surface
            A = np.linspace(-3, 3, 100)
            B = np.linspace(-3, 3, 100)
            AA, BB = np.meshgrid(A, B)

            # √Ä compl√©ter : calculer Z = f(a,b) = a^2 + b^2
            Z = ...

            # 2) Pr√©parer une figure 3D
            fig = plt.figure(figsize=(7, 5))
            ax = fig.add_subplot(111, projection='3d')
            ax.plot_surface(AA, BB, Z, alpha=0.5)

            # 3) Descente de gradient depuis un point de d√©part
            lr = 0.1   # learning rate
            a, b = 2.5, -2.0
            n_iter = 15

            traj = [(a, b, a**2 + b**2)]

            for _ in range(n_iter):
                # √Ä compl√©ter : calculer le gradient ga, gb
                ga, gb = ...
                
                # √Ä compl√©ter : mettre √† jour a et b avec le learning rate
                a, b = ...
                
                traj.append((a, b, a**2 + b**2))

            # 4) Repr√©senter la trajectoire (quiver pour fl√®ches)
            for (a1, b1, z1), (a2, b2, z2) in zip(traj[:-1], traj[1:]):
                # √Ä compl√©ter : dessiner une fl√®che de (a1,b1,z1) vers (a2,b2,z2)
                ax.quiver(...)

            ax.set_xlabel('a')
            ax.set_ylabel('b')
            ax.set_zlabel('f(a,b)')
            ax.set_title('Surface de perte et descente de gradient')
            plt.tight_layout()
            plt.show()


**R√©sultat attendu :**  
Un graphique 3D montrant la surface convexe de la fonction et la descente du point de d√©part vers le minimum global en $$(0,0)$$ avec ``lr=0.1`` :  

.. image:: images/chap1_exo_sup_4_resultat.png
    :alt: droite ajust√©e aux points
    :align: center






