.. slide::
Chapitre 2 ‚Äî Perceptron multi-couches 
===========================================

üéØ Objectifs du Chapitre
----------------------


.. important::

    √Ä la fin de cette section,  vous saurez :  

    - Le fonctionnement du perceptron simple.
    - Utiliser une fonction d'activation adapt√©e.  
    - L‚Äôimportance de la normalisation / standardisation des donn√©es et l'usage des epochs.  
    - Construire un r√©seau de neurones avec ``torch.nn``. 
    - Faire un entra√Ænement simple d‚Äôun MLP pour un probl√®me de r√©gression.   
    - Suivre l‚Äô√©volution de la loss et interpr√©ter les r√©sultats.  
    - Utiliser ``torch-summary`` pour inspecter l‚Äôarchitecture du r√©seau.  


.. slide::

üìñ 1. Rappels sur les perceptrons
----------------------

Le perceptron multi-couches (MLP de Multi-Layers Perceptron en anglais) est la brique de base des r√©seaux de neurones modernes. Dans ce chapitre, nous allons l‚Äôappliquer √† des probl√®mes de r√©gression simple. Avant de commencer, voici quelques rappels.

1.1. Perceptron simple
~~~~~~~~~~~~~~~~~~~~~~

Le perceptron est le bloc de base d‚Äôun r√©seau de neurones. Il r√©alise une transformation lin√©aire suivie (ou pas) d‚Äôune fonction d‚Äôactivation telle que :  

  .. math::

     y = \sigma(Wx + b)

  o√π :  
   - $$y$$ est la sortie du perceptron, 
   - $$\sigma$$ est une fonction d‚Äôactivation, 
   - $$W$$ est la matrice des poids,  
   - $$b$$ est le biais et  
   - $$x$$ est l'ensemble des entr√©es du perceptron.  

.. slide::
1.2. Perceptron intuition
~~~~~~~~~~~~~~~~~~~~~~
.. image:: images/chap2_perceptron.png
    :alt: perceptron
    :align: center
    :width: 40%

avec $$y= \sigma(x_1*w_1 + x_2*w_2 + ...+ x_i*w_i + ... + x_n*w_n + b)$$

üí° **Intuition :**

    - Chaque poids $$w_i$$ mesure l‚Äôimportance de la caract√©ristique $$x_i$$.  
    - Le biais $$b$$ d√©place la fronti√®re de d√©cision.  
    - La fonction d‚Äôactivation permet d‚Äôintroduire de la non-lin√©arit√©, indispensable pour mod√©liser des relations complexes mais nous en parleront plus en d√©tails par la suite.  






.. slide::
üìñ 7. Observer la loss et d√©terminer le nombre d‚Äôepochs
------------------------------------------------------
Lorsqu‚Äôon entra√Æne un mod√®le, il est essentiel de suivre l‚Äô√©volution de la loss pour savoir si le mod√®le apprend correctement et converge vers une solution. Dans l‚Äôexemple pr√©c√©dent, nous avons compar√© l‚Äôimpact de la standardisation sur les pr√©dictions finales. Nous allons maintenant observer l‚Äô√©volution de la loss pendant l‚Äôentra√Ænement pour mieux comprendre la convergence et d√©terminer un nombre d‚Äôepochs appropri√©. Nous allons continuer √† utiliser les donn√©es suivantes pour entra√Æner le mod√®le :

.. code-block:: python

   # Donn√©es d'entra√Ænement
   X = torch.tensor([[0.],[10.],[20.],[30.],[40.],[50.]])
   y = 2*X + 1



################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################

################################ STOP ICI ################################



#####################

parler de dataset loader dans le chapitre 5 et parler de la gestion des outliers

Exemple avec scikit-learn :  √Ä ajouter pour standardiser les donn√©es ????? 

.. code-block:: python

   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)

#####################

############################ Il faudra penser √† cr√©er un gitlab pour le cours ##################

